{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360a36db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    # Python built-in random\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Numpy\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if using multi-GPU\n",
    "    \n",
    "    # For reproducibility\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # Hash-based operations\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "# Call this once at the very top of your notebook\n",
    "set_seed(42)\n",
    "import random\n",
    "\n",
    "# --- TAMBAHKAN BLOK INI UNTUK REPRODUCIBILITY ---\n",
    "SEED = 42\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) # jika menggunakan multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec0abcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# === 1. IMPORT LIBRARY ===\n",
    "# ===================================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import cv2\n",
    "import timm\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import imagehash\n",
    "\n",
    "# ===================================================================\n",
    "# === 2. KONFIGURASI DAN SETUP (TIDAK DIUBAH) ===\n",
    "# ===================================================================\n",
    "# Direktori\n",
    "TRAIN_DIR = \"/kaggle/input/semoga-ajaa/dataset fixx/train\"\n",
    "TEST_DIR = \"/kaggle/input/semoga-ajaa/dataset fixx/test/test\"\n",
    "\n",
    "# Konfigurasi Model dan Training\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMG_SIZE = 518\n",
    "BATCH_SIZE = 8\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "# --- HYPERPARAMETER UNTUK GRADUAL UNFREEZING ---\n",
    "EPOCHS_S1 = 3\n",
    "LR_S1 = 3e-4\n",
    "EPOCHS_S2 = 5\n",
    "LR_S2 = 5e-5\n",
    "EPOCHS_S3 = 12\n",
    "LR_S3 = 1.59e-06\n",
    "\n",
    "# Statistik ImageNet\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "print(f\"Menggunakan device: {DEVICE}\")\n",
    "print(f\"Ukuran gambar: {IMG_SIZE}x{IMG_SIZE}, Batch Size: {BATCH_SIZE}\")\n",
    "\n",
    "# ===================================================================\n",
    "# === 3. FUNGSI DAN KELAS HELPER (TIDAK DIUBAH) ===\n",
    "# ===================================================================\n",
    "def convert_path_to_df(dataset, is_test=False):\n",
    "    image_dir = Path(dataset)\n",
    "    filepaths = list(image_dir.glob(r'**/*.*'))\n",
    "    if is_test:\n",
    "        filepaths = pd.Series(filepaths, name='Filepath').astype(str)\n",
    "        return pd.DataFrame({'Filepath': filepaths})\n",
    "    else:\n",
    "        labels = [p.parts[-2] for p in filepaths]\n",
    "        filepaths = pd.Series(filepaths, name='Filepath').astype(str)\n",
    "        labels = pd.Series(labels, name='Label')\n",
    "        return pd.concat([filepaths, labels], axis=1)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_column, label_column=None, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_column = image_column\n",
    "        self.label_column = label_column\n",
    "        self.transform = transform\n",
    "    def __len__(self): return len(self.dataframe)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.dataframe.iloc[idx][self.image_column]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform: image = self.transform(image)\n",
    "        if self.label_column:\n",
    "            label = self.dataframe.iloc[idx][self.label_column]\n",
    "            return image, torch.tensor(label, dtype=torch.long)\n",
    "        return image\n",
    "class DualTransformDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_column, label_column, transform_main, transform_extra):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_column = image_column\n",
    "        self.label_column = label_column\n",
    "        self.transform_main = transform_main\n",
    "        self.transform_extra = transform_extra\n",
    "\n",
    "    def __len__(self):\n",
    "        # Gandakan ukuran dataset (dua versi per gambar)\n",
    "        return len(self.dataframe) * 2\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Tentukan apakah pakai augmentasi utama atau tambahan\n",
    "        base_idx = idx // 2\n",
    "        use_extra = idx % 2 == 1\n",
    "\n",
    "        row = self.dataframe.iloc[base_idx]\n",
    "        img_path = row[self.image_column]\n",
    "        label = torch.tensor(row[self.label_column], dtype=torch.long)\n",
    "\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if use_extra:\n",
    "            image = self.transform_extra(image)\n",
    "        else:\n",
    "            image = self.transform_main(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "class CLAHETransform:\n",
    "    def __init__(self, clip_limit=2.0, tile_grid_size=(8, 8)):\n",
    "        self.clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n",
    "    def __call__(self, img):\n",
    "        img_np = np.array(img); img_lab = cv2.cvtColor(img_np, cv2.COLOR_RGB2Lab)\n",
    "        l, a, b = cv2.split(img_lab); l_clahe = self.clahe.apply(l)\n",
    "        img_lab_clahe = cv2.merge((l_clahe, a, b))\n",
    "        img_rgb_clahe = cv2.cvtColor(img_lab_clahe, cv2.COLOR_Lab2RGB)\n",
    "        return Image.fromarray(img_rgb_clahe)\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_column, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_column = image_column\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.dataframe.iloc[idx][self.image_column]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, img_path\n",
    "\n",
    "# ===================================================================\n",
    "# === 4. PERSIAPAN DATA DENGAN STRATEGI \"VALIDASI BERSIH\" (DIUBAH) ===\n",
    "# ===================================================================\n",
    "def get_phash(filepath):\n",
    "    try:\n",
    "        with Image.open(filepath) as img: return imagehash.phash(img)\n",
    "    except Exception: return None\n",
    "\n",
    "# --- Langkah 4.1: Memuat dan memfilter data training secara manual ---\n",
    "train_df = convert_path_to_df(TRAIN_DIR)\n",
    "\n",
    "print(f\"Jumlah data training sebelum filtering manual: {len(train_df)}\")\n",
    "\n",
    "\n",
    "# --- Langkah 4.2: Identifikasi Kebocoran dan Pisahkan Data ---\n",
    "test_df = convert_path_to_df(TEST_DIR, is_test=True)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Mengidentifikasi kebocoran data (train vs test)...\")\n",
    "tqdm.pandas(desc=\"Menghitung Hash Data Test\")\n",
    "test_hashes = set(test_df['Filepath'].progress_apply(get_phash))\n",
    "test_hashes.discard(None)\n",
    "\n",
    "tqdm.pandas(desc=\"Menghitung Hash Data Train\")\n",
    "train_df['hash'] = train_df['Filepath'].progress_apply(get_phash)\n",
    "\n",
    "train_df['is_leak'] = train_df['hash'].isin(test_hashes)\n",
    "print(f\"Ditemukan {train_df['is_leak'].sum()} gambar di training set yang identik dengan gambar di test set.\")\n",
    "\n",
    "print(\"\\nMenerapkan strategi 'Validasi Bersih'...\")\n",
    "leaked_df = train_df[train_df['is_leak']].copy()\n",
    "clean_df = train_df[~train_df['is_leak']].copy()\n",
    "\n",
    "# --- Langkah 4.3: Pemetaan Label dan Split Data ---\n",
    "label_mapping = {\n",
    "    \"Ayam Bakar\": 0,\n",
    "    \"Ayam Betutu\": 1,\n",
    "    \"Ayam Goreng\": 2,\n",
    "    \"Ayam Pop\": 3,\n",
    "    \"Bakso\": 4,\n",
    "    \"Coto Makassar\": 5,\n",
    "    \"Gado Gado\": 6,\n",
    "    \"Gudeg\": 7,\n",
    "    \"Nasi Goreng\": 8,\n",
    "    \"Pempek\": 9,\n",
    "    \"Rawon\": 10,\n",
    "    \"Rendang\": 11,\n",
    "    \"Sate Madura\": 12,\n",
    "    \"Sate Padang\": 13,\n",
    "    \"Soto\": 14\n",
    "}\n",
    "\n",
    "clean_df['Label'] = clean_df['Label'].map(label_mapping)\n",
    "leaked_df['Label'] = leaked_df['Label'].map(label_mapping)\n",
    "\n",
    "val_split = pd.DataFrame()\n",
    "if not clean_df.empty:\n",
    "    try:\n",
    "        clean_train_split, val_split = train_test_split(\n",
    "            clean_df, test_size=0.2, random_state=42, stratify=clean_df['Label'])\n",
    "    except ValueError:\n",
    "        print(\"Peringatan: Gagal stratify, menggunakan split biasa.\")\n",
    "        clean_train_split, val_split = train_test_split(clean_df, test_size=0.2, random_state=42)\n",
    "else:\n",
    "    clean_train_split = clean_df\n",
    "\n",
    "train_split = pd.concat([clean_train_split, leaked_df], ignore_index=True)\n",
    "train_split.drop(columns=['hash', 'is_leak'], inplace=True)\n",
    "val_split.drop(columns=['hash', 'is_leak'], inplace=True)\n",
    "print(f\"Ukuran set training final (sisa bersih + semua bocor): {len(train_split)}\")\n",
    "print(f\"Ukuran set validasi murni (hanya dari data bersih): {len(val_split)}\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# --- Langkah 4.4: Lanjutkan dengan pipeline seperti biasa ---\n",
    "train_transform = transforms.Compose([\n",
    "    CLAHETransform(),\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN,std=IMAGENET_STD)\n",
    "])\n",
    "# Transform tambahan: rotasi 90° kiri/kanan\n",
    "train_transform_extra = transforms.Compose([\n",
    "    CLAHETransform(),\n",
    "    transforms.RandomChoice([\n",
    "        transforms.RandomRotation((90, 90)),\n",
    "        transforms.RandomRotation((-90, -90))\n",
    "    ]),\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    CLAHETransform(),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "train_dataset = DualTransformDataset(\n",
    "    train_split,\n",
    "    image_column='Filepath',\n",
    "    label_column='Label',\n",
    "    transform_main=train_transform,\n",
    "    transform_extra=train_transform_extra\n",
    ")\n",
    "\n",
    "val_dataset = CustomDataset(val_split, 'Filepath', 'Label', val_test_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, persistent_workers=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, persistent_workers=True)\n",
    "# PASTIKAN SEMUA LABEL BERTIPE INTEGER\n",
    "train_split[\"Label\"] = train_split[\"Label\"].astype(int)\n",
    "\n",
    "# Kode Anda yang sebelumnya\n",
    "class_weights = compute_class_weight(\"balanced\", classes=np.unique(train_split[\"Label\"]), y=train_split[\"Label\"])\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(DEVICE)\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# === 5. DEFINISI MODEL (TIDAK DIUBAH) ===\n",
    "# ===================================================================\n",
    "class FusionDINOv2(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.dinov2 = timm.create_model(\"vit_base_patch14_dinov2\", pretrained=True, num_classes=0)\n",
    "        self.convnext = timm.create_model(\"convnext_tiny\", pretrained=True, num_classes=0)\n",
    "        fusion_dim = self.dinov2.num_features + self.convnext.num_features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        feat_dino = self.dinov2(x)\n",
    "        feat_conv = self.convnext(x)\n",
    "        feat_combined = torch.cat((feat_dino, feat_conv), dim=1)\n",
    "        out = self.classifier(feat_combined)\n",
    "        return out\n",
    "class SingleSwinModel(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(\"swin_base_patch4_window7_224\", pretrained=True, num_classes=num_classes)\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# ===================================================================\n",
    "# === 6. INISIALISASI MODEL, LOSS, DAN SCALER (TIDAK DIUBAH) ===\n",
    "# ===================================================================\n",
    "model = FusionDINOv2(num_classes=len(label_mapping)).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "scaler = torch.amp.GradScaler()\n",
    "best_f1 = 0.0\n",
    "\n",
    "# ==================================================================================\n",
    "# === 7. STRATEGI TRAINING: GRADUAL UNFREEZING (TIDAK DIUBAH) ===\n",
    "#==================================================================================\n",
    "# --- TAHAP 1: Latih hanya Classifier Head ---\n",
    "print(\"\\n\" + \"=\"*50 + \"\\nTAHAP 1: Melatih Classifier Head\\n\" + \"=\"*50)\n",
    "for param in model.dinov2.parameters(): param.requires_grad = False\n",
    "for param in model.convnext.parameters(): param.requires_grad = False\n",
    "for param in model.classifier.parameters(): param.requires_grad = True\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LR_S1)\n",
    "for epoch in range(EPOCHS_S1):\n",
    "    model.train()\n",
    "    progress_bar = tqdm(train_loader, desc=f\"S1 Epoch {epoch+1}/{EPOCHS_S1}\")\n",
    "    for images, labels in progress_bar:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.amp.autocast(device_type = 'cuda'):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "# --- TAHAP 2: Latih Head + Setengah Atas DINOv2 ---\n",
    "print(\"\\n\" + \"=\"*50 + \"\\nTAHAP 2: Melatih Head + Setengah Atas DINOv2\\n\" + \"=\"*50)\n",
    "total_blocks = len(model.dinov2.blocks)\n",
    "for i in range(total_blocks // 2, total_blocks):\n",
    "    for param in model.dinov2.blocks[i].parameters():\n",
    "        param.requires_grad = True\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LR_S2)\n",
    "for epoch in range(EPOCHS_S2):\n",
    "    model.train()\n",
    "    progress_bar = tqdm(train_loader, desc=f\"S2 Epoch {epoch+1}/{EPOCHS_S2}\")\n",
    "    for images, labels in progress_bar:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.amp.autocast(device_type = 'cuda'):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\nTAHAP 3: Fine-tuning Seluruh Model\\n\" + \"=\"*50)\n",
    "for param in model.parameters(): param.requires_grad = True\n",
    "optimizer = optim.AdamW(model.parameters(), weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, \n",
    "    max_lr=LR_S3, \n",
    "    epochs=EPOCHS_S3, \n",
    "    steps_per_epoch=len(train_loader)\n",
    ")\n",
    "\n",
    "for epoch in range(EPOCHS_S3):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_bar = tqdm(train_loader, desc=f\"S3 Epoch {epoch+1}/{EPOCHS_S3} (Train)\")\n",
    "    for images, labels in train_bar:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.amp.autocast(device_type = 'cuda'):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss, all_preds, all_labels = 0.0, [], []\n",
    "    val_bar = tqdm(val_loader, desc=f\"S3 Epoch {epoch+1}/{EPOCHS_S3} (Val)\")\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_bar:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            with torch.amp.autocast(device_type = 'cuda'):\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "    \n",
    "    print(f\"\\nEpoch [{epoch+1}/{EPOCHS_S3}] | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Macro F1: {f1:.4f}\")\n",
    "    \n",
    "    report = classification_report(all_labels, all_preds, target_names=list(label_mapping.keys()))\n",
    "    print(\"\\n--- Laporan Klasifikasi Validasi ---\")\n",
    "    print(report)\n",
    "    \n",
    "    \n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        torch.save(model.state_dict(), \"best_gradual_unfreeze_model_one.pth\")\n",
    "        print(f\"✅ Model disimpan (F1 terbaik baru: {best_f1:.4f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efd591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################\n",
    "### BLOK C: FINAL INFERENCE & SUBMISSION (FusionDINOv2 SAJA) ###\n",
    "###################################################################################\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"### MEMULAI BLOK C: INFERENCE & SUBMISSION (FusionDINOv2 SAJA) ###\")\n",
    "print(\"#\"*80 + \"\\n\")\n",
    "\n",
    "# C.1. Inisialisasi model FusionDINOv2\n",
    "model_fusion = FusionDINOv2(num_classes=len(label_mapping)).to(DEVICE)\n",
    "\n",
    "# C.2. Muat bobot terbaik\n",
    "MODEL_PATH = \"best_gradual_unfreeze_model_one.pth\"\n",
    "print(f\"Memuat bobot dari: {MODEL_PATH}\")\n",
    "model_fusion.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "model_fusion.eval()\n",
    "\n",
    "# C.3. Siapkan Test Loader (resolusi tinggi)\n",
    "final_test_transform = transforms.Compose([\n",
    "    CLAHETransform(),\n",
    "    transforms.Resize((518, 518)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "test_df = convert_path_to_df(TEST_DIR, is_test=True)\n",
    "test_dataset = TestDataset(test_df, 'Filepath', transform=final_test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "reverse_label_mapping = {v: k for k, v in label_mapping.items()}\n",
    "\n",
    "# C.4. Inference\n",
    "submission_data = []\n",
    "with torch.no_grad():\n",
    "    for images, paths in tqdm(test_loader, desc=\"Inferensi FusionDINOv2\"):\n",
    "        images = images.to(DEVICE)\n",
    "        outputs = model_fusion(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        for i, path in enumerate(paths):\n",
    "            img_id = os.path.splitext(os.path.basename(path))[0]\n",
    "            label_int = preds[i].item()\n",
    "            label_str = reverse_label_mapping[label_int]\n",
    "            submission_data.append({'id': img_id, 'style': label_str})\n",
    "\n",
    "# C.5. Simpan hasil akhir\n",
    "print(\"\\nMenyimpan hasil prediksi ke submission_fusion_only.csv...\")\n",
    "submission_df = pd.DataFrame(submission_data)\n",
    "submission_df.sort_values(by='id', inplace=True)\n",
    "submission_df.to_csv(\"submission_fusion_only.csv\", index=False)\n",
    "\n",
    "print(\"✅ File submission_fusion_only.csv berhasil dibuat!\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\nPROSES SELESAI (FUSIONDINOv2 SAJA)\\n\" + \"=\"*50)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
