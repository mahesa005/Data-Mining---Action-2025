{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T15:31:55.624598Z",
     "iopub.status.busy": "2025-11-08T15:31:55.624416Z",
     "iopub.status.idle": "2025-11-08T15:31:59.848089Z",
     "shell.execute_reply": "2025-11-08T15:31:59.847331Z",
     "shell.execute_reply.started": "2025-11-08T15:31:55.624582Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    # Python built-in random\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Numpy\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if using multi-GPU\n",
    "    \n",
    "    # For reproducibility\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # Hash-based operations\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "# Call this once at the very top of your notebook\n",
    "set_seed(42)\n",
    "import random\n",
    "\n",
    "# --- TAMBAHKAN BLOK INI UNTUK REPRODUCIBILITY ---\n",
    "SEED = 42\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) # jika menggunakan multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T15:31:59.850149Z",
     "iopub.status.busy": "2025-11-08T15:31:59.849794Z",
     "iopub.status.idle": "2025-11-08T15:32:55.330988Z",
     "shell.execute_reply": "2025-11-08T15:32:55.330349Z",
     "shell.execute_reply.started": "2025-11-08T15:31:59.850112Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menggunakan device: cuda\n",
      "Ukuran gambar: 518x518, Batch Size: 8\n",
      "Jumlah data training before filtering manual: 4052\n",
      "\n",
      "==================================================\n",
      "Mengidentifikasi kebocoran data (train vs test)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d4865e0e2ca4427b9615d549ec3376c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Menghitung Hash Data Test:   0%|          | 0/2057 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# === 1. IMPORT LIBRARY ===\n",
    "# ===================================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "import cv2\n",
    "import timm\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import imagehash\n",
    "\n",
    "# ===================================================================\n",
    "# === 2. KONFIGURASI DAN SETUP (TIDAK DIUBAH) ===\n",
    "# ===================================================================\n",
    "# Direktori\n",
    "BASE_DIR = os.getcwd()  # Menggunakan current working directory\n",
    "TRAIN_DIR = os.path.join(BASE_DIR, \"dataset fixx\", \"train\")\n",
    "TEST_DIR = os.path.join(BASE_DIR, \"dataset fixx\", \"test\", \"test\")\n",
    "\n",
    "# Konfigurasi Model dan Training\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMG_SIZE = 518\n",
    "BATCH_SIZE = 8\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "# --- HYPERPARAMETER UNTUK GRADUAL UNFREEZING ---\n",
    "EPOCHS_S1 = 3\n",
    "LR_S1 = 3e-4\n",
    "EPOCHS_S2 = 5\n",
    "LR_S2 = 5e-5\n",
    "EPOCHS_S3 = 12\n",
    "LR_S3 = 1.59e-06 # <-- Anda mungkin perlu MENYESUAIKAN ini untuk model baru\n",
    "\n",
    "# Statistik ImageNet\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "print(f\"Menggunakan device: {DEVICE}\")\n",
    "print(f\"Ukuran gambar: {IMG_SIZE}x{IMG_SIZE}, Batch Size: {BATCH_SIZE}\")\n",
    "\n",
    "# ===================================================================\n",
    "# === 3. FUNGSI DAN KELAS HELPER (TIDAK DIUBAH) ===\n",
    "# ===================================================================\n",
    "def convert_path_to_df(dataset, is_test=False):\n",
    "    image_dir = Path(dataset)\n",
    "    \n",
    "    # 1. Tentukan ekstensi gambar yang valid (dalam huruf kecil)\n",
    "    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.gif', '.tif', '.tiff'}\n",
    "\n",
    "    # 2. Ambil semua file path terlebih dahulu\n",
    "    all_filepaths = list(image_dir.glob(r'**/*.*'))\n",
    "    \n",
    "    # 3. Filter daftar file, hanya ambil yang ekstensinya ada di set\n",
    "    filepaths = [p for p in all_filepaths if p.suffix.lower() in image_extensions]\n",
    "    \n",
    "    # Cek jika tidak ada gambar yang ditemukan\n",
    "    if not filepaths:\n",
    "        print(f\"Peringatan: Tidak ada file gambar yang ditemukan di {image_dir}\")\n",
    "        if is_test:\n",
    "            return pd.DataFrame(columns=['Filepath'])\n",
    "        else:\n",
    "            return pd.DataFrame(columns=['Filepath', 'Label'])\n",
    "\n",
    "    # 4. Sisa logika Anda tetap sama\n",
    "    if is_test:\n",
    "        filepaths = pd.Series(filepaths, name='Filepath').astype(str)\n",
    "        return pd.DataFrame({'Filepath': filepaths})\n",
    "    else:\n",
    "        labels = [p.parts[-2] for p in filepaths]\n",
    "        filepaths = pd.Series(filepaths, name='Filepath').astype(str)\n",
    "        labels = pd.Series(labels, name='Label')\n",
    "        return pd.concat([filepaths, labels], axis=1)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_column, label_column=None, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_column = image_column\n",
    "        self.label_column = label_column\n",
    "        self.transform = transform\n",
    "    def __len__(self): return len(self.dataframe)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.dataframe.iloc[idx][self.image_column]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform: image = self.transform(image)\n",
    "        if self.label_column:\n",
    "            label = self.dataframe.iloc[idx][self.label_column]\n",
    "            return image, torch.tensor(label, dtype=torch.long)\n",
    "        return image\n",
    "class DualTransformDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_column, label_column, transform_main, transform_extra):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_column = image_column\n",
    "        self.label_column = label_column\n",
    "        self.transform_main = transform_main\n",
    "        self.transform_extra = transform_extra\n",
    "\n",
    "    def __len__(self):\n",
    "        # Gandakan ukuran dataset (dua versi per gambar)\n",
    "        return len(self.dataframe) * 2\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Tentukan apakah pakai augmentasi utama atau tambahan\n",
    "        base_idx = idx // 2\n",
    "        use_extra = idx % 2 == 1\n",
    "\n",
    "        row = self.dataframe.iloc[base_idx]\n",
    "        img_path = row[self.image_column]\n",
    "        label = torch.tensor(row[self.label_column], dtype=torch.long)\n",
    "\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if use_extra:\n",
    "            image = self.transform_extra(image)\n",
    "        else:\n",
    "            image = self.transform_main(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "class CLAHETransform:\n",
    "    def __init__(self, clip_limit=2.0, tile_grid_size=(8, 8)):\n",
    "        self.clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n",
    "    def __call__(self, img):\n",
    "        img_np = np.array(img); img_lab = cv2.cvtColor(img_np, cv2.COLOR_RGB2Lab)\n",
    "        l, a, b = cv2.split(img_lab); l_clahe = self.clahe.apply(l)\n",
    "        img_lab_clahe = cv2.merge((l_clahe, a, b))\n",
    "        img_rgb_clahe = cv2.cvtColor(img_lab_clahe, cv2.COLOR_Lab2RGB)\n",
    "        return Image.fromarray(img_rgb_clahe)\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_column, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_column = image_column\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.dataframe.iloc[idx][self.image_column]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, img_path\n",
    "\n",
    "# ===================================================================\n",
    "# === 4. PERSIAPAN DATA DENGAN STRATEGI \"VALIDASI BERSIH\" (TIDAK DIUBAH) ===\n",
    "# ===================================================================\n",
    "def get_phash(filepath):\n",
    "    try:\n",
    "        with Image.open(filepath) as img: return imagehash.phash(img)\n",
    "    except Exception: return None\n",
    "\n",
    "# --- Langkah 4.1: Memuat dan memfilter data training secara manual ---\n",
    "train_df = convert_path_to_df(TRAIN_DIR)\n",
    "\n",
    "print(f\"Jumlah data training before filtering manual: {len(train_df)}\")\n",
    "\n",
    "\n",
    "# --- Langkah 4.2: Identifikasi Kebocoran dan Pisahkan Data ---\n",
    "test_df = convert_path_to_df(TEST_DIR, is_test=True)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Mengidentifikasi kebocoran data (train vs test)...\")\n",
    "tqdm.pandas(desc=\"Menghitung Hash Data Test\")\n",
    "test_hashes = set(test_df['Filepath'].progress_apply(get_phash))\n",
    "test_hashes.discard(None)\n",
    "\n",
    "tqdm.pandas(desc=\"Menghitung Hash Data Train\")\n",
    "train_df['hash'] = train_df['Filepath'].progress_apply(get_phash)\n",
    "\n",
    "train_df['is_leak'] = train_df['hash'].isin(test_hashes)\n",
    "print(f\"Ditemukan {train_df['is_leak'].sum()} gambar di training set yang identik dengan gambar di test set.\")\n",
    "\n",
    "print(\"\\nMenerapkan strategi 'Validasi Bersih'...\")\n",
    "leaked_df = train_df[train_df['is_leak']].copy()\n",
    "clean_df = train_df[~train_df['is_leak']].copy()\n",
    "\n",
    "# --- Langkah 4.3: Pemetaan Label dan Split Data ---\n",
    "label_mapping = {\n",
    "    \"Ayam Bakar\": 0,\n",
    "    \"Ayam Betutu\": 1,\n",
    "    \"Ayam Goreng\": 2,\n",
    "    \"Ayam Pop\": 3,\n",
    "    \"Bakso\": 4,\n",
    "    \"Coto Makassar\": 5,\n",
    "    \"Gado Gado\": 6,\n",
    "    \"Gudeg\": 7,\n",
    "    \"Nasi Goreng\": 8,\n",
    "    \"Pempek\": 9,\n",
    "    \"Rawon\": 10,\n",
    "    \"Rendang\": 11,\n",
    "    \"Sate Madura\": 12,\n",
    "    \"Sate Padang\": 13,\n",
    "    \"Soto\": 14\n",
    "}\n",
    "\n",
    "clean_df['Label'] = clean_df['Label'].map(label_mapping)\n",
    "leaked_df['Label'] = leaked_df['Label'].map(label_mapping)\n",
    "\n",
    "val_split = pd.DataFrame()\n",
    "if not clean_df.empty:\n",
    "    try:\n",
    "        clean_train_split, val_split = train_test_split(\n",
    "            clean_df, test_size=0.2, random_state=42, stratify=clean_df['Label'])\n",
    "    except ValueError:\n",
    "        print(\"Peringatan: Gagal stratify, menggunakan split biasa.\")\n",
    "        clean_train_split, val_split = train_test_split(clean_df, test_size=0.2, random_state=42)\n",
    "else:\n",
    "    clean_train_split = clean_df\n",
    "\n",
    "train_split = pd.concat([clean_train_split, leaked_df], ignore_index=True)\n",
    "train_split.drop(columns=['hash', 'is_leak'], inplace=True)\n",
    "val_split.drop(columns=['hash', 'is_leak'], inplace=True)\n",
    "print(f\"Ukuran set training final (sisa bersih + semua bocor): {len(train_split)}\")\n",
    "print(f\"Ukuran set validasi murni (hanya dari data bersih): {len(val_split)}\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# --- Langkah 4.4: Lanjutkan dengan pipeline seperti biasa ---\n",
    "train_transform = transforms.Compose([\n",
    "    CLAHETransform(),\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN,std=IMAGENET_STD)\n",
    "])\n",
    "# Transform tambahan: rotasi 90° kiri/kanan\n",
    "train_transform_extra = transforms.Compose([\n",
    "    CLAHETransform(),\n",
    "    transforms.RandomChoice([\n",
    "        transforms.RandomRotation((90, 90)),\n",
    "        transforms.RandomRotation((-90, -90))\n",
    "    ]),\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    CLAHETransform(),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "train_dataset = DualTransformDataset(\n",
    "    train_split,\n",
    "    image_column='Filepath',\n",
    "    label_column='Label',\n",
    "    transform_main=train_transform,\n",
    "    transform_extra=train_transform_extra\n",
    ")\n",
    "\n",
    "val_dataset = CustomDataset(val_split, 'Filepath', 'Label', val_test_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, persistent_workers=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, persistent_workers=True)\n",
    "# PASTIKAN SEMUA LABEL BERTIPE INTEGER\n",
    "train_split[\"Label\"] = train_split[\"Label\"].astype(int)\n",
    "\n",
    "# Kode Anda yang sebelumnya\n",
    "class_weights = compute_class_weight(\"balanced\", classes=np.unique(train_split[\"Label\"]), y=train_split[\"Label\"])\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(DEVICE)\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# === 5. DEFINISI MODEL (DIUBAH) ===\n",
    "# ===================================================================\n",
    "\n",
    "# --- MODEL BARU: SINGLE DINOV2 ---\n",
    "class SingleDINOv2Model(nn.Module):\n",
    "    def __init__(self, num_classes=5): # num_classes akan diisi oleh len(label_mapping)\n",
    "        super().__init__()\n",
    "        # timm akan otomatis menambahkan head klasifikasi untuk kita\n",
    "        self.backbone = timm.create_model(\n",
    "            \"vit_base_patch14_dinov2\", \n",
    "            pretrained=True, \n",
    "            num_classes=num_classes \n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# --- MODEL LAMA (Untuk referensi) ---\n",
    "# class FusionDINOv2(nn.Module):\n",
    "#     def __init__(self, num_classes=5):\n",
    "#         super().__init__()\n",
    "#         self.dinov2 = timm.create_model(\"vit_base_patch14_dinov2\", pretrained=True, num_classes=0)\n",
    "#         self.convnext = timm.create_model(\"convnext_tiny\", pretrained=True, num_classes=0)\n",
    "#         fusion_dim = self.dinov2.num_features + self.convnext.num_features\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Linear(fusion_dim, 512),\n",
    "#             nn.LayerNorm(512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(512, num_classes)\n",
    "#         )\n",
    "#     def forward(self, x):\n",
    "#         feat_dino = self.dinov2(x)\n",
    "#         feat_conv = self.convnext(x)\n",
    "#         feat_combined = torch.cat((feat_dino, feat_conv), dim=1)\n",
    "#         out = self.classifier(feat_combined)\n",
    "#         return out\n",
    "        \n",
    "class SingleSwinModel(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(\"swin_base_patch4_window7_224\", pretrained=True, num_classes=num_classes)\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T15:35:04.042571Z",
     "iopub.status.busy": "2025-11-08T15:35:04.042295Z",
     "iopub.status.idle": "2025-11-08T17:07:27.598370Z",
     "shell.execute_reply": "2025-11-08T17:07:27.597318Z",
     "shell.execute_reply.started": "2025-11-08T15:35:04.042551Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "### MEMULAI BLOK B: TRAINING MODEL EVA TRANSFORMER (MIXUP/CUTMIX) ###\n",
      "################################################################################\n",
      "\n",
      "Menggunakan Ukuran Gambar: 224x224, Batch Size: 16\n",
      "Menggunakan model: eva02_base_patch14_224\n",
      "Menginisialisasi Mixup/CutMix...\n",
      "\n",
      "==================================================\n",
      "TAHAP 1 (EVA): Melatih Classifier Head\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c54fb2965488486dbeb0deec420465a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S1 EVA Epoch 1/3:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f73bb5a894e74c8ca8056b273bc72480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S1 EVA Epoch 2/3:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4de295c68913495db83aabc5ccebabc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S1 EVA Epoch 3/3:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TAHAP 2 (EVA): Melatih Head + Blok Atas\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5fa541c03334054b96539700620a0fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S2 EVA Epoch 1/5:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1754b52d4f6b4af89d0341d4bc24d95f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S2 EVA Epoch 2/5:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d144f80a59d94c52bf39c3cbc9cbb733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S2 EVA Epoch 3/5:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "439f57c849cd4a069bfba38deb03dea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S2 EVA Epoch 4/5:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b1202c2e08b483e95b44b8629f94a05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S2 EVA Epoch 5/5:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAHAP 3 akan berjalan selama 25 epochs (lebih lama untuk Mixup/CutMix)\n",
      "\n",
      "==================================================\n",
      "TAHAP 3 (EVA): Fine-tuning Seluruh Model (dengan Mixup/CutMix)\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec1c8d117a24448789786aa5b47390e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 EVA Epoch 1/25 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/25] | LR: 9.96e-06 | Val F1: 0.9302| Val acc: 0.9308\n",
      "✅ Model EVA disimpan ke best_eva_standalone.pth (F1 terbaik baru: 0.9308)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b64b8fdc3b14f8bb43e47d9a23e5b94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 EVA Epoch 2/25 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [2/25] | LR: 9.86e-06 | Val F1: 0.9340| Val acc: 0.9346\n",
      "✅ Model EVA disimpan ke best_eva_standalone.pth (F1 terbaik baru: 0.9346)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eabcae9e79614087be48ca875fa63b3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 EVA Epoch 3/25 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [3/25] | LR: 9.68e-06 | Val F1: 0.9335| Val acc: 0.9346\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48e151582ffb4099836a942908d633aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 EVA Epoch 4/25 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [4/25] | LR: 9.44e-06 | Val F1: 0.9425| Val acc: 0.9434\n",
      "✅ Model EVA disimpan ke best_eva_standalone.pth (F1 terbaik baru: 0.9434)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b1a5509fcc40f5bcb6c62f6ed96f83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 EVA Epoch 5/25 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [5/25] | LR: 9.14e-06 | Val F1: 0.9363| Val acc: 0.9371\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4403f5edec3a454d894d2819b957171d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 EVA Epoch 6/25 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [6/25] | LR: 8.78e-06 | Val F1: 0.9443| Val acc: 0.9447\n",
      "✅ Model EVA disimpan ke best_eva_standalone.pth (F1 terbaik baru: 0.9447)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dfe723b98c74dd19b9203481eeda26b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 EVA Epoch 7/25 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [7/25] | LR: 8.37e-06 | Val F1: 0.9419| Val acc: 0.9421\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a63abdde42d4cf7917f1ed753b0d949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 EVA Epoch 8/25 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8/25] | LR: 7.91e-06 | Val F1: 0.9393| Val acc: 0.9396\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1114e48f2fde4d03a7a74de3a991bb45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 EVA Epoch 9/25 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [9/25] | LR: 7.42e-06 | Val F1: 0.9442| Val acc: 0.9447\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "330a2ecb5e7847009561eb61c20f8d94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 EVA Epoch 10/25 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [11/25] | LR: 6.34e-06 | Val F1: 0.9457| Val acc: 0.9459\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "294408be9f8d47f78755b2999dd6d1ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 EVA Epoch 12/25 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [12/25] | LR: 5.78e-06 | Val F1: 0.9484| Val acc: 0.9484\n",
      "✅ Model EVA disimpan ke best_eva_standalone.pth (F1 terbaik baru: 0.9484)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5334e0cc83424ed3a157a0c9ee0ad533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 EVA Epoch 13/25 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [13/25] | LR: 5.22e-06 | Val F1: 0.9428| Val acc: 0.9434\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9dac45fb92c408dadc64b1454479006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 EVA Epoch 14/25 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [14/25] | LR: 4.66e-06 | Val F1: 0.9442| Val acc: 0.9447\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd36aeb9ab554628ba7c7608b3f2c5cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 EVA Epoch 15/25 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [15/25] | LR: 4.11e-06 | Val F1: 0.9532| Val acc: 0.9535\n",
      "✅ Model EVA disimpan ke best_eva_standalone.pth (F1 terbaik baru: 0.9535)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c5cc39b71e34c4a9ad92c9703e63503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 EVA Epoch 16/25 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [16/25] | LR: 3.58e-06 | Val F1: 0.9454| Val acc: 0.9459\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a430f4d72d94453bd5907206325aa1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 EVA Epoch 17/25 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [17/25] | LR: 3.09e-06 | Val F1: 0.9481| Val acc: 0.9484\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ce5a2f907954a388841a2b4c540c784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 EVA Epoch 18/25 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [18/25] | LR: 2.63e-06 | Val F1: 0.9455| Val acc: 0.9459\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9ce01fae0214705a82f994f3628c382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 EVA Epoch 19/25 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [19/25] | LR: 2.22e-06 | Val F1: 0.9458| Val acc: 0.9459\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b15e23938f1247f1b84dcb1af4a61a7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 EVA Epoch 20/25 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [20/25] | LR: 1.86e-06 | Val F1: 0.9482| Val acc: 0.9484\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d34e23df5a5244c78153536634031d02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 EVA Epoch 21/25 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [21/25] | LR: 1.56e-06 | Val F1: 0.9494| Val acc: 0.9497\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3d7a6a3cf764ae4b4051795c2e59c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 EVA Epoch 22/25 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [22/25] | LR: 1.32e-06 | Val F1: 0.9482| Val acc: 0.9484\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e9133a249a6478d94d0c3d23b26e105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 EVA Epoch 23/25 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [23/25] | LR: 1.14e-06 | Val F1: 0.9484| Val acc: 0.9484\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aba0debadf6e4d0f9342c6b0650273d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 EVA Epoch 24/25 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [24/25] | LR: 1.04e-06 | Val F1: 0.9495| Val acc: 0.9497\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df519b51c35d459087d0374113c69f3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 EVA Epoch 25/25 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [25/25] | LR: 1.00e-06 | Val F1: 0.9522| Val acc: 0.9522\n",
      "\n",
      "Training Model EVA Selesai. Bobot terbaik disimpan di best_eva_standalone.pth\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# === 1. TAMBAHKAN IMPORT INI DI BAGIAN ATAS SKRIP ANDA ===\n",
    "# ===================================================================\n",
    "from timm.data.mixup import Mixup\n",
    "# (Asumsi impor lain seperti torch, timm, dll sudah ada)\n",
    "# ...\n",
    "import torch.nn.functional as F\n",
    "\n",
    "###################################################################################\n",
    "### BLOK B: MELATIH MODEL EVA TRANSFORMER (DENGAN MIXUP/CUTMIX)\n",
    "###################################################################################\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"### MEMULAI BLOK B: TRAINING MODEL EVA TRANSFORMER (MIXUP/CUTMIX) ###\")\n",
    "print(\"#\"*80 + \"\\n\")\n",
    "\n",
    "# B.1. Konfigurasi khusus untuk Model EVA\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 16\n",
    "BEST_MODEL_NAME_EVA = \"best_eva_standalone.pth\" \n",
    "print(f\"Menggunakan Ukuran Gambar: {IMG_SIZE}x{IMG_SIZE}, Batch Size: {BATCH_SIZE}\")\n",
    "\n",
    "# B.2. Buat Ulang Transforms dan DataLoaders\n",
    "# (Tidak ada perubahan di sini, kode Anda sudah benar)\n",
    "train_transform_main_224 = transforms.Compose([\n",
    "    CLAHETransform(),\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN,std=IMAGENET_STD)\n",
    "])\n",
    "train_transform_extra_224 = transforms.Compose([\n",
    "    CLAHETransform(),\n",
    "    transforms.RandomChoice([transforms.RandomRotation((90, 90)), transforms.RandomRotation((-90, -90))]),\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "val_test_transform_224 = transforms.Compose([\n",
    "    CLAHETransform(),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "train_dataset = DualTransformDataset(train_split, 'Filepath', 'Label', train_transform_main_224, train_transform_extra_224)\n",
    "val_dataset = CustomDataset(val_split, 'Filepath', 'Label', val_test_transform_224)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, persistent_workers=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, persistent_workers=True)\n",
    "\n",
    "\n",
    "# B.3. Inisialisasi Model\n",
    "class SingleEvaModel(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(\"eva02_base_patch14_224\", pretrained=True, num_classes=num_classes)\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "model = SingleEvaModel(num_classes=len(label_mapping)).to(DEVICE)\n",
    "print(f\"Menggunakan model: eva02_base_patch14_224\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights,label_smoothing=0.1)\n",
    "\n",
    "# --- TAMBAHKAN INISIALISASI MIXUP ---\n",
    "print(\"Menginisialisasi Mixup/CutMix...\")\n",
    "mixup_fn = Mixup(\n",
    "    mixup_alpha=0.8,    # Nilai standar timm\n",
    "    cutmix_alpha=1.0,   # Nilai standar timm\n",
    "    prob=1.0,           # Terapkan salah satu (mixup/cutmix) pada SETIAP batch\n",
    "    switch_prob=0.5,    # 50% Mixup, 50% CutMix\n",
    "    mode='batch',\n",
    "    label_smoothing=0.1,# Selaras dengan criterion Anda\n",
    "    num_classes=len(label_mapping)\n",
    ")\n",
    "# ------------------------------------\n",
    "\n",
    "scaler = torch.amp.GradScaler()\n",
    "best_f1 = 0.0\n",
    "best_acc = 0.0\n",
    "\n",
    "# B.4. Lakukan Training (3 Tahap disesuaikan untuk EVA)\n",
    "# --- TAHAP 1 (Tidak Berubah) ---\n",
    "print(\"\\n\" + \"=\"*50 + \"\\nTAHAP 1 (EVA): Melatih Classifier Head\\n\" + \"=\"*50)\n",
    "for param in model.backbone.parameters(): param.requires_grad = False\n",
    "for param in model.backbone.head.parameters(): param.requires_grad = True \n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LR_S1)\n",
    "for epoch in range(EPOCHS_S1):\n",
    "    model.train()\n",
    "    progress_bar = tqdm(train_loader, desc=f\"S1 EVA Epoch {epoch+1}/{EPOCHS_S1}\")\n",
    "    for images, labels in progress_bar:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        \n",
    "        # --- PERBAIKAN: Jangan gunakan Mixup di Tahap 1 ---\n",
    "        # Kita ingin head belajar pemetaan sederhana terlebih dahulu\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        with torch.amp.autocast(device_type = 'cuda'):\n",
    "            # CrossEntropyLoss TIDAK bisa menangani label [B] dan smoothing\n",
    "            # Jadi kita harus membuatnya one-hot manual untuk Tahap 1 & 2\n",
    "            labels_one_hot = F.one_hot(labels, num_classes=len(label_mapping)).float()\n",
    "            outputs = model(images); loss = criterion(outputs, labels_one_hot)\n",
    "            \n",
    "        scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()\n",
    "\n",
    "# --- TAHAP 2 (Tidak Berubah) ---\n",
    "print(\"\\n\" + \"=\"*50 + \"\\nTAHAP 2 (EVA): Melatih Head + Blok Atas\\n\" + \"=\"*50)\n",
    "for param in model.backbone.blocks[-1].parameters(): param.requires_grad = True\n",
    "for param in model.backbone.norm.parameters(): param.requires_grad = True\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LR_S2)\n",
    "for epoch in range(EPOCHS_S2):\n",
    "    model.train()\n",
    "    progress_bar = tqdm(train_loader, desc=f\"S2 EVA Epoch {epoch+1}/{EPOCHS_S2}\")\n",
    "    for images, labels in progress_bar:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        with torch.amp.autocast(device_type = 'cuda'):\n",
    "            # Gunakan one-hot yang sama\n",
    "            labels_one_hot = F.one_hot(labels, num_classes=len(label_mapping)).float()\n",
    "            outputs = model(images); loss = criterion(outputs, labels_one_hot)\n",
    "            \n",
    "        scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()\n",
    "        \n",
    "# --- TAHAP 3 (DIMODIFIKASI) ---\n",
    "\n",
    "# --- REKOMENDASI: TAMBAHKAN EPOCH KARENA MIXUP ---\n",
    "EPOCHS_S3_MIXUP = 25 # Coba 25-30 epoch, karena Mixup butuh waktu lebih lama\n",
    "print(f\"TAHAP 3 akan berjalan selama {EPOCHS_S3_MIXUP} epochs (lebih lama untuk Mixup/CutMix)\")\n",
    "# -------------------------------------------------\n",
    "\n",
    "LR_S3_EVA = 1e-5 \n",
    "print(\"\\n\" + \"=\"*50 + \"\\nTAHAP 3 (EVA): Fine-tuning Seluruh Model (dengan Mixup/CutMix)\\n\" + \"=\"*50)\n",
    "for param in model.parameters(): param.requires_grad = True\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR_S3_EVA, weight_decay=1e-4)\n",
    "\n",
    "# --- SESUAIKAN T_max DENGAN EPOCH BARU ---\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=EPOCHS_S3_MIXUP, # <-- GANTI\n",
    "    eta_min=1e-6    \n",
    ")\n",
    "# ----------------------------------------\n",
    "\n",
    "for epoch in range(EPOCHS_S3_MIXUP): # <-- GANTI\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_bar = tqdm(train_loader, desc=f\"S3 EVA Epoch {epoch+1}/{EPOCHS_S3_MIXUP} (Train)\") # <-- GANTI\n",
    "    for images, labels in train_bar:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        \n",
    "        # --- TERAPKAN MIXUP/CUTMIX DI SINI ---\n",
    "        images, labels = mixup_fn(images, labels)\n",
    "        # 'labels' sekarang menjadi [B, N_CLASSES] (soft labels)\n",
    "        # ------------------------------------\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        with torch.amp.autocast(device_type = 'cuda'):\n",
    "            outputs = model(images)\n",
    "            # 'criterion' bisa langsung menangani 'labels' soft [B, N_CLASSES]\n",
    "            loss = criterion(outputs, labels) \n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scaler.step(optimizer); scaler.update()\n",
    "        # scheduler.step() # <-- HAPUS DARI SINI\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # --- PINDAHKAN SCHEDULER.STEP() KE SINI ---\n",
    "    # Panggil scheduler HANYA sekali per epoch\n",
    "    scheduler.step()\n",
    "    # ---------------------------------------\n",
    "\n",
    "    model.eval()\n",
    "    val_loss, all_preds, all_labels = 0.0, [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            with torch.amp.autocast(device_type = 'cuda'):\n",
    "                # Untuk validasi, kita gunakan label asli (hard labels)\n",
    "                # jadi kita TIDAK perlu one-hot\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels) # <-- 'labels' di sini [B]\n",
    "            val_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy()); all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "    acc = accuracy_score(all_labels, all_preds) \n",
    "    \n",
    "    # Opsi: Tampilkan LR saat ini untuk memverifikasi\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"\\nEpoch [{epoch+1}/{EPOCHS_S3_MIXUP}] | LR: {current_lr:.2e} | Val F1: {f1:.4f}| Val acc: {acc:.4f}\")\n",
    "    \n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        torch.save(model.state_dict(), BEST_MODEL_NAME_EVA) \n",
    "        print(f\"✅ Model EVA disimpan ke {BEST_MODEL_NAME_EVA} (F1 terbaik baru: {best_acc:.4f})\") \n",
    "\n",
    "print(f\"\\nTraining Model EVA Selesai. Bobot terbaik disimpan di {BEST_MODEL_NAME_EVA}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-07T12:31:55.180488Z",
     "iopub.status.busy": "2025-11-07T12:31:55.179938Z",
     "iopub.status.idle": "2025-11-07T12:33:14.082577Z",
     "shell.execute_reply": "2025-11-07T12:33:14.081711Z",
     "shell.execute_reply.started": "2025-11-07T12:31:55.180466Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menggunakan device: cuda\n",
      "Ukuran gambar: 518x518, Batch Size: 8\n",
      "Jumlah data training sebelum filtering manual: 4052\n",
      "\n",
      "==================================================\n",
      "Mengidentifikasi kebocoran data (train vs test)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afef5f638e2349d8b0fe9bfd4a5a8e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Menghitung Hash Data Test:   0%|          | 0/2057 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c71ffff4c0494e28a88e2a4336add7f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Menghitung Hash Data Train:   0%|          | 0/4052 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ditemukan 80 gambar di training set yang identik dengan gambar di test set.\n",
      "\n",
      "Menerapkan strategi 'Validasi Bersih'...\n",
      "Ukuran set training final (sisa bersih + semua bocor): 3257\n",
      "Ukuran set validasi murni (hanya dari data bersih): 795\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a9513d798c84c968117566374fa9dca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a0b3e2ee8e54f1bb005ceda3a7576e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/114M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==================================================================\n",
    "# === 1. IMPORT LIBRARY ===\n",
    "# ===================================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "import cv2\n",
    "import timm\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import imagehash\n",
    "\n",
    "# ===================================================================\n",
    "# === 2. KONFIGURASI DAN SETUP (TIDAK DIUBAH) ===\n",
    "# ===================================================================\n",
    "# Direktori\n",
    "TRAIN_DIR = \"/kaggle/input/newdatasets/train\"\n",
    "TEST_DIR = \"/kaggle/input/newdatasets/test/test\"\n",
    "\n",
    "# Konfigurasi Model dan Training\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMG_SIZE = 518\n",
    "BATCH_SIZE = 8\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "# --- HYPERPARAMETER UNTUK GRADUAL UNFREEZING ---\n",
    "EPOCHS_S1 = 3\n",
    "LR_S1 = 3e-4\n",
    "EPOCHS_S2 = 5\n",
    "LR_S2 = 5e-5\n",
    "EPOCHS_S3 = 12\n",
    "LR_S3 = 1.59e-06\n",
    "\n",
    "# Statistik ImageNet\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "print(f\"Menggunakan device: {DEVICE}\")\n",
    "print(f\"Ukuran gambar: {IMG_SIZE}x{IMG_SIZE}, Batch Size: {BATCH_SIZE}\")\n",
    "\n",
    "# ===================================================================\n",
    "# === 3. FUNGSI DAN KELAS HELPER (TIDAK DIUBAH) ===\n",
    "# ===================================================================\n",
    "def convert_path_to_df(dataset, is_test=False):\n",
    "    image_dir = Path(dataset)\n",
    "    \n",
    "    # 1. Tentukan ekstensi gambar yang valid (dalam huruf kecil)\n",
    "    # Menggunakan set ( {} ) agar pencarian lebih cepat\n",
    "    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.gif', '.tif', '.tiff'}\n",
    "\n",
    "    # 2. Ambil semua file path terlebih dahulu\n",
    "    all_filepaths = list(image_dir.glob(r'**/*.*'))\n",
    "    \n",
    "    # 3. Filter daftar file, hanya ambil yang ekstensinya ada di set\n",
    "    # p.suffix akan mengambil ekstensi (mis: \".JPG\")\n",
    "    # .lower() mengubahnya jadi huruf kecil (mis: \".jpg\")\n",
    "    filepaths = [p for p in all_filepaths if p.suffix.lower() in image_extensions]\n",
    "    \n",
    "    # Cek jika tidak ada gambar yang ditemukan\n",
    "    if not filepaths:\n",
    "        print(f\"Peringatan: Tidak ada file gambar yang ditemukan di {image_dir}\")\n",
    "        if is_test:\n",
    "            return pd.DataFrame(columns=['Filepath'])\n",
    "        else:\n",
    "            return pd.DataFrame(columns=['Filepath', 'Label'])\n",
    "\n",
    "    # 4. Sisa logika Anda tetap sama\n",
    "    if is_test:\n",
    "        filepaths = pd.Series(filepaths, name='Filepath').astype(str)\n",
    "        return pd.DataFrame({'Filepath': filepaths})\n",
    "    else:\n",
    "        labels = [p.parts[-2] for p in filepaths]\n",
    "        filepaths = pd.Series(filepaths, name='Filepath').astype(str)\n",
    "        labels = pd.Series(labels, name='Label')\n",
    "        return pd.concat([filepaths, labels], axis=1)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_column, label_column=None, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_column = image_column\n",
    "        self.label_column = label_column\n",
    "        self.transform = transform\n",
    "    def __len__(self): return len(self.dataframe)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.dataframe.iloc[idx][self.image_column]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform: image = self.transform(image)\n",
    "        if self.label_column:\n",
    "            label = self.dataframe.iloc[idx][self.label_column]\n",
    "            return image, torch.tensor(label, dtype=torch.long)\n",
    "        return image\n",
    "class DualTransformDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_column, label_column, transform_main, transform_extra):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_column = image_column\n",
    "        self.label_column = label_column\n",
    "        self.transform_main = transform_main\n",
    "        self.transform_extra = transform_extra\n",
    "\n",
    "    def __len__(self):\n",
    "        # Gandakan ukuran dataset (dua versi per gambar)\n",
    "        return len(self.dataframe) * 2\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Tentukan apakah pakai augmentasi utama atau tambahan\n",
    "        base_idx = idx // 2\n",
    "        use_extra = idx % 2 == 1\n",
    "\n",
    "        row = self.dataframe.iloc[base_idx]\n",
    "        img_path = row[self.image_column]\n",
    "        label = torch.tensor(row[self.label_column], dtype=torch.long)\n",
    "\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if use_extra:\n",
    "            image = self.transform_extra(image)\n",
    "        else:\n",
    "            image = self.transform_main(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "class CLAHETransform:\n",
    "    def __init__(self, clip_limit=2.0, tile_grid_size=(8, 8)):\n",
    "        self.clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n",
    "    def __call__(self, img):\n",
    "        img_np = np.array(img); img_lab = cv2.cvtColor(img_np, cv2.COLOR_RGB2Lab)\n",
    "        l, a, b = cv2.split(img_lab); l_clahe = self.clahe.apply(l)\n",
    "        img_lab_clahe = cv2.merge((l_clahe, a, b))\n",
    "        img_rgb_clahe = cv2.cvtColor(img_lab_clahe, cv2.COLOR_Lab2RGB)\n",
    "        return Image.fromarray(img_rgb_clahe)\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_column, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_column = image_column\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.dataframe.iloc[idx][self.image_column]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, img_path\n",
    "\n",
    "# ===================================================================\n",
    "# === 4. PERSIAPAN DATA DENGAN STRATEGI \"VALIDASI BERSIH\" (DIUBAH) ===\n",
    "# ===================================================================\n",
    "def get_phash(filepath):\n",
    "    try:\n",
    "        with Image.open(filepath) as img: return imagehash.phash(img)\n",
    "    except Exception: return None\n",
    "\n",
    "# --- Langkah 4.1: Memuat dan memfilter data training secara manual ---\n",
    "train_df = convert_path_to_df(TRAIN_DIR)\n",
    "\n",
    "print(f\"Jumlah data training sebelum filtering manual: {len(train_df)}\")\n",
    "\n",
    "\n",
    "# --- Langkah 4.2: Identifikasi Kebocoran dan Pisahkan Data ---\n",
    "test_df = convert_path_to_df(TEST_DIR, is_test=True)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Mengidentifikasi kebocoran data (train vs test)...\")\n",
    "tqdm.pandas(desc=\"Menghitung Hash Data Test\")\n",
    "test_hashes = set(test_df['Filepath'].progress_apply(get_phash))\n",
    "test_hashes.discard(None)\n",
    "\n",
    "tqdm.pandas(desc=\"Menghitung Hash Data Train\")\n",
    "train_df['hash'] = train_df['Filepath'].progress_apply(get_phash)\n",
    "\n",
    "train_df['is_leak'] = train_df['hash'].isin(test_hashes)\n",
    "print(f\"Ditemukan {train_df['is_leak'].sum()} gambar di training set yang identik dengan gambar di test set.\")\n",
    "\n",
    "print(\"\\nMenerapkan strategi 'Validasi Bersih'...\")\n",
    "leaked_df = train_df[train_df['is_leak']].copy()\n",
    "clean_df = train_df[~train_df['is_leak']].copy()\n",
    "\n",
    "# --- Langkah 4.3: Pemetaan Label dan Split Data ---\n",
    "label_mapping = {\n",
    "    \"Ayam Bakar\": 0,\n",
    "    \"Ayam Betutu\": 1,\n",
    "    \"Ayam Goreng\": 2,\n",
    "    \"Ayam Pop\": 3,\n",
    "    \"Bakso\": 4,\n",
    "    \"Coto Makassar\": 5,\n",
    "    \"Gado Gado\": 6,\n",
    "    \"Gudeg\": 7,\n",
    "    \"Nasi Goreng\": 8,\n",
    "    \"Pempek\": 9,\n",
    "    \"Rawon\": 10,\n",
    "    \"Rendang\": 11,\n",
    "    \"Sate Madura\": 12,\n",
    "    \"Sate Padang\": 13,\n",
    "    \"Soto\": 14\n",
    "}\n",
    "\n",
    "clean_df['Label'] = clean_df['Label'].map(label_mapping)\n",
    "leaked_df['Label'] = leaked_df['Label'].map(label_mapping)\n",
    "\n",
    "val_split = pd.DataFrame()\n",
    "if not clean_df.empty:\n",
    "    try:\n",
    "        clean_train_split, val_split = train_test_split(\n",
    "            clean_df, test_size=0.2, random_state=42, stratify=clean_df['Label'])\n",
    "    except ValueError:\n",
    "        print(\"Peringatan: Gagal stratify, menggunakan split biasa.\")\n",
    "        clean_train_split, val_split = train_test_split(clean_df, test_size=0.2, random_state=42)\n",
    "else:\n",
    "    clean_train_split = clean_df\n",
    "\n",
    "train_split = pd.concat([clean_train_split, leaked_df], ignore_index=True)\n",
    "train_split.drop(columns=['hash', 'is_leak'], inplace=True)\n",
    "val_split.drop(columns=['hash', 'is_leak'], inplace=True)\n",
    "print(f\"Ukuran set training final (sisa bersih + semua bocor): {len(train_split)}\")\n",
    "print(f\"Ukuran set validasi murni (hanya dari data bersih): {len(val_split)}\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# --- Langkah 4.4: Lanjutkan dengan pipeline seperti biasa ---\n",
    "train_transform = transforms.Compose([\n",
    "    CLAHETransform(),\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN,std=IMAGENET_STD)\n",
    "])\n",
    "# Transform tambahan: rotasi 90° kiri/kanan\n",
    "train_transform_extra = transforms.Compose([\n",
    "    CLAHETransform(),\n",
    "    transforms.RandomChoice([\n",
    "        transforms.RandomRotation((90, 90)),\n",
    "        transforms.RandomRotation((-90, -90))\n",
    "    ]),\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    CLAHETransform(),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "train_dataset = DualTransformDataset(\n",
    "    train_split,\n",
    "    image_column='Filepath',\n",
    "    label_column='Label',\n",
    "    transform_main=train_transform,\n",
    "    transform_extra=train_transform_extra\n",
    ")\n",
    "\n",
    "val_dataset = CustomDataset(val_split, 'Filepath', 'Label', val_test_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, persistent_workers=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, persistent_workers=True)\n",
    "# PASTIKAN SEMUA LABEL BERTIPE INTEGER\n",
    "train_split[\"Label\"] = train_split[\"Label\"].astype(int)\n",
    "\n",
    "# Kode Anda yang sebelumnya\n",
    "class_weights = compute_class_weight(\"balanced\", classes=np.unique(train_split[\"Label\"]), y=train_split[\"Label\"])\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(DEVICE)\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# === 5. DEFINISI MODEL (TIDAK DIUBAH) ===\n",
    "# ===================================================================\n",
    "class FusionDINOv2(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.dinov2 = timm.create_model(\"vit_base_patch14_dinov2\", pretrained=True, num_classes=0)\n",
    "        self.convnext = timm.create_model(\"convnext_tiny\", pretrained=True, num_classes=0)\n",
    "        fusion_dim = self.dinov2.num_features + self.convnext.num_features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        feat_dino = self.dinov2(x)\n",
    "        feat_conv = self.convnext(x)\n",
    "        feat_combined = torch.cat((feat_dino, feat_conv), dim=1)\n",
    "        out = self.classifier(feat_combined)\n",
    "        return out\n",
    "class SingleSwinModel(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(\"swin_base_patch4_window7_224\", pretrained=True, num_classes=num_classes)\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# # ===================================================================\n",
    "# # === 6. INISIALISASI MODEL, LOSS, DAN SCALER (TIDAK DIUBAH) ===\n",
    "# # ===================================================================\n",
    "model = FusionDINOv2(num_classes=len(label_mapping)).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.1)\n",
    "scaler = torch.amp.GradScaler()\n",
    "best_f1 = 0.0\n",
    "best_acc = 0.0\n",
    "\n",
    "# ==================================================================================\n",
    "# === 7. STRATEGI TRAINING: GRADUAL UNFREEZING (TIDAK DIUBAH) ===\n",
    "#==================================================================================\n",
    "# # --- TAHAP 1: Latih hanya Classifier Head ---\n",
    "# print(\"\\n\" + \"=\"*50 + \"\\nTAHAP 1: Melatih Classifier Head\\n\" + \"=\"*50)\n",
    "# for param in model.dinov2.parameters(): param.requires_grad = False\n",
    "# for param in model.convnext.parameters(): param.requires_grad = False\n",
    "# for param in model.classifier.parameters(): param.requires_grad = True\n",
    "# optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LR_S1)\n",
    "# for epoch in range(EPOCHS_S1):\n",
    "#     model.train()\n",
    "#     progress_bar = tqdm(train_loader, desc=f\"S1 Epoch {epoch+1}/{EPOCHS_S1}\")\n",
    "#     for images, labels in progress_bar:\n",
    "#         images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "#         optimizer.zero_grad()\n",
    "#         with torch.amp.autocast(device_type = 'cuda'):\n",
    "#             outputs = model(images)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#         scaler.scale(loss).backward()\n",
    "#         scaler.step(optimizer)\n",
    "#         scaler.update()\n",
    "\n",
    "# # --- TAHAP 2: Latih Head + Setengah Atas DINOv2 ---\n",
    "# print(\"\\n\" + \"=\"*50 + \"\\nTAHAP 2: Melatih Head + Setengah Atas DINOv2\\n\" + \"=\"*50)\n",
    "# total_blocks = len(model.dinov2.blocks)\n",
    "# for i in range(total_blocks // 2, total_blocks):\n",
    "#     for param in model.dinov2.blocks[i].parameters():\n",
    "#         param.requires_grad = True\n",
    "# optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LR_S2)\n",
    "# for epoch in range(EPOCHS_S2):\n",
    "#     model.train()\n",
    "#     progress_bar = tqdm(train_loader, desc=f\"S2 Epoch {epoch+1}/{EPOCHS_S2}\")\n",
    "#     for images, labels in progress_bar:\n",
    "#         images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "#         optimizer.zero_grad()\n",
    "#         with torch.amp.autocast(device_type = 'cuda'):\n",
    "#             outputs = model(images)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#         scaler.scale(loss).backward()\n",
    "#         scaler.step(optimizer)\n",
    "#         scaler.update()\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\n\" + \"=\"*50 + \"\\nTAHAP 3: Fine-tuning Seluruh Model\\n\" + \"=\"*50)\n",
    "# for param in model.parameters(): param.requires_grad = True\n",
    "# optimizer = optim.AdamW(model.parameters(), weight_decay=1e-4)\n",
    "# scheduler = scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "#     optimizer,\n",
    "#     T_max=EPOCHS_S3,   # jumlah iterasi (biasanya epoch)\n",
    "#     eta_min=1e-6       # nilai LR minimum saat akhir siklus\n",
    "# )\n",
    "\n",
    "# for epoch in range(EPOCHS_S3):\n",
    "#     model.train()\n",
    "#     train_loss = 0.0\n",
    "#     train_bar = tqdm(train_loader, desc=f\"S3 Epoch {epoch+1}/{EPOCHS_S3} (Train)\")\n",
    "#     for images, labels in train_bar:\n",
    "#         images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "#         optimizer.zero_grad()\n",
    "#         with torch.amp.autocast(device_type = 'cuda'):\n",
    "#             outputs = model(images)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#         scaler.scale(loss).backward()\n",
    "#         nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "#         scaler.step(optimizer)\n",
    "#         scaler.update()\n",
    "#         scheduler.step()\n",
    "#         train_loss += loss.item()\n",
    "    \n",
    "#     model.eval()\n",
    "#     val_loss, all_preds, all_labels = 0.0, [], []\n",
    "#     val_bar = tqdm(val_loader, desc=f\"S3 Epoch {epoch+1}/{EPOCHS_S3} (Val)\")\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in val_bar:\n",
    "#             images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "#             with torch.amp.autocast(device_type = 'cuda'):\n",
    "#                 outputs = model(images)\n",
    "#                 loss = criterion(outputs, labels)\n",
    "#             val_loss += loss.item()\n",
    "#             preds = torch.argmax(outputs, dim=1)\n",
    "#             all_preds.extend(preds.cpu().numpy())\n",
    "#             all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "#     avg_train_loss = train_loss / len(train_loader)\n",
    "#     avg_val_loss = val_loss / len(val_loader)\n",
    "#     f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "#     acc = accuracy_score(all_labels, all_preds) \n",
    "#     print(f\"\\nEpoch [{epoch+1}/{EPOCHS_S3}] | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Macro F1: {f1:.4f} | acc= {acc:.4f}\")\n",
    "    \n",
    "#     report = classification_report(all_labels, all_preds, target_names=list(label_mapping.keys()))\n",
    "#     print(\"\\n--- Laporan Klasifikasi Validasi ---\")\n",
    "#     print(report)\n",
    "    \n",
    "    \n",
    "#     if acc > best_acc:\n",
    "#         best_acc = acc\n",
    "#         torch.save(model.state_dict(), \"best_gradual_unfreeze_model_one.pth\")\n",
    "#         print(f\"✅ Model disimpan (F1 terbaik baru: {best_acc:.4f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-07T12:33:14.083999Z",
     "iopub.status.busy": "2025-11-07T12:33:14.083623Z",
     "iopub.status.idle": "2025-11-07T15:27:56.894369Z",
     "shell.execute_reply": "2025-11-07T15:27:56.893477Z",
     "shell.execute_reply.started": "2025-11-07T12:33:14.083981Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "### MEMULAI BLOK B: TRAINING MODEL CONVNEXT ###\n",
      "################################################################################\n",
      "\n",
      "Menggunakan Ukuran Gambar: 224x224, Batch Size: 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f91edfe8a91451e855fe77795ba43a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/354M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TAHAP 1 (CONVNEXT): Melatih Classifier Head\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b3cd5b66da4472c8d82ac049b6e8313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S1 ConvNext Epoch 1/3:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24624c1e66e34378a0a5bab0bf518610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S1 ConvNext Epoch 2/3:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "705e79637e5d4943a1fcdf7fb82b0dd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S1 ConvNext Epoch 3/3:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TAHAP 2 (CONVNEXT): Melatih Head + Lapisan Atas\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "120c0812d7ff4a0194281ce5f184093b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S2 ConvNext Epoch 1/5:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2fa8be0ecb04cab84620c9a48a15027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S2 ConvNext Epoch 2/5:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71e00b6fdd0440c08dbf5d95f7dd139a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S2 ConvNext Epoch 3/5:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6beb75e7066c465aa59e511e97d9ab32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S2 ConvNext Epoch 4/5:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad6f643b6b7440b8b41b9c39059d4fad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S2 ConvNext Epoch 5/5:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TAHAP 3 (CONVNEXT): Fine-tuning Seluruh Model\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7646c3dbf79d4dd48eb7cffc7d68b07b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 ConvNext Epoch 1/12 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/12] | Val F1: 0.9136| Val acc: 0.9145\n",
      "✅ Model ConvNext disimpan ke best_convnext_standalone.pth (acc terbaik baru: 0.9145)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0575bd107b343e7ab927f3be3c3f717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 ConvNext Epoch 2/12 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [2/12] | Val F1: 0.9150| Val acc: 0.9157\n",
      "✅ Model ConvNext disimpan ke best_convnext_standalone.pth (acc terbaik baru: 0.9157)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a5588b83bf49c2879a9a8bbd989198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 ConvNext Epoch 3/12 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [3/12] | Val F1: 0.9151| Val acc: 0.9157\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65f2c5bd8d0b4721b2b075245891f478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 ConvNext Epoch 4/12 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [4/12] | Val F1: 0.9188| Val acc: 0.9195\n",
      "✅ Model ConvNext disimpan ke best_convnext_standalone.pth (acc terbaik baru: 0.9195)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3404b99202db4cfeafd9e0e0ef30c67c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 ConvNext Epoch 5/12 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [5/12] | Val F1: 0.9129| Val acc: 0.9132\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd1cf7aa16a947cb8a7ee5fcd791eb96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 ConvNext Epoch 6/12 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [6/12] | Val F1: 0.9141| Val acc: 0.9145\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5a2cc80dc2c48c299a6d204808c47dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 ConvNext Epoch 7/12 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [7/12] | Val F1: 0.9179| Val acc: 0.9182\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "295a1d92c43d429b9c55cbca307995bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 ConvNext Epoch 8/12 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8/12] | Val F1: 0.9177| Val acc: 0.9182\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08b39730440e486aa18405cc252af916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 ConvNext Epoch 9/12 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [9/12] | Val F1: 0.9189| Val acc: 0.9195\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a500b21b8d0c4dba83264b45a5d19f29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 ConvNext Epoch 10/12 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [10/12] | Val F1: 0.9119| Val acc: 0.9119\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4550665e24f4464f94dafe9c25ebad2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 ConvNext Epoch 11/12 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [11/12] | Val F1: 0.9179| Val acc: 0.9182\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1d433c1f8584b6cbb13fa523d4415e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 ConvNext Epoch 12/12 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [12/12] | Val F1: 0.9179| Val acc: 0.9182\n",
      "\n",
      "Training Model ConvNext Selesai. Bobot terbaik disimpan di best_convnext_standalone.pth\n",
      "Membersihkan memori GPU sebelum inferensi...\n"
     ]
    }
   ],
   "source": [
    "# Impor tambahan yang Anda perlukan untuk evaluasi\n",
    "from sklearn.metrics import accuracy_score, f1_score \n",
    "import timm # Pastikan timm sudah diimpor\n",
    "\n",
    "###################################################################################\n",
    "### BLOK B: MELATIH MODEL CONVNEXT ###\n",
    "###################################################################################\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"### MEMULAI BLOK B: TRAINING MODEL CONVNEXT ###\") # <-- GANTI\n",
    "print(\"#\"*80 + \"\\n\")\n",
    "\n",
    "# B.1. Konfigurasi khusus untuk Model ConvNext\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 16\n",
    "BEST_MODEL_NAME_CONVNEXT = \"best_convnext_standalone.pth\" # <-- GANTI\n",
    "print(f\"Menggunakan Ukuran Gambar: {IMG_SIZE}x{IMG_SIZE}, Batch Size: {BATCH_SIZE}\")\n",
    "\n",
    "# B.2. Buat Ulang Transforms dan DataLoaders untuk 224x224\n",
    "# (Tidak ada perubahan, 224x224 sudah cocok untuk ConvNext)\n",
    "train_transform_main_224 = transforms.Compose([\n",
    "    CLAHETransform(),\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN,std=IMAGENET_STD)\n",
    "])\n",
    "train_transform_extra_224 = transforms.Compose([\n",
    "    CLAHETransform(),\n",
    "    transforms.RandomChoice([transforms.RandomRotation((90, 90)), transforms.RandomRotation((-90, -90))]),\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "val_test_transform_224 = transforms.Compose([\n",
    "    CLAHETransform(),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "train_dataset = DualTransformDataset(train_split, 'Filepath', 'Label', train_transform_main_224, train_transform_extra_224)\n",
    "val_dataset = CustomDataset(val_split, 'Filepath', 'Label', val_test_transform_224)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, persistent_workers=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, persistent_workers=True)\n",
    "\n",
    "# B.3. Inisialisasi Model (DIGANTI KE CONVNEXT)\n",
    "# <-- GANTI KELAS MODEL\n",
    "class SingleConvNextModel(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super().__init__()\n",
    "        # <-- GANTI ARSITEKTUR MODEL\n",
    "        self.backbone = timm.create_model(\"convnext_base\", pretrained=True, num_classes=num_classes)\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "model = SingleConvNextModel(num_classes=len(label_mapping)).to(DEVICE) # <-- GANTI\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights,label_smoothing=0.1)\n",
    "scaler = torch.amp.GradScaler()\n",
    "best_f1 = 0.0\n",
    "best_acc = 0.0\n",
    "\n",
    "# B.4. Lakukan Training (3 Tahap disesuaikan untuk ConvNext)\n",
    "# --- TAHAP 1 ---\n",
    "print(\"\\n\" + \"=\"*50 + \"\\nTAHAP 1 (CONVNEXT): Melatih Classifier Head\\n\" + \"=\"*50) # <-- GANTI\n",
    "for param in model.backbone.parameters(): param.requires_grad = False\n",
    "for param in model.backbone.head.parameters(): param.requires_grad = True\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LR_S1)\n",
    "for epoch in range(EPOCHS_S1):\n",
    "    model.train()\n",
    "    progress_bar = tqdm(train_loader, desc=f\"S1 ConvNext Epoch {epoch+1}/{EPOCHS_S1}\") # <-- GANTI\n",
    "    for images, labels in progress_bar:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.amp.autocast(device_type = 'cuda'):\n",
    "            outputs = model(images); loss = criterion(outputs, labels)\n",
    "        scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()\n",
    "\n",
    "# --- TAHAP 2 ---\n",
    "print(\"\\n\" + \"=\"*50 + \"\\nTAHAP 2 (CONVNEXT): Melatih Head + Lapisan Atas\\n\" + \"=\"*50) # <-- GANTI\n",
    "# <-- GANTI LOGIKA UNFREEZING\n",
    "# ConvNext menggunakan 'stages' bukan 'layers'\n",
    "for param in model.backbone.stages[-1].parameters(): param.requires_grad = True\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LR_S2)\n",
    "for epoch in range(EPOCHS_S2):\n",
    "    model.train()\n",
    "    progress_bar = tqdm(train_loader, desc=f\"S2 ConvNext Epoch {epoch+1}/{EPOCHS_S2}\") # <-- GANTI\n",
    "    for images, labels in progress_bar:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.amp.autocast(device_type = 'cuda'):\n",
    "            outputs = model(images); loss = criterion(outputs, labels)\n",
    "        scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()\n",
    "        \n",
    "# --- TAHAP 3 ---\n",
    "LR_S3_CONVNEXT = 1e-5 # <-- GANTI NAMA\n",
    "print(\"\\n\" + \"=\"*50 + \"\\nTAHAP 3 (CONVNEXT): Fine-tuning Seluruh Model\\n\" + \"=\"*50) # <-- GANTI\n",
    "for param in model.parameters(): param.requires_grad = True\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR_S3_CONVNEXT, weight_decay=1e-4) # <-- GANTI LR\n",
    "scheduler = scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=EPOCHS_S3, \n",
    "    eta_min=1e-6    \n",
    ")\n",
    "\n",
    "for epoch in range(EPOCHS_S3):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_bar = tqdm(train_loader, desc=f\"S3 ConvNext Epoch {epoch+1}/{EPOCHS_S3} (Train)\") # <-- GANTI\n",
    "    for images, labels in train_bar:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.amp.autocast(device_type = 'cuda'):\n",
    "            outputs = model(images); loss = criterion(outputs, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scaler.step(optimizer); scaler.update(); scheduler.step()\n",
    "        train_loss += loss.item()\n",
    "    model.eval()\n",
    "    val_loss, all_preds, all_labels = 0.0, [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            with torch.amp.autocast(device_type = 'cuda'):\n",
    "                outputs = model(images); loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy()); all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "    acc = accuracy_score(all_labels, all_preds) \n",
    "    print(f\"\\nEpoch [{epoch+1}/{EPOCHS_S3}] | Val F1: {f1:.4f}| Val acc: {acc:.4f}\")\n",
    "    \n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        torch.save(model.state_dict(), BEST_MODEL_NAME_CONVNEXT) # <-- GANTI\n",
    "        print(f\"✅ Model ConvNext disimpan ke {BEST_MODEL_NAME_CONVNEXT} (acc terbaik baru: {best_acc:.4f})\") # <-- GANTI\n",
    "\n",
    "print(f\"\\nTraining Model ConvNext Selesai. Bobot terbaik disimpan di {BEST_MODEL_NAME_CONVNEXT}\") # <-- GANTI\n",
    "\n",
    "# B.5. Bersihkan Memori GPU\n",
    "print(\"Membersihkan memori GPU sebelum inferensi...\")\n",
    "#del model, optimizer, scheduler, train_loader, val_loader, train_dataset, val_dataset, train_df, clean_df, leaked_df, train_split, val_split\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-07T15:28:21.969839Z",
     "iopub.status.busy": "2025-11-07T15:28:21.969543Z",
     "iopub.status.idle": "2025-11-07T15:29:06.820891Z",
     "shell.execute_reply": "2025-11-07T15:29:06.819945Z",
     "shell.execute_reply.started": "2025-11-07T15:28:21.969807Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "### MEMULAI BLOK C: INFERENSI DAN SUBMISI CONVNEXT ###\n",
      "################################################################################\n",
      "\n",
      "Menggunakan IMG_SIZE: 224x224\n",
      "Memuat bobot dari: best_convnext_standalone.pth\n",
      "Bobot ConvNext berhasil dimuat.\n",
      "Memulai inferensi pada data tes...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "752b678519e142119db8ef6270748f4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferensi ConvNext:   0%|          | 0/129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferensi selesai. Total prediksi: 2057\n",
      "\n",
      "==================================================\n",
      "🎉 Submisi berhasil dibuat!\n",
      "File submisi disimpan di: submission_convnext.csv\n",
      "Contoh 5 baris pertama:\n",
      "      ID        label\n",
      "0  0001  Sate Padang\n",
      "1  0002      Rendang\n",
      "2  0003   Ayam Bakar\n",
      "3  0004      Rendang\n",
      "4  0005  Ayam Goreng\n",
      "==================================================\n",
      "Membersihkan memori GPU setelah inferensi...\n"
     ]
    }
   ],
   "source": [
    "###################################################################################\n",
    "### BLOK C: INFERENSI DAN SUBMISI (CONVNEXT)\n",
    "###################################################################################\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"### MEMULAI BLOK C: INFERENSI DAN SUBMISI CONVNEXT ###\")\n",
    "print(\"#\"*80 + \"\\n\")\n",
    "label_mapping = {\n",
    "    \"Ayam Bakar\": 0,\n",
    "    \"Ayam Betutu\": 1,\n",
    "    \"Ayam Goreng\": 2,\n",
    "    \"Ayam Pop\": 3,\n",
    "    \"Bakso\": 4,\n",
    "    \"Coto Makassar\": 5,\n",
    "    \"Gado Gado\": 6,\n",
    "    \"Gudeg\": 7,\n",
    "    \"Nasi Goreng\": 8,\n",
    "    \"Pempek\": 9,\n",
    "    \"Rawon\": 10,\n",
    "    \"Rendang\": 11,\n",
    "    \"Sate Madura\": 12,\n",
    "    \"Sate Padang\": 13,\n",
    "    \"Soto\": 14\n",
    "}\n",
    "# C.1. Definisikan Custom Dataset untuk Data Tes\n",
    "# (Menggunakan dataset tes yang bersih, hanya memuat gambar)\n",
    "class TestImageDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        # Ambil semua nama file di direktori tes\n",
    "        self.image_files = sorted([f for f in os.listdir(img_dir) if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Mengembalikan gambar dan nama file (sebagai ID)\n",
    "        return image, img_name\n",
    "\n",
    "# C.2. Tentukan Transformasi untuk Data Tes\n",
    "# (Kita gunakan transform validasi/tes dari Blok B)\n",
    "test_transform = val_test_transform_224 \n",
    "print(f\"Menggunakan IMG_SIZE: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "\n",
    "# C.3. Inisialisasi Dataset dan DataLoader\n",
    "# Pastikan TEST_DIR sudah terdefinisi di skrip Anda\n",
    "test_dataset = TestImageDataset(TEST_DIR, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "# Buat reverse mapping: indeks ke nama kelas\n",
    "idx_to_class = {v: k for k, v in label_mapping.items()}\n",
    "\n",
    "# C.4. Inisialisasi Model dan Pemuatan Bobot\n",
    "# Pastikan SingleConvNextModel() telah didefinisikan sebelumnya (di Blok B)\n",
    "model_inferensi = SingleConvNextModel(num_classes=len(label_mapping)).to(DEVICE)\n",
    "print(f\"Memuat bobot dari: {BEST_MODEL_NAME_CONVNEXT}\")\n",
    "\n",
    "if os.path.exists(BEST_MODEL_NAME_CONVNEXT):\n",
    "    model_inferensi.load_state_dict(torch.load(BEST_MODEL_NAME_CONVNEXT, map_location=DEVICE))\n",
    "    print(\"Bobot ConvNext berhasil dimuat.\")\n",
    "else:\n",
    "    print(f\"❌ Error: File bobot {BEST_MODEL_NAME_CONVNEXT} tidak ditemukan!\")\n",
    "    # Hentikan proses jika bobot tidak ditemukan\n",
    "    # raise FileNotFoundError(f\"File bobot {BEST_MODEL_NAME_CONVNEXT} tidak ditemukan.\")\n",
    "\n",
    "# C.5. Melakukan Inferensi\n",
    "model_inferensi.eval() # Set model ke mode evaluasi\n",
    "image_ids = []\n",
    "predictions = []\n",
    "\n",
    "print(\"Memulai inferensi pada data tes...\")\n",
    "with torch.no_grad():\n",
    "    for images, file_names in tqdm(test_loader, desc=\"Inferensi ConvNext\"):\n",
    "        images = images.to(DEVICE)\n",
    "        \n",
    "        # Inferensi dengan autocast\n",
    "        with torch.amp.autocast(device_type = 'cuda' if 'cuda' in DEVICE.type else 'cpu'):\n",
    "            outputs = model_inferensi(images)\n",
    "        \n",
    "        # Ambil prediksi kelas\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        \n",
    "        # Simpan nama file (ID) dan prediksi\n",
    "        image_ids.extend(file_names)\n",
    "        predictions.extend(preds)\n",
    "\n",
    "print(\"Inferensi selesai. Total prediksi:\", len(predictions))\n",
    "\n",
    "# C.6. Pemetaan Ulang Label ke Nama Kelas\n",
    "predicted_labels = [idx_to_class[p] for p in predictions]\n",
    "\n",
    "# C.7. Membuat DataFrame Submisi\n",
    "# Mengubah nama file (misal: \"image123.jpg\") menjadi ID (misal: \"image123\")\n",
    "image_ids_no_ext = [os.path.splitext(f)[0] for f in image_ids]\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'ID': image_ids_no_ext,\n",
    "    'label': predicted_labels # Sesuaikan nama kolom 'label'/'style'\n",
    "})\n",
    "\n",
    "# C.8. Menyimpan ke File CSV\n",
    "submission_filename = \"submission_convnext.csv\"\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"🎉 Submisi berhasil dibuat!\")\n",
    "print(f\"File submisi disimpan di: {submission_filename}\")\n",
    "print(\"Contoh 5 baris pertama:\\n\", submission_df.head())\n",
    "print(\"=\"*50)\n",
    "\n",
    "# C.9. Bersihkan Memori GPU \n",
    "print(\"Membersihkan memori GPU setelah inferensi...\")\n",
    "del model_inferensi, test_loader, test_dataset\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T13:20:50.677097Z",
     "iopub.status.busy": "2025-11-06T13:20:50.676668Z",
     "iopub.status.idle": "2025-11-06T14:04:51.154982Z",
     "shell.execute_reply": "2025-11-06T14:04:51.153875Z",
     "shell.execute_reply.started": "2025-11-06T13:20:50.677071Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "### MEMULAI BLOK B: TRAINING MODEL SWIN TRANSFORMER ###\n",
      "################################################################################\n",
      "\n",
      "Menggunakan Ukuran Gambar: 224x224, Batch Size: 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f514fee81a444b92b8383bd5b52f8ff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TAHAP 1 (SWIN): Melatih Classifier Head\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eb849323c264f4ea661699bdf95ffcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S1 Swin Epoch 1/3:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b03eec01eba43c1894a456881ac5db5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S1 Swin Epoch 2/3:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b1f27c0a764d5bb35d122627c55f5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S1 Swin Epoch 3/3:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TAHAP 2 (SWIN): Melatih Head + Lapisan Atas\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e261c3b2808241918bef00b9d68be2e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S2 Swin Epoch 1/5:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bf654017b544837977b6d9a76137ec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S2 Swin Epoch 2/5:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81e2004d9e184a8b8327a954675f915d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S2 Swin Epoch 3/5:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ace5b0e0f354daa9ed1d4d6b6f03fb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S2 Swin Epoch 4/5:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70dc22d5f4144cf79febe30d1667ef76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S2 Swin Epoch 5/5:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TAHAP 3 (SWIN): Fine-tuning Seluruh Model\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "648aab04e2a84cc785a2daa60eb7b2e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 Swin Epoch 1/12 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/12] | Val F1: 0.9138| Val acc: 0.9145\n",
      "✅ Model Swin disimpan ke best_swin_standalone.pth (acc terbaik baru: 0.9145)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf050d63e0ca41408eb1fb7812a00f00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 Swin Epoch 2/12 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [2/12] | Val F1: 0.9229| Val acc: 0.9233\n",
      "✅ Model Swin disimpan ke best_swin_standalone.pth (acc terbaik baru: 0.9233)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2ee537c10f94ef79755e9d831d8ad4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 Swin Epoch 3/12 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [3/12] | Val F1: 0.9257| Val acc: 0.9258\n",
      "✅ Model Swin disimpan ke best_swin_standalone.pth (acc terbaik baru: 0.9258)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fc2876f49a94051b6b25fe6d7012fde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 Swin Epoch 4/12 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [4/12] | Val F1: 0.9269| Val acc: 0.9270\n",
      "✅ Model Swin disimpan ke best_swin_standalone.pth (acc terbaik baru: 0.9270)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b22471e7697400fa62758c8164d4362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 Swin Epoch 5/12 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [5/12] | Val F1: 0.9297| Val acc: 0.9296\n",
      "✅ Model Swin disimpan ke best_swin_standalone.pth (acc terbaik baru: 0.9296)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2713aeee9004a728cbbd78fad4f1b94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 Swin Epoch 6/12 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [6/12] | Val F1: 0.9321| Val acc: 0.9321\n",
      "✅ Model Swin disimpan ke best_swin_standalone.pth (acc terbaik baru: 0.9321)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50cb5c6f88e8448bb530798c190c3ce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 Swin Epoch 7/12 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [7/12] | Val F1: 0.9232| Val acc: 0.9233\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da40b5c1e103485e943f4ae2e6e93175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 Swin Epoch 8/12 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8/12] | Val F1: 0.9295| Val acc: 0.9296\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2c63059d0cd4ab4a96b379dee84c9f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 Swin Epoch 9/12 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [9/12] | Val F1: 0.9272| Val acc: 0.9270\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ece5bac93f54b69ae2a3eecb182d453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 Swin Epoch 10/12 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [10/12] | Val F1: 0.9270| Val acc: 0.9270\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afcdcb550f6440d6ac4b8bc3fb28220e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 Swin Epoch 11/12 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [11/12] | Val F1: 0.9288| Val acc: 0.9283\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3441662078640aca94b14fd52b0d0cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 Swin Epoch 12/12 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [12/12] | Val F1: 0.9244| Val acc: 0.9245\n",
      "\n",
      "Training Model Swin Selesai. Bobot terbaik disimpan di best_swin_standalone.pth\n",
      "Membersihkan memori GPU sebelum inferensi...\n"
     ]
    }
   ],
   "source": [
    "###################################################################################\n",
    "### BLOK B: MELATIH MODEL SWIN TRANSFORMER ###\n",
    "###################################################################################\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"### MEMULAI BLOK B: TRAINING MODEL SWIN TRANSFORMER ###\")\n",
    "print(\"#\"*80 + \"\\n\")\n",
    "\n",
    "# B.1. Konfigurasi khusus untuk Model Swin\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 16\n",
    "BEST_MODEL_NAME_SWIN = \"best_swin_standalone.pth\"\n",
    "print(f\"Menggunakan Ukuran Gambar: {IMG_SIZE}x{IMG_SIZE}, Batch Size: {BATCH_SIZE}\")\n",
    "\n",
    "# B.2. Buat Ulang Transforms dan DataLoaders untuk 224x224\n",
    "train_transform_main_224 = transforms.Compose([\n",
    "    CLAHETransform(),\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN,std=IMAGENET_STD)\n",
    "])\n",
    "train_transform_extra_224 = transforms.Compose([\n",
    "    CLAHETransform(),\n",
    "    transforms.RandomChoice([transforms.RandomRotation((90, 90)), transforms.RandomRotation((-90, -90))]),\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "val_test_transform_224 = transforms.Compose([\n",
    "    CLAHETransform(),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "train_dataset = DualTransformDataset(train_split, 'Filepath', 'Label', train_transform_main_224, train_transform_extra_224)\n",
    "val_dataset = CustomDataset(val_split, 'Filepath', 'Label', val_test_transform_224)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, persistent_workers=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, persistent_workers=True)\n",
    "\n",
    "# B.3. Inisialisasi Model\n",
    "model = SingleSwinModel(num_classes=len(label_mapping)).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights,label_smoothing=0.1)\n",
    "scaler = torch.amp.GradScaler()\n",
    "best_f1 = 0.0\n",
    "best_acc = 0.0\n",
    "# B.4. Lakukan Training (3 Tahap disesuaikan untuk Swin)\n",
    "# --- TAHAP 1 ---\n",
    "print(\"\\n\" + \"=\"*50 + \"\\nTAHAP 1 (SWIN): Melatih Classifier Head\\n\" + \"=\"*50)\n",
    "for param in model.backbone.parameters(): param.requires_grad = False\n",
    "for param in model.backbone.head.parameters(): param.requires_grad = True\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LR_S1)\n",
    "for epoch in range(EPOCHS_S1):\n",
    "    model.train()\n",
    "    progress_bar = tqdm(train_loader, desc=f\"S1 Swin Epoch {epoch+1}/{EPOCHS_S1}\")\n",
    "    for images, labels in progress_bar:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.amp.autocast(device_type = 'cuda'):\n",
    "            outputs = model(images); loss = criterion(outputs, labels)\n",
    "        scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()\n",
    "\n",
    "# --- TAHAP 2 ---\n",
    "print(\"\\n\" + \"=\"*50 + \"\\nTAHAP 2 (SWIN): Melatih Head + Lapisan Atas\\n\" + \"=\"*50)\n",
    "for param in model.backbone.layers[-1].parameters(): param.requires_grad = True\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LR_S2)\n",
    "for epoch in range(EPOCHS_S2):\n",
    "    model.train()\n",
    "    progress_bar = tqdm(train_loader, desc=f\"S2 Swin Epoch {epoch+1}/{EPOCHS_S2}\")\n",
    "    for images, labels in progress_bar:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.amp.autocast(device_type = 'cuda'):\n",
    "            outputs = model(images); loss = criterion(outputs, labels)\n",
    "        scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()\n",
    "        \n",
    "# --- TAHAP 3 ---\n",
    "LR_S3_SWIN = 1e-5 \n",
    "print(\"\\n\" + \"=\"*50 + \"\\nTAHAP 3 (SWIN): Fine-tuning Seluruh Model\\n\" + \"=\"*50)\n",
    "for param in model.parameters(): param.requires_grad = True\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR_S3_SWIN, weight_decay=1e-4)\n",
    "scheduler = scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=EPOCHS_S3,   # jumlah iterasi (biasanya epoch)\n",
    "    eta_min=1e-6       # nilai LR minimum saat akhir siklus\n",
    ")\n",
    "\n",
    "for epoch in range(EPOCHS_S3):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_bar = tqdm(train_loader, desc=f\"S3 Swin Epoch {epoch+1}/{EPOCHS_S3} (Train)\")\n",
    "    for images, labels in train_bar:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.amp.autocast(device_type = 'cuda'):\n",
    "            outputs = model(images); loss = criterion(outputs, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scaler.step(optimizer); scaler.update(); scheduler.step()\n",
    "        train_loss += loss.item()\n",
    "    model.eval()\n",
    "    val_loss, all_preds, all_labels = 0.0, [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            with torch.amp.autocast(device_type = 'cuda'):\n",
    "                outputs = model(images); loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy()); all_labels.extend(labels.cpu().numpy())\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "    acc = accuracy_score(all_labels, all_preds) \n",
    "    print(f\"\\nEpoch [{epoch+1}/{EPOCHS_S3}] | Val F1: {f1:.4f}| Val acc: {acc:.4f}\")\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        torch.save(model.state_dict(), BEST_MODEL_NAME_SWIN)\n",
    "        print(f\"✅ Model Swin disimpan ke {BEST_MODEL_NAME_SWIN} (acc terbaik baru: {best_acc:.4f})\")\n",
    "\n",
    "print(f\"\\nTraining Model Swin Selesai. Bobot terbaik disimpan di {BEST_MODEL_NAME_SWIN}\")\n",
    "# B.5. Bersihkan Memori GPU\n",
    "print(\"Membersihkan memori GPU sebelum inferensi...\")\n",
    "#del model, optimizer, scheduler, train_loader, val_loader, train_dataset, val_dataset, train_df, clean_df, leaked_df, train_split, val_split\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T14:04:51.157885Z",
     "iopub.status.busy": "2025-11-06T14:04:51.157537Z",
     "iopub.status.idle": "2025-11-06T14:05:29.787589Z",
     "shell.execute_reply": "2025-11-06T14:05:29.786916Z",
     "shell.execute_reply.started": "2025-11-06T14:04:51.157864Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "### MEMULAI BLOK C: INFERENSI DAN SUBMISI SWIN TRANSFORMER ###\n",
      "################################################################################\n",
      "\n",
      "Memuat bobot dari: best_swin_standalone.pth\n",
      "Bobot Swin Transformer berhasil dimuat.\n",
      "Memulai inferensi pada data tes...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16b9d55a052543ea9bceff876f098836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferensi:   0%|          | 0/129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferensi selesai. Total prediksi: 2057\n",
      "\n",
      "==================================================\n",
      "🎉 Submisi berhasil dibuat!\n",
      "File submisi disimpan di: submission_swin_transformer.csv\n",
      "Contoh 5 baris pertama:\n",
      "          id        label\n",
      "0  0001.jpg  Sate Padang\n",
      "1  0002.jpg  Nasi Goreng\n",
      "2  0003.jpg  Ayam Goreng\n",
      "3  0004.jpg      Rendang\n",
      "4  0005.jpg  Ayam Goreng\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- Asumsi Variabel Global Anda (Perlu Didefinisikan di Lingkungan Anda) ---\n",
    "# Misalnya:\n",
    "# test_dir = '/path/to/your/test_images'\n",
    "# DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# IMG_SIZE = 224 # Dari Blok B.1\n",
    "# BATCH_SIZE = 16 # Dari Blok B.1\n",
    "BEST_MODEL_NAME_SWIN = \"best_swin_standalone.pth\" # Dari Blok B.1\n",
    "# IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "# IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Asumsi definisi label_mapping Anda:\n",
    "label_mapping = {\n",
    "    \"Ayam Bakar\": 0, \"Ayam Betutu\": 1, \"Ayam Goreng\": 2, \"Ayam Pop\": 3,\n",
    "    \"Bakso\": 4, \"Coto Makassar\": 5, \"Gado Gado\": 6, \"Gudeg\": 7,\n",
    "    \"Nasi Goreng\": 8, \"Pempek\": 9, \"Rawon\": 10, \"Rendang\": 11,\n",
    "    \"Sate Madura\": 12, \"Sate Padang\": 13, \"Soto\": 14\n",
    "}\n",
    "\n",
    "# Membuat inverse mapping: indeks ke nama kelas\n",
    "idx_to_class = {v: k for k, v in label_mapping.items()}\n",
    "\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"### MEMULAI BLOK C: INFERENSI DAN SUBMISI SWIN TRANSFORMER ###\")\n",
    "print(\"#\"*80 + \"\\n\")\n",
    "\n",
    "# C.1. Definisikan Custom Dataset untuk Data Tes\n",
    "# Data tes hanya memiliki gambar, tidak ada label.\n",
    "class TestImageDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        # Ambil semua nama file di direktori tes\n",
    "        self.image_files = sorted([f for f in os.listdir(img_dir) if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Mengembalikan gambar dan nama file (sebagai ID)\n",
    "        return image, img_name\n",
    "\n",
    "# C.2. Tentukan Transformasi untuk Data Tes (Sama seperti val_test_transform_224)\n",
    "test_transform = transforms.Compose([\n",
    "    CLAHETransform(), # Pastikan CLAHETransform terdefinisi\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "# C.3. Inisialisasi Dataset dan DataLoader\n",
    "# Anda perlu mengganti '/path/to/your/test_dir' dengan lokasi direktori gambar tes Anda\n",
    "# Contoh:\n",
    "# test_dir = \"test_data_folder\" \n",
    "test_dataset = TestImageDataset(TEST_DIR, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0) # num_workers 0 jika masalah, atau gunakan NUM_WORKERS\n",
    "\n",
    "# C.4. Inisialisasi Model dan Pemuatan Bobot\n",
    "# Pastikan SingleSwinModel() telah didefinisikan sebelumnya di skrip Anda\n",
    "model_inferensi = SingleSwinModel(num_classes=len(label_mapping)).to(DEVICE)\n",
    "print(f\"Memuat bobot dari: {BEST_MODEL_NAME_SWIN}\")\n",
    "\n",
    "if os.path.exists(BEST_MODEL_NAME_SWIN):\n",
    "    model_inferensi.load_state_dict(torch.load(BEST_MODEL_NAME_SWIN, map_location=DEVICE))\n",
    "    print(\"Bobot Swin Transformer berhasil dimuat.\")\n",
    "else:\n",
    "    print(f\"❌ Error: File bobot {BEST_MODEL_NAME_SWIN} tidak ditemukan!\")\n",
    "    # Hentikan proses jika bobot tidak ditemukan\n",
    "    # raise FileNotFoundError(f\"File bobot {BEST_MODEL_NAME_SWIN} tidak ditemukan.\")\n",
    "\n",
    "\n",
    "# C.5. Melakukan Inferensi\n",
    "model_inferensi.eval() # Set model ke mode evaluasi\n",
    "image_ids = []\n",
    "predictions = []\n",
    "\n",
    "print(\"Memulai inferensi pada data tes...\")\n",
    "with torch.no_grad():\n",
    "    for images, file_names in tqdm(test_loader, desc=\"Inferensi\"):\n",
    "        images = images.to(DEVICE)\n",
    "        \n",
    "        # Inferensi dengan autocast (jika menggunakan GPU)\n",
    "        with torch.amp.autocast(device_type = 'cuda' if 'cuda' in DEVICE.type else 'cpu'):\n",
    "            outputs = model_inferensi(images)\n",
    "        \n",
    "        # Ambil prediksi kelas (indeks dengan probabilitas tertinggi)\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        \n",
    "        # Simpan nama file (ID) dan prediksi\n",
    "        image_ids.extend(file_names)\n",
    "        predictions.extend(preds)\n",
    "\n",
    "print(\"Inferensi selesai. Total prediksi:\", len(predictions))\n",
    "\n",
    "# C.6. Pemetaan Ulang Label ke Nama Kelas\n",
    "# Mengubah indeks numerik menjadi string nama kelas\n",
    "predicted_labels = [idx_to_class[p] for p in predictions]\n",
    "\n",
    "# C.7. Membuat DataFrame Submisi\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': image_ids,\n",
    "    'label': predicted_labels\n",
    "})\n",
    "\n",
    "# C.8. Menyimpan ke File CSV\n",
    "submission_filename = \"submission_swin_transformer.csv\"\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"🎉 Submisi berhasil dibuat!\")\n",
    "print(f\"File submisi disimpan di: {submission_filename}\")\n",
    "print(\"Contoh 5 baris pertama:\\n\", submission_df.head())\n",
    "print(\"=\"*50)\n",
    "\n",
    "# C.9. Bersihkan Memori GPU (Opsional, sudah dilakukan di Blok B, tetapi baik untuk diulang)\n",
    "# print(\"Membersihkan memori GPU setelah inferensi...\")\n",
    "# del model_inferensi\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T14:05:29.788764Z",
     "iopub.status.busy": "2025-11-06T14:05:29.788551Z",
     "iopub.status.idle": "2025-11-06T14:55:13.349645Z",
     "shell.execute_reply": "2025-11-06T14:55:13.347980Z",
     "shell.execute_reply.started": "2025-11-06T14:05:29.788747Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "### MEMULAI BLOK B: TRAINING MODEL EVA TRANSFORMER ###\n",
      "################################################################################\n",
      "\n",
      "Menggunakan Ukuran Gambar: 224x224, Batch Size: 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d94ff7e52ad4ef29d257992160a256f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/343M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menggunakan model: eva02_base_patch14_224\n",
      "\n",
      "==================================================\n",
      "TAHAP 1 (EVA): Melatih Classifier Head\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00fa0cf2e53d42dba2e5a45530112708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S1 EVA Epoch 1/3:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c3947a93ae64cd7a44001c26a4c03b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S1 EVA Epoch 2/3:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9713d75d63d742ab8831bb6bbd96575a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S1 EVA Epoch 3/3:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TAHAP 2 (EVA): Melatih Head + Blok Atas\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a82e19afc8ae49efbfd9408179c42dfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S2 EVA Epoch 1/5:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c192eaef50614a73a19ed373c426cdcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S2 EVA Epoch 2/5:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc6ccd2a54ae40509998e555e96f2f99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S2 EVA Epoch 3/5:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfc0cc358ad4409ca14a3b6f196f2342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S2 EVA Epoch 4/5:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7a649021d09400d836c9e7f3c874bb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S2 EVA Epoch 5/5:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TAHAP 3 (EVA): Fine-tuning Seluruh Model\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22e29f6b38094e598a976670f19c1d72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 EVA Epoch 1/12 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/12] | Val F1: 0.9234| Val acc: 0.9245\n",
      "✅ Model EVA disimpan ke best_eva_standalone.pth (F1 terbaik baru: 0.9245)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0853900104814930be840e1e25c804d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 EVA Epoch 2/12 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [2/12] | Val F1: 0.9209| Val acc: 0.9220\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b190877fd0240ea857b00dc2b08b1d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 EVA Epoch 3/12 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [3/12] | Val F1: 0.9298| Val acc: 0.9308\n",
      "✅ Model EVA disimpan ke best_eva_standalone.pth (F1 terbaik baru: 0.9308)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e39460d5c1e94edcac8147950d3de237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 EVA Epoch 4/12 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [4/12] | Val F1: 0.9297| Val acc: 0.9308\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f61ea7751de446f3b1611e470cbfc0a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 EVA Epoch 5/12 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [5/12] | Val F1: 0.9298| Val acc: 0.9308\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ab27533b83b42a7849ec1e138aefb68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 EVA Epoch 6/12 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [6/12] | Val F1: 0.9375| Val acc: 0.9384\n",
      "✅ Model EVA disimpan ke best_eva_standalone.pth (F1 terbaik baru: 0.9384)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3519138df12741adb7641dc1ffe1fbf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 EVA Epoch 7/12 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [7/12] | Val F1: 0.9387| Val acc: 0.9396\n",
      "✅ Model EVA disimpan ke best_eva_standalone.pth (F1 terbaik baru: 0.9396)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae1333abb330445c853417a73bf70157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 EVA Epoch 8/12 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8/12] | Val F1: 0.9349| Val acc: 0.9358\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a5621c49416417496b5ef9cba399d39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 EVA Epoch 9/12 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [9/12] | Val F1: 0.9366| Val acc: 0.9371\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e48d5ff84104a54b357a0a469f834fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 EVA Epoch 10/12 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [10/12] | Val F1: 0.9375| Val acc: 0.9384\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53fbbb613c3849279928addc6d4a2804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 EVA Epoch 11/12 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [11/12] | Val F1: 0.9364| Val acc: 0.9371\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22f9e6fdb74f42fd96f43931ddba8008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 EVA Epoch 12/12 (Train):   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [12/12] | Val F1: 0.9364| Val acc: 0.9371\n",
      "\n",
      "Training Model EVA Selesai. Bobot terbaik disimpan di best_eva_standalone.pth\n",
      "\n",
      "################################################################################\n",
      "### MEMULAI BLOK C: INFERENSI DAN SUBMISI EVA TRANSFORMER ###\n",
      "################################################################################\n",
      "\n",
      "Memuat bobot dari: best_eva_standalone.pth\n",
      "Bobot EVA Transformer berhasil dimuat.\n",
      "Memulai inferensi pada data tes...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5da777b9b3714819bc9d7f751f583267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferensi:   0%|          | 0/129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferensi selesai. Total prediksi: 2057\n",
      "\n",
      "==================================================\n",
      "🎉 Submisi berhasil dibuat!\n",
      "File submisi disimpan di: submission_eva_transformer.csv\n",
      "Contoh 5 baris pertama:\n",
      "          id        label\n",
      "0  0001.jpg  Sate Padang\n",
      "1  0002.jpg      Rendang\n",
      "2  0003.jpg  Ayam Betutu\n",
      "3  0004.jpg      Rendang\n",
      "4  0005.jpg  Ayam Goreng\n",
      "==================================================\n",
      "Membersihkan memori GPU setelah inferensi...\n"
     ]
    }
   ],
   "source": [
    "###################################################################################\n",
    "### BLOK B: MELATIH MODEL EVA TRANSFORMER ###\n",
    "###################################################################################\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"### MEMULAI BLOK B: TRAINING MODEL EVA TRANSFORMER ###\") # Ganti teks\n",
    "print(\"#\"*80 + \"\\n\")\n",
    "\n",
    "# B.1. Konfigurasi khusus untuk Model EVA\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 16\n",
    "# GANTI NAMA FILE BOBOT\n",
    "BEST_MODEL_NAME_EVA = \"best_eva_standalone.pth\" \n",
    "print(f\"Menggunakan Ukuran Gambar: {IMG_SIZE}x{IMG_SIZE}, Batch Size: {BATCH_SIZE}\")\n",
    "\n",
    "# B.2. Buat Ulang Transforms dan DataLoaders untuk 224x224\n",
    "# (Tidak ada perubahan di sini, kode Anda sudah benar)\n",
    "train_transform_main_224 = transforms.Compose([\n",
    "    CLAHETransform(),\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN,std=IMAGENET_STD)\n",
    "])\n",
    "train_transform_extra_224 = transforms.Compose([\n",
    "    CLAHETransform(),\n",
    "    transforms.RandomChoice([transforms.RandomRotation((90, 90)), transforms.RandomRotation((-90, -90))]),\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "val_test_transform_224 = transforms.Compose([\n",
    "    CLAHETransform(),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "train_dataset = DualTransformDataset(train_split, 'Filepath', 'Label', train_transform_main_224, train_transform_extra_224)\n",
    "val_dataset = CustomDataset(val_split, 'Filepath', 'Label', val_test_transform_224)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, persistent_workers=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, persistent_workers=True)\n",
    "\n",
    "\n",
    "# B.3. Inisialisasi Model (DIGANTI KE EVA)\n",
    "# Kita ganti nama kelasnya agar jelas\n",
    "class SingleEvaModel(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super().__init__()\n",
    "        # GANTI MODEL DI SINI: dari \"swin_base...\" ke \"eva02_base...\"\n",
    "        self.backbone = timm.create_model(\"eva02_base_patch14_224\", pretrained=True, num_classes=num_classes)\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "model = SingleEvaModel(num_classes=len(label_mapping)).to(DEVICE) # Panggil kelas baru\n",
    "print(f\"Menggunakan model: eva02_base_patch14_224\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights,label_smoothing=0.1)\n",
    "scaler = torch.amp.GradScaler()\n",
    "best_f1 = 0.0\n",
    "best_acc = 0.0\n",
    "\n",
    "# B.4. Lakukan Training (3 Tahap disesuaikan untuk EVA)\n",
    "# --- TAHAP 1 ---\n",
    "print(\"\\n\" + \"=\"*50 + \"\\nTAHAP 1 (EVA): Melatih Classifier Head\\n\" + \"=\"*50)\n",
    "for param in model.backbone.parameters(): param.requires_grad = False\n",
    "# Model EVA (dan ViT) biasanya juga menggunakan 'head'\n",
    "for param in model.backbone.head.parameters(): param.requires_grad = True \n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LR_S1)\n",
    "for epoch in range(EPOCHS_S1):\n",
    "    model.train()\n",
    "    progress_bar = tqdm(train_loader, desc=f\"S1 EVA Epoch {epoch+1}/{EPOCHS_S1}\")\n",
    "    for images, labels in progress_bar:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.amp.autocast(device_type = 'cuda'):\n",
    "            outputs = model(images); loss = criterion(outputs, labels)\n",
    "        scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()\n",
    "\n",
    "# --- TAHAP 2 (LOGIKA UNFREEZE BERBEDA) ---\n",
    "print(\"\\n\" + \"=\"*50 + \"\\nTAHAP 2 (EVA): Melatih Head + Blok Atas\\n\" + \"=\"*50)\n",
    "# Model ViT/EVA menggunakan 'blocks', bukan 'layers'\n",
    "# Kita unfreeze blok terakhir\n",
    "for param in model.backbone.blocks[-1].parameters(): param.requires_grad = True\n",
    "# Juga unfreeze LayerNorm terakhir sebelum head\n",
    "for param in model.backbone.norm.parameters(): param.requires_grad = True\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LR_S2)\n",
    "for epoch in range(EPOCHS_S2):\n",
    "    model.train()\n",
    "    progress_bar = tqdm(train_loader, desc=f\"S2 EVA Epoch {epoch+1}/{EPOCHS_S2}\")\n",
    "    for images, labels in progress_bar:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.amp.autocast(device_type = 'cuda'):\n",
    "            outputs = model(images); loss = criterion(outputs, labels)\n",
    "        scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()\n",
    "        \n",
    "# --- TAHAP 3 ---\n",
    "\n",
    "LR_S3_EVA = 1e-5 # Anda bisa menggunakan LR yang sama atau berbeda\n",
    "print(\"\\n\" + \"=\"*50 + \"\\nTAHAP 3 (EVA): Fine-tuning Seluruh Model\\n\" + \"=\"*50)\n",
    "for param in model.parameters(): param.requires_grad = True\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR_S3_EVA, weight_decay=1e-4)\n",
    "scheduler = scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=EPOCHS_S3,   # jumlah iterasi (biasanya epoch)\n",
    "    eta_min=1e-6       # nilai LR minimum saat akhir siklus\n",
    ")\n",
    "\n",
    "for epoch in range(EPOCHS_S3):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_bar = tqdm(train_loader, desc=f\"S3 EVA Epoch {epoch+1}/{EPOCHS_S3} (Train)\")\n",
    "    for images, labels in train_bar:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.amp.autocast(device_type = 'cuda'):\n",
    "            outputs = model(images); loss = criterion(outputs, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scaler.step(optimizer); scaler.update(); scheduler.step()\n",
    "        train_loss += loss.item()\n",
    "    model.eval()\n",
    "    val_loss, all_preds, all_labels = 0.0, [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            with torch.amp.autocast(device_type = 'cuda'):\n",
    "                outputs = model(images); loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy()); all_labels.extend(labels.cpu().numpy())\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "    acc = accuracy_score(all_labels, all_preds) \n",
    "    print(f\"\\nEpoch [{epoch+1}/{EPOCHS_S3}] | Val F1: {f1:.4f}| Val acc: {acc:.4f}\")\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        # GANTI NAMA FILE DISINI\n",
    "        torch.save(model.state_dict(), BEST_MODEL_NAME_EVA) \n",
    "        print(f\"✅ Model EVA disimpan ke {BEST_MODEL_NAME_EVA} (F1 terbaik baru: {best_acc:.4f})\") # Ganti teks\n",
    "\n",
    "print(f\"\\nTraining Model EVA Selesai. Bobot terbaik disimpan di {BEST_MODEL_NAME_EVA}\") # Ganti teks\n",
    "\n",
    "# B.5. Bersihkan Memori GPU\n",
    "# print(\"Membersihkan memori GPU sebelum inferensi...\")\n",
    "# del model, optimizer, scheduler, train_loader, val_loader, train_dataset, val_dataset, train_df, clean_df, leaked_df, train_split, val_split\n",
    "# torch.cuda.empty_cache()\n",
    "# (Asumsi impor dan variabel global sudah ada dari kode sebelumnya)\n",
    "\n",
    "# Pastikan Anda MENDEFINISIKAN ULANG kelas SingleEvaModel di sini\n",
    "# agar skrip inferensi tahu strukturnya.\n",
    "class SingleEvaModel(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super().__init__()\n",
    "        # Pastikan nama model sama persis dengan saat training\n",
    "        self.backbone = timm.create_model(\"eva02_base_patch14_224\", pretrained=False, num_classes=num_classes) # pretrained=False saat inferensi\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# GANTI NAMA FILE BOBOT\n",
    "BEST_MODEL_NAME_EVA = \"best_eva_standalone.pth\"\n",
    "# (Asumsi idx_to_class, TEST_DIR, test_transform, dll sudah terdefinisi)\n",
    "# ...\n",
    "\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"### MEMULAI BLOK C: INFERENSI DAN SUBMISI EVA TRANSFORMER ###\") # Ganti teks\n",
    "print(\"#\"*80 + \"\\n\")\n",
    "\n",
    "# C.1. Definisikan Custom Dataset untuk Data Tes\n",
    "# (Tidak ada perubahan di sini, TestImageDataset sudah benar)\n",
    "class TestImageDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.image_files = sorted([f for f in os.listdir(img_dir) if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, img_name\n",
    "\n",
    "# C.2. Tentukan Transformasi untuk Data Tes\n",
    "# (Tidak ada perubahan di sini, test_transform sudah benar)\n",
    "test_transform = transforms.Compose([\n",
    "    CLAHETransform(), \n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "# C.3. Inisialisasi Dataset dan DataLoader\n",
    "# (Tidak ada perubahan di sini)\n",
    "test_dataset = TestImageDataset(TEST_DIR, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0) \n",
    "\n",
    "# C.4. Inisialisasi Model dan Pemuatan Bobot (DIGANTI KE EVA)\n",
    "model_inferensi = SingleEvaModel(num_classes=len(label_mapping)).to(DEVICE) # Panggil kelas EVA\n",
    "print(f\"Memuat bobot dari: {BEST_MODEL_NAME_EVA}\") # Ganti variabel\n",
    "\n",
    "if os.path.exists(BEST_MODEL_NAME_EVA): # Ganti variabel\n",
    "    model_inferensi.load_state_dict(torch.load(BEST_MODEL_NAME_EVA, map_location=DEVICE)) # Ganti variabel\n",
    "    print(\"Bobot EVA Transformer berhasil dimuat.\") # Ganti teks\n",
    "else:\n",
    "    print(f\"❌ Error: File bobot {BEST_MODEL_NAME_EVA} tidak ditemukan!\") # Ganti variabel\n",
    "    # raise FileNotFoundError(f\"File bobot {BEST_MODEL_NAME_EVA} tidak ditemukan.\")\n",
    "\n",
    "\n",
    "# C.5. Melakukan Inferensi\n",
    "# (Tidak ada perubahan di sini)\n",
    "model_inferensi.eval() \n",
    "image_ids = []\n",
    "predictions = []\n",
    "\n",
    "print(\"Memulai inferensi pada data tes...\")\n",
    "with torch.no_grad():\n",
    "    for images, file_names in tqdm(test_loader, desc=\"Inferensi\"):\n",
    "        images = images.to(DEVICE)\n",
    "        with torch.amp.autocast(device_type = 'cuda' if 'cuda' in DEVICE.type else 'cpu'):\n",
    "            outputs = model_inferensi(images)\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        image_ids.extend(file_names)\n",
    "        predictions.extend(preds)\n",
    "\n",
    "print(\"Inferensi selesai. Total prediksi:\", len(predictions))\n",
    "\n",
    "# C.6. Pemetaan Ulang Label ke Nama Kelas\n",
    "# (Tidak ada perubahan di sini)\n",
    "predicted_labels = [idx_to_class[p] for p in predictions]\n",
    "\n",
    "# C.7. Membuat DataFrame Submisi\n",
    "# (Tidak ada perubahan di sini)\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': image_ids,\n",
    "    'label': predicted_labels\n",
    "})\n",
    "\n",
    "# C.8. Menyimpan ke File CSV (GANTI NAMA FILE)\n",
    "submission_filename = \"submission_eva_transformer.csv\" \n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"🎉 Submisi berhasil dibuat!\")\n",
    "print(f\"File submisi disimpan di: {submission_filename}\")\n",
    "print(\"Contoh 5 baris pertama:\\n\", submission_df.head())\n",
    "print(\"=\"*50)\n",
    "\n",
    "# C.9. Bersihkan Memori GPU\n",
    "print(\"Membersihkan memori GPU setelah inferensi...\")\n",
    "# del model_inferensi\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T14:55:13.352323Z",
     "iopub.status.busy": "2025-11-06T14:55:13.351756Z",
     "iopub.status.idle": "2025-11-06T14:55:51.887621Z",
     "shell.execute_reply": "2025-11-06T14:55:51.885644Z",
     "shell.execute_reply.started": "2025-11-06T14:55:13.352276Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memuat bobot dari: best_eva_standalone.pth\n",
      "Bobot EVA Transformer berhasil dimuat.\n",
      "Memulai inferensi pada data tes...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97e6420b8b434bd28be2be5e836ba26a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferensi:   0%|          | 0/129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_37/3789818377.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Memulai inferensi pada data tes...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_names\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Inferensi\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'cuda'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_37/2234545578.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mimg_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    984\u001b[0m             \u001b[0mdeprecate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0mhas_transparency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"transparency\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# C.3. Inisialisasi Dataset dan DataLoader\n",
    "# (Tidak ada perubahan di sini)\n",
    "test_dataset = TestImageDataset(TEST_DIR, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0) \n",
    "\n",
    "# C.4. Inisialisasi Model dan Pemuatan Bobot (DIGANTI KE EVA)\n",
    "model_inferensi = SingleEvaModel(num_classes=len(label_mapping)).to(DEVICE) # Panggil kelas EVA\n",
    "print(f\"Memuat bobot dari: {BEST_MODEL_NAME_EVA}\") # Ganti variabel\n",
    "\n",
    "if os.path.exists(BEST_MODEL_NAME_EVA): # Ganti variabel\n",
    "    model_inferensi.load_state_dict(torch.load(BEST_MODEL_NAME_EVA, map_location=DEVICE)) # Ganti variabel\n",
    "    print(\"Bobot EVA Transformer berhasil dimuat.\") # Ganti teks\n",
    "else:\n",
    "    print(f\"❌ Error: File bobot {BEST_MODEL_NAME_EVA} tidak ditemukan!\") # Ganti variabel\n",
    "    # raise FileNotFoundError(f\"File bobot {BEST_MODEL_NAME_EVA} tidak ditemukan.\")\n",
    "\n",
    "label_mapping = {\n",
    "    \"Ayam Bakar\": 0, \"Ayam Betutu\": 1, \"Ayam Goreng\": 2, \"Ayam Pop\": 3,\n",
    "    \"Bakso\": 4, \"Coto Makassar\": 5, \"Gado Gado\": 6, \"Gudeg\": 7,\n",
    "    \"Nasi Goreng\": 8, \"Pempek\": 9, \"Rawon\": 10, \"Rendang\": 11,\n",
    "    \"Sate Madura\": 12, \"Sate Padang\": 13, \"Soto\": 14\n",
    "}\n",
    "\n",
    "# Membuat inverse mapping: indeks ke nama kelas\n",
    "idx_to_class = {v: k for k, v in label_mapping.items()}\n",
    "# C.5. Melakukan Inferensi\n",
    "# (Tidak ada perubahan di sini)\n",
    "model_inferensi.eval() \n",
    "image_ids = []\n",
    "predictions = []\n",
    "\n",
    "print(\"Memulai inferensi pada data tes...\")\n",
    "with torch.no_grad():\n",
    "    for images, file_names in tqdm(test_loader, desc=\"Inferensi\"):\n",
    "        images = images.to(DEVICE)\n",
    "        with torch.amp.autocast(device_type = 'cuda' if 'cuda' in DEVICE.type else 'cpu'):\n",
    "            outputs = model_inferensi(images)\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        image_ids.extend(file_names)\n",
    "        predictions.extend(preds)\n",
    "\n",
    "print(\"Inferensi selesai. Total prediksi:\", len(predictions))\n",
    "\n",
    "# C.6. Pemetaan Ulang Label ke Nama Kelas\n",
    "# (Tidak ada perubahan di sini)\n",
    "predicted_labels = [idx_to_class[p] for p in predictions]\n",
    "\n",
    "# C.7. Membuat DataFrame Submisi\n",
    "# (Tidak ada perubahan di sini)\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': image_ids,\n",
    "    'label': predicted_labels\n",
    "})\n",
    "\n",
    "# C.8. Menyimpan ke File CSV (GANTI NAMA FILE)\n",
    "submission_filename = \"submission_eva_transformer.csv\" \n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"🎉 Submisi berhasil dibuat!\")\n",
    "print(f\"File submisi disimpan di: {submission_filename}\")\n",
    "print(\"Contoh 5 baris pertama:\\n\", submission_df.head())\n",
    "print(\"=\"*50)\n",
    "\n",
    "# C.9. Bersihkan Memori GPU\n",
    "print(\"Membersihkan memori GPU setelah inferensi...\")\n",
    "# del model_inferensi\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BATAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T17:29:26.475375Z",
     "iopub.status.busy": "2025-11-06T17:29:26.474970Z",
     "iopub.status.idle": "2025-11-06T17:30:00.124239Z",
     "shell.execute_reply": "2025-11-06T17:30:00.123629Z",
     "shell.execute_reply.started": "2025-11-06T17:29:26.475356Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah data training sebelum filtering manual: 4065\n",
      "\n",
      "==================================================\n",
      "Mengidentifikasi kebocoran data (train vs test)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66ad7895b9a541c59ab39c9e6f03cdbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Menghitung Hash Data Test:   0%|          | 0/2057 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecb89ed01f914a4884a68a078987535e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Menghitung Hash Data Train:   0%|          | 0/4065 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ditemukan 80 gambar di training set yang identik dengan gambar di test set.\n",
      "\n",
      "Menerapkan strategi 'Validasi Bersih'...\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# === 3. FUNGSI DAN KELAS HELPER (TIDAK DIUBAH) ===\n",
    "# ===================================================================\n",
    "def convert_path_to_df(dataset, is_test=False):\n",
    "    image_dir = Path(dataset)\n",
    "    filepaths = list(image_dir.glob(r'**/*.*'))\n",
    "    if is_test:\n",
    "        filepaths = pd.Series(filepaths, name='Filepath').astype(str)\n",
    "        return pd.DataFrame({'Filepath': filepaths})\n",
    "    else:\n",
    "        labels = [p.parts[-2] for p in filepaths]\n",
    "        filepaths = pd.Series(filepaths, name='Filepath').astype(str)\n",
    "        labels = pd.Series(labels, name='Label')\n",
    "        return pd.concat([filepaths, labels], axis=1)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_column, label_column=None, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_column = image_column\n",
    "        self.label_column = label_column\n",
    "        self.transform = transform\n",
    "    def __len__(self): return len(self.dataframe)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.dataframe.iloc[idx][self.image_column]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform: image = self.transform(image)\n",
    "        if self.label_column:\n",
    "            label = self.dataframe.iloc[idx][self.label_column]\n",
    "            return image, torch.tensor(label, dtype=torch.long)\n",
    "        return image\n",
    "class DualTransformDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_column, label_column, transform_main, transform_extra):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_column = image_column\n",
    "        self.label_column = label_column\n",
    "        self.transform_main = transform_main\n",
    "        self.transform_extra = transform_extra\n",
    "\n",
    "    def __len__(self):\n",
    "        # Gandakan ukuran dataset (dua versi per gambar)\n",
    "        return len(self.dataframe) * 2\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Tentukan apakah pakai augmentasi utama atau tambahan\n",
    "        base_idx = idx // 2\n",
    "        use_extra = idx % 2 == 1\n",
    "\n",
    "        row = self.dataframe.iloc[base_idx]\n",
    "        img_path = row[self.image_column]\n",
    "        label = torch.tensor(row[self.label_column], dtype=torch.long)\n",
    "\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if use_extra:\n",
    "            image = self.transform_extra(image)\n",
    "        else:\n",
    "            image = self.transform_main(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "class CLAHETransform:\n",
    "    def __init__(self, clip_limit=2.0, tile_grid_size=(8, 8)):\n",
    "        self.clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n",
    "    def __call__(self, img):\n",
    "        img_np = np.array(img); img_lab = cv2.cvtColor(img_np, cv2.COLOR_RGB2Lab)\n",
    "        l, a, b = cv2.split(img_lab); l_clahe = self.clahe.apply(l)\n",
    "        img_lab_clahe = cv2.merge((l_clahe, a, b))\n",
    "        img_rgb_clahe = cv2.cvtColor(img_lab_clahe, cv2.COLOR_Lab2RGB)\n",
    "        return Image.fromarray(img_rgb_clahe)\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_column, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_column = image_column\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.dataframe.iloc[idx][self.image_column]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, img_path\n",
    "\n",
    "# ===================================================================\n",
    "# === 4. PERSIAPAN DATA DENGAN STRATEGI \"VALIDASI BERSIH\" (DIUBAH) ===\n",
    "# ===================================================================\n",
    "def get_phash(filepath):\n",
    "    try:\n",
    "        with Image.open(filepath) as img: return imagehash.phash(img)\n",
    "    except Exception: return None\n",
    "\n",
    "# --- Langkah 4.1: Memuat dan memfilter data training secara manual ---\n",
    "train_df = convert_path_to_df(TRAIN_DIR)\n",
    "print(f\"Jumlah data training sebelum filtering manual: {len(train_df)}\")\n",
    "\n",
    "# --- Langkah 4.2: Identifikasi Kebocoran dan Pisahkan Data ---\n",
    "test_df = convert_path_to_df(TEST_DIR, is_test=True)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Mengidentifikasi kebocoran data (train vs test)...\")\n",
    "tqdm.pandas(desc=\"Menghitung Hash Data Test\")\n",
    "test_hashes = set(test_df['Filepath'].progress_apply(get_phash))\n",
    "test_hashes.discard(None)\n",
    "\n",
    "tqdm.pandas(desc=\"Menghitung Hash Data Train\")\n",
    "train_df['hash'] = train_df['Filepath'].progress_apply(get_phash)\n",
    "\n",
    "train_df['is_leak'] = train_df['hash'].isin(test_hashes)\n",
    "print(f\"Ditemukan {train_df['is_leak'].sum()} gambar di training set yang identik dengan gambar di test set.\")\n",
    "\n",
    "print(\"\\nMenerapkan strategi 'Validasi Bersih'...\")\n",
    "leaked_df = train_df[train_df['is_leak']].copy()\n",
    "clean_df = train_df[~train_df['is_leak']].copy()\n",
    "\n",
    "# --- Langkah 4.3: Pemetaan Label dan Split Data ---\n",
    "label_mapping = {\n",
    "    \"Ayam Bakar\": 0,\n",
    "    \"Ayam Betutu\": 1,\n",
    "    \"Ayam Goreng\": 2,\n",
    "    \"Ayam Pop\": 3,\n",
    "    \"Bakso\": 4,\n",
    "    \"Coto Makassar\": 5,\n",
    "    \"Gado Gado\": 6,\n",
    "    \"Gudeg\": 7,\n",
    "    \"Nasi Goreng\": 8,\n",
    "    \"Pempek\": 9,\n",
    "    \"Rawon\": 10,\n",
    "    \"Rendang\": 11,\n",
    "    \"Sate Madura\": 12,\n",
    "    \"Sate Padang\": 13,\n",
    "    \"Soto\": 14\n",
    "}\n",
    "clean_df['Label'] = clean_df['Label'].map(label_mapping)\n",
    "leaked_df['Label'] = leaked_df['Label'].map(label_mapping)\n",
    "\n",
    "val_split = pd.DataFrame()\n",
    "if not clean_df.empty:\n",
    "    try:\n",
    "        clean_train_split, val_split = train_test_split(\n",
    "            clean_df, test_size=0.2, random_state=42, stratify=clean_df['Label'])\n",
    "    except ValueError:\n",
    "        print(\"Peringatan: Gagal stratify, menggunakan split biasa.\")\n",
    "        clean_train_split, val_split = train_test_split(clean_df, test_size=0.2, random_state=42)\n",
    "else:\n",
    "    clean_train_split = clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-06T14:55:51.890473Z",
     "iopub.status.idle": "2025-11-06T14:55:51.890764Z",
     "shell.execute_reply": "2025-11-06T14:55:51.890658Z",
     "shell.execute_reply.started": "2025-11-06T14:55:51.890644Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import torch.nn.functional as F\n",
    "\n",
    "# ###################################################################################\n",
    "# ### BLOK C: FINAL ENSEMBLE INFERENCE & SUBMISSION ###\n",
    "# ###################################################################################\n",
    "# print(\"\\n\" + \"#\"*80)\n",
    "# print(\"### MEMULAI BLOK C: ENSEMBLE INFERENCE & SUBMISSION ###\")\n",
    "# print(\"#\"*80 + \"\\n\")\n",
    "\n",
    "# # C.1. Inisialisasi kedua model untuk inferensi\n",
    "# model_a = FusionDINOv2(num_classes=len(label_mapping)).to(DEVICE)\n",
    "# model_b = SingleSwinModel(num_classes=len(label_mapping)).to(DEVICE)\n",
    "\n",
    "# # C.2. Muat bobot yang baru saja dilatih\n",
    "# print(f\"Memuat Model A dari: best_gradual_unfreeze_model\")\n",
    "# model_a.load_state_dict(torch.load(\"/kaggle/input/swim/pytorch/default/1/best_gradual_unfreeze_model_one (1).pth\"))\n",
    "# print(f\"Memuat Model B dari: best_swin_standalone.pth\")\n",
    "# model_b.load_state_dict(torch.load(\"/kaggle/input/swim/pytorch/default/1/best_swin_standalone (1).pth\"))\n",
    "# model_a.eval()\n",
    "# model_b.eval()\n",
    "\n",
    "# # C.3. Siapkan Test Loader (resolusi tinggi untuk input awal)\n",
    "# final_test_transform = transforms.Compose([\n",
    "#     CLAHETransform(),\n",
    "#     transforms.Resize((518, 518)), # Gunakan IMG_SIZE untuk Fusion\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "# ])\n",
    "# test_df = convert_path_to_df(TEST_DIR, is_test=True)\n",
    "# test_dataset = TestDataset(test_df, 'Filepath', transform=final_test_transform)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "# reverse_label_mapping = {v: k for k, v in label_mapping.items()}\n",
    "# submission_data = []\n",
    "\n",
    "# # C.4. Lakukan prediksi dan ensemble voting\n",
    "# with torch.no_grad():\n",
    "#     for images, paths in tqdm(test_loader, desc=\"Membuat Prediksi Ensemble Final\"):\n",
    "#         images_518 = images.to(DEVICE)\n",
    "#         outputs_a = model_a(images_518)\n",
    "#         images_224 = F.interpolate(images_518, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "#         outputs_b = model_b(images_224)\n",
    "#         probs_a = torch.softmax(outputs_a, dim=1)\n",
    "#         probs_b = torch.softmax(outputs_b, dim=1)\n",
    "#         avg_probs = (probs_a + probs_b) / 2.0\n",
    "#         preds = torch.argmax(avg_probs, dim=1)\n",
    "#         for i, path in enumerate(paths):\n",
    "#             img_id = os.path.splitext(os.path.basename(path))[0]\n",
    "#             label_int = preds[i].item()\n",
    "#             label_str = reverse_label_mapping[label_int]\n",
    "#             submission_data.append({'id': img_id, 'style': label_str})\n",
    "\n",
    "# # C.5. Simpan hasil akhir\n",
    "# print(\"Menyimpan hasil prediksi ke submission_ensemble_final.csv...\")\n",
    "# submission_df = pd.DataFrame(submission_data)\n",
    "# submission_df.sort_values(by='id', inplace=True)\n",
    "# submission_df.to_csv(\"submission_ensemble_final.csv\", index=False)\n",
    "# print(\"✅ File submission_ensemble_final.csv berhasil dibuat!\")\n",
    "# print(\"\\n\" + \"=\"*50 + \"\\nSELURUH PROSES SELESAI\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-06T14:55:51.892277Z",
     "iopub.status.idle": "2025-11-06T14:55:51.892845Z",
     "shell.execute_reply": "2025-11-06T14:55:51.892739Z",
     "shell.execute_reply.started": "2025-11-06T14:55:51.892724Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "###################################################################################\n",
    "### BLOK C: FINAL ENSEMBLE INFERENCE & SUBMISSION (MULTI-MODEL) ###\n",
    "###################################################################################\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"### MEMULAI BLOK C: ENSEMBLE INFERENCE & SUBMISSION (MULTI-MODEL) ###\")\n",
    "print(\"#\"*80 + \"\\n\")\n",
    "\n",
    "# C.1. Inisialisasi ketiga model untuk inferensi\n",
    "model_a = FusionDINOv2(num_classes=len(label_mapping)).to(DEVICE)   # Fusion\n",
    "model_b = SingleSwinModel(num_classes=len(label_mapping)).to(DEVICE) # Swin\n",
    "model_c = SingleEvaModel(num_classes=len(label_mapping)).to(DEVICE)  # EVA\n",
    "\n",
    "# C.2. Muat bobot yang baru saja dilatih\n",
    "print(f\"Memuat Model A (FusionDINOv2) dari: best_gradual_unfreeze_model.pth\")\n",
    "model_a.load_state_dict(torch.load(\"/kaggle/input/swim/pytorch/default/1/best_gradual_unfreeze_model_one (1).pth\", map_location=DEVICE))\n",
    "print(f\"Memuat Model B (SingleSwinModel) dari: best_swin_standalone.pth\")\n",
    "model_b.load_state_dict(torch.load(\"/kaggle/working/best_swin_standalone.pth.pth\", map_location=DEVICE))\n",
    "print(f\"Memuat Model C (SingleEVAModel) dari: EVA.pth\")\n",
    "model_c.load_state_dict(torch.load(\"/kaggle/working/best_eva_standalone.pth\", map_location=DEVICE))\n",
    "\n",
    "# Set ke eval mode\n",
    "model_a.eval()\n",
    "model_b.eval()\n",
    "model_c.eval()\n",
    "\n",
    "# C.3. Siapkan Test Loader (resolusi tinggi untuk input awal)\n",
    "final_test_transform = transforms.Compose([\n",
    "    CLAHETransform(),\n",
    "    transforms.Resize((518, 518)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "test_df = convert_path_to_df(TEST_DIR, is_test=True)\n",
    "test_dataset = TestDataset(test_df, 'Filepath', transform=final_test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "reverse_label_mapping = {v: k for k, v in label_mapping.items()}\n",
    "\n",
    "# C.4. Fungsi bantu untuk ensemble\n",
    "def make_submission(ensemble_name, prob_list, paths):\n",
    "    avg_probs = torch.stack(prob_list).mean(dim=0)\n",
    "    preds = torch.argmax(avg_probs, dim=1)\n",
    "    submission_data = []\n",
    "    for i, path in enumerate(paths):\n",
    "        img_id = os.path.splitext(os.path.basename(path))[0]\n",
    "        label_int = preds[i].item()\n",
    "        label_str = reverse_label_mapping[label_int]\n",
    "        submission_data.append({'ID': img_id, 'label': label_str})\n",
    "    df = pd.DataFrame(submission_data)\n",
    "    df.sort_values(by='ID', inplace=True)\n",
    "    filename = f\"submission_{ensemble_name}.csv\"\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"✅ File {filename} berhasil dibuat!\")\n",
    "\n",
    "# C.5. Proses inference\n",
    "print(\"\\nMembuat prediksi ensemble dari ketiga model...\")\n",
    "all_probs_a, all_probs_b, all_probs_c, all_paths = [], [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, paths in tqdm(test_loader, desc=\"Inferensi Multi-Model\"):\n",
    "        images_518 = images.to(DEVICE)\n",
    "\n",
    "        # Model FusionDINOv2 dan EVA pakai resolusi 518\n",
    "        outputs_a = model_a(images_518)\n",
    "        \n",
    "\n",
    "        # Model Swin pakai resolusi 224\n",
    "        images_224 = F.interpolate(images_518, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "        outputs_b = model_b(images_224)\n",
    "        outputs_c = model_c(images_224)\n",
    "\n",
    "        # Softmax probabilities\n",
    "        probs_a = torch.softmax(outputs_a, dim=1)\n",
    "        probs_b = torch.softmax(outputs_b, dim=1)\n",
    "        probs_c = torch.softmax(outputs_c, dim=1)\n",
    "\n",
    "        # Simpan hasil batch\n",
    "        all_probs_a.append(probs_a.cpu())\n",
    "        all_probs_b.append(probs_b.cpu())\n",
    "        all_probs_c.append(probs_c.cpu())\n",
    "        all_paths.extend(paths)\n",
    "\n",
    "# Gabungkan semua batch\n",
    "all_probs_a = torch.cat(all_probs_a, dim=0)\n",
    "all_probs_b = torch.cat(all_probs_b, dim=0)\n",
    "all_probs_c = torch.cat(all_probs_c, dim=0)\n",
    "\n",
    "# C.6. Buat berbagai kombinasi ensemble\n",
    "print(\"\\nMenyusun berbagai kombinasi ensemble...\")\n",
    "\n",
    "# 1. EVA + SWIN\n",
    "make_submission(\"ensemble_eva_swin\", [all_probs_c, all_probs_b], all_paths)\n",
    "\n",
    "# 2. EVA + FUSION\n",
    "make_submission(\"ensemble_eva_fusion\", [all_probs_c, all_probs_a], all_paths)\n",
    "\n",
    "# 3. SWIN + FUSION\n",
    "make_submission(\"ensemble_swin_fusion\", [all_probs_b, all_probs_a], all_paths)\n",
    "\n",
    "# 4. EVA + SWIN + FUSION\n",
    "make_submission(\"ensemble_eva_swin_fusion\", [all_probs_c, all_probs_b, all_probs_a], all_paths)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\nSELURUH PROSES ENSEMBLE SELESAI\\n\" + \"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T17:30:00.125706Z",
     "iopub.status.busy": "2025-11-06T17:30:00.125488Z",
     "iopub.status.idle": "2025-11-06T17:30:33.344817Z",
     "shell.execute_reply": "2025-11-06T17:30:33.344059Z",
     "shell.execute_reply.started": "2025-11-06T17:30:00.125689Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menggunakan device: cuda\n",
      "Ukuran gambar: 518x518, Batch Size: 8\n",
      "Jumlah data training sebelum filtering manual: 4065\n",
      "\n",
      "==================================================\n",
      "Mengidentifikasi kebocoran data (train vs test)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1afbc7b076da4337a7fe4765f375588e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Menghitung Hash Data Test:   0%|          | 0/2057 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cab81752138f4a88bceaa4c5a8eb96af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Menghitung Hash Data Train:   0%|          | 0/4065 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ditemukan 80 gambar di training set yang identik dengan gambar di test set.\n",
      "\n",
      "Menerapkan strategi 'Validasi Bersih'...\n",
      "Ukuran set training final (sisa bersih + semua bocor): 3268\n",
      "Ukuran set validasi murni (hanya dari data bersih): 797\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# === 2. KONFIGURASI DAN SETUP (TIDAK DIUBAH) ===\n",
    "# ===================================================================\n",
    "# Direktori\n",
    "TRAIN_DIR = \"/kaggle/input/newdatasets/train\"\n",
    "TEST_DIR = \"/kaggle/input/newdatasets/test\"\n",
    "\n",
    "# Konfigurasi Model dan Training\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMG_SIZE = 518\n",
    "BATCH_SIZE = 8\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "# --- HYPERPARAMETER UNTUK GRADUAL UNFREEZING ---\n",
    "EPOCHS_S1 = 3\n",
    "LR_S1 = 3e-4\n",
    "EPOCHS_S2 = 5\n",
    "LR_S2 = 5e-5\n",
    "EPOCHS_S3 = 12\n",
    "LR_S3 = 1.59e-06\n",
    "\n",
    "# Statistik ImageNet\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "print(f\"Menggunakan device: {DEVICE}\")\n",
    "print(f\"Ukuran gambar: {IMG_SIZE}x{IMG_SIZE}, Batch Size: {BATCH_SIZE}\")\n",
    "\n",
    "# ===================================================================\n",
    "# === 3. FUNGSI DAN KELAS HELPER (TIDAK DIUBAH) ===\n",
    "# ===================================================================\n",
    "def convert_path_to_df(dataset, is_test=False):\n",
    "    image_dir = Path(dataset)\n",
    "    filepaths = list(image_dir.glob(r'**/*.*'))\n",
    "    if is_test:\n",
    "        filepaths = pd.Series(filepaths, name='Filepath').astype(str)\n",
    "        return pd.DataFrame({'Filepath': filepaths})\n",
    "    else:\n",
    "        labels = [p.parts[-2] for p in filepaths]\n",
    "        filepaths = pd.Series(filepaths, name='Filepath').astype(str)\n",
    "        labels = pd.Series(labels, name='Label')\n",
    "        return pd.concat([filepaths, labels], axis=1)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_column, label_column=None, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_column = image_column\n",
    "        self.label_column = label_column\n",
    "        self.transform = transform\n",
    "    def __len__(self): return len(self.dataframe)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.dataframe.iloc[idx][self.image_column]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform: image = self.transform(image)\n",
    "        if self.label_column:\n",
    "            label = self.dataframe.iloc[idx][self.label_column]\n",
    "            return image, torch.tensor(label, dtype=torch.long)\n",
    "        return image\n",
    "class DualTransformDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_column, label_column, transform_main, transform_extra):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_column = image_column\n",
    "        self.label_column = label_column\n",
    "        self.transform_main = transform_main\n",
    "        self.transform_extra = transform_extra\n",
    "\n",
    "    def __len__(self):\n",
    "        # Gandakan ukuran dataset (dua versi per gambar)\n",
    "        return len(self.dataframe) * 2\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Tentukan apakah pakai augmentasi utama atau tambahan\n",
    "        base_idx = idx // 2\n",
    "        use_extra = idx % 2 == 1\n",
    "\n",
    "        row = self.dataframe.iloc[base_idx]\n",
    "        img_path = row[self.image_column]\n",
    "        label = torch.tensor(row[self.label_column], dtype=torch.long)\n",
    "\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if use_extra:\n",
    "            image = self.transform_extra(image)\n",
    "        else:\n",
    "            image = self.transform_main(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "class CLAHETransform:\n",
    "    def __init__(self, clip_limit=2.0, tile_grid_size=(8, 8)):\n",
    "        self.clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n",
    "    def __call__(self, img):\n",
    "        img_np = np.array(img); img_lab = cv2.cvtColor(img_np, cv2.COLOR_RGB2Lab)\n",
    "        l, a, b = cv2.split(img_lab); l_clahe = self.clahe.apply(l)\n",
    "        img_lab_clahe = cv2.merge((l_clahe, a, b))\n",
    "        img_rgb_clahe = cv2.cvtColor(img_lab_clahe, cv2.COLOR_Lab2RGB)\n",
    "        return Image.fromarray(img_rgb_clahe)\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_column, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_column = image_column\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.dataframe.iloc[idx][self.image_column]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, img_path\n",
    "\n",
    "# ===================================================================\n",
    "# === 4. PERSIAPAN DATA DENGAN STRATEGI \"VALIDASI BERSIH\" (DIUBAH) ===\n",
    "# ===================================================================\n",
    "def get_phash(filepath):\n",
    "    try:\n",
    "        with Image.open(filepath) as img: return imagehash.phash(img)\n",
    "    except Exception: return None\n",
    "\n",
    "# --- Langkah 4.1: Memuat dan memfilter data training secara manual ---\n",
    "train_df = convert_path_to_df(TRAIN_DIR)\n",
    "print(f\"Jumlah data training sebelum filtering manual: {len(train_df)}\")\n",
    "\n",
    "# --- Langkah 4.2: Identifikasi Kebocoran dan Pisahkan Data ---\n",
    "test_df = convert_path_to_df(TEST_DIR, is_test=True)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Mengidentifikasi kebocoran data (train vs test)...\")\n",
    "tqdm.pandas(desc=\"Menghitung Hash Data Test\")\n",
    "test_hashes = set(test_df['Filepath'].progress_apply(get_phash))\n",
    "test_hashes.discard(None)\n",
    "\n",
    "tqdm.pandas(desc=\"Menghitung Hash Data Train\")\n",
    "train_df['hash'] = train_df['Filepath'].progress_apply(get_phash)\n",
    "\n",
    "train_df['is_leak'] = train_df['hash'].isin(test_hashes)\n",
    "print(f\"Ditemukan {train_df['is_leak'].sum()} gambar di training set yang identik dengan gambar di test set.\")\n",
    "\n",
    "print(\"\\nMenerapkan strategi 'Validasi Bersih'...\")\n",
    "leaked_df = train_df[train_df['is_leak']].copy()\n",
    "clean_df = train_df[~train_df['is_leak']].copy()\n",
    "\n",
    "# --- Langkah 4.3: Pemetaan Label dan Split Data ---\n",
    "label_mapping = {\n",
    "    \"Ayam Bakar\": 0,\n",
    "    \"Ayam Betutu\": 1,\n",
    "    \"Ayam Goreng\": 2,\n",
    "    \"Ayam Pop\": 3,\n",
    "    \"Bakso\": 4,\n",
    "    \"Coto Makassar\": 5,\n",
    "    \"Gado Gado\": 6,\n",
    "    \"Gudeg\": 7,\n",
    "    \"Nasi Goreng\": 8,\n",
    "    \"Pempek\": 9,\n",
    "    \"Rawon\": 10,\n",
    "    \"Rendang\": 11,\n",
    "    \"Sate Madura\": 12,\n",
    "    \"Sate Padang\": 13,\n",
    "    \"Soto\": 14\n",
    "}\n",
    "clean_df['Label'] = clean_df['Label'].map(label_mapping)\n",
    "leaked_df['Label'] = leaked_df['Label'].map(label_mapping)\n",
    "\n",
    "val_split = pd.DataFrame()\n",
    "if not clean_df.empty:\n",
    "    try:\n",
    "        clean_train_split, val_split = train_test_split(\n",
    "            clean_df, test_size=0.2, random_state=42, stratify=clean_df['Label'])\n",
    "    except ValueError:\n",
    "        print(\"Peringatan: Gagal stratify, menggunakan split biasa.\")\n",
    "        clean_train_split, val_split = train_test_split(clean_df, test_size=0.2, random_state=42)\n",
    "else:\n",
    "    clean_train_split = clean_df\n",
    "\n",
    "train_split = pd.concat([clean_train_split, leaked_df], ignore_index=True)\n",
    "train_split.drop(columns=['hash', 'is_leak'], inplace=True)\n",
    "val_split.drop(columns=['hash', 'is_leak'], inplace=True)\n",
    "print(f\"Ukuran set training final (sisa bersih + semua bocor): {len(train_split)}\")\n",
    "print(f\"Ukuran set validasi murni (hanya dari data bersih): {len(val_split)}\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# --- Langkah 4.4: Lanjutkan dengan pipeline seperti biasa ---\n",
    "train_transform = transforms.Compose([\n",
    "    CLAHETransform(),\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN,std=IMAGENET_STD)\n",
    "])\n",
    "# Transform tambahan: rotasi 90° kiri/kanan\n",
    "train_transform_extra = transforms.Compose([\n",
    "    CLAHETransform(),\n",
    "    transforms.RandomChoice([\n",
    "        transforms.RandomRotation((90, 90)),\n",
    "        transforms.RandomRotation((-90, -90))\n",
    "    ]),\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    CLAHETransform(),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "train_dataset = DualTransformDataset(\n",
    "    train_split,\n",
    "    image_column='Filepath',\n",
    "    label_column='Label',\n",
    "    transform_main=train_transform,\n",
    "    transform_extra=train_transform_extra\n",
    ")\n",
    "\n",
    "val_dataset = CustomDataset(val_split, 'Filepath', 'Label', val_test_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, persistent_workers=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, persistent_workers=True)\n",
    "class_weights = compute_class_weight(\"balanced\", classes=np.unique(train_split[\"Label\"]), y=train_split[\"Label\"])\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(DEVICE)\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# === 5. DEFINISI MODEL (TIDAK DIUBAH) ===\n",
    "# ===================================================================\n",
    "class FusionDINOv2(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.dinov2 = timm.create_model(\"vit_base_patch14_dinov2\", pretrained=True, num_classes=0)\n",
    "        self.convnext = timm.create_model(\"convnext_tiny\", pretrained=True, num_classes=0)\n",
    "        fusion_dim = self.dinov2.num_features + self.convnext.num_features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        feat_dino = self.dinov2(x)\n",
    "        feat_conv = self.convnext(x)\n",
    "        feat_combined = torch.cat((feat_dino, feat_conv), dim=1)\n",
    "        out = self.classifier(feat_combined)\n",
    "        return out\n",
    "class SingleSwinModel(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(\"swin_base_patch4_window7_224\", pretrained=True, num_classes=num_classes)\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "class SingleEVAModel(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super().__init__()\n",
    "        # Menggunakan model EVA-02 yang sangat kuat\n",
    "        model_name = \"eva02_base_patch14_224.mim_in22k\"\n",
    "        self.backbone = timm.create_model(model_name, pretrained=True, num_classes=num_classes)\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-07T15:33:36.658340Z",
     "iopub.status.busy": "2025-11-07T15:33:36.657763Z",
     "iopub.status.idle": "2025-11-07T15:33:36.662561Z",
     "shell.execute_reply": "2025-11-07T15:33:36.661829Z",
     "shell.execute_reply.started": "2025-11-07T15:33:36.658320Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SingleEVAModel(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super().__init__()\n",
    "        # Menggunakan model EVA-02 yang sangat kuat\n",
    "        model_name = \"eva02_base_patch14_224.mim_in22k\"\n",
    "        self.backbone = timm.create_model(model_name, pretrained=True, num_classes=num_classes)\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T17:32:29.758375Z",
     "iopub.status.busy": "2025-11-06T17:32:29.757696Z",
     "iopub.status.idle": "2025-11-06T17:54:43.487282Z",
     "shell.execute_reply": "2025-11-06T17:54:43.486252Z",
     "shell.execute_reply.started": "2025-11-06T17:32:29.758348Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "### MEMULAI BLOK C: ENSEMBLE INFERENCE & SUBMISSION (MULTI-MODEL TTA) ###\n",
      "################################################################################\n",
      "\n",
      "Mendefinisikan TTA transforms (Flip + Rotasi)...\n",
      "\n",
      "Inisialisasi model dasar...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef631c7ca22e45ffa0240f81459c62b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf258973b2b64cf19cc56f8cdb1a9685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/114M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0357917bfb0144358cfe310a36e05499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b02d06810034454f8599915a5183fc42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/343M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memuat bobot...\n",
      "Memuat Model A (FusionDINOv2) dari: best_gradual_unfreeze_model_one.pth\n",
      "Memuat Model B (SingleSwinModel) dari: best_swin_standalone.pth\n",
      "Memuat Model C (SingleEVAModel) dari: EVA.pth\n",
      "\n",
      "Membungkus model dengan TTA Wrapper...\n",
      " > Wrapper 224px dibuat\n",
      " > Wrapper 224px dibuat\n",
      "Semua model siap untuk TTA.\n",
      "\n",
      "Membuat prediksi TTA ensemble dari ketiga model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e16398f473cd4009987fa4774714c537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferensi Multi-Model TTA:   0%|          | 0/258 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Menyusun berbagai kombinasi ensemble...\n",
      "Membuat submit (Average): tta_ensemble_eva_swin...\n",
      "✅ File submission_tta_ensemble_eva_swin.csv berhasil dibuat!\n",
      "Membuat submit (Average): tta_ensemble_eva_fusion...\n",
      "✅ File submission_tta_ensemble_eva_fusion.csv berhasil dibuat!\n",
      "Membuat submit (Average): tta_ensemble_swin_fusion...\n",
      "✅ File submission_tta_ensemble_swin_fusion.csv berhasil dibuat!\n",
      "Membuat submit (Average): tta_ensemble_eva_swin_fusion...\n",
      "✅ File submission_tta_ensemble_eva_swin_fusion.csv berhasil dibuat!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import ttach as tta  # <-- Impor TTA\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# =================================================================================\n",
    "# ASUMSI: Variabel dan Kelas ini sudah ada dari blok sebelumnya:\n",
    "#\n",
    "# - DEVICE = torch.device(...)\n",
    "# - label_mapping = {...}\n",
    "# - TEST_DIR = \"...\"\n",
    "# - BATCH_SIZE, NUM_WORKERS, IMAGENET_MEAN, IMAGENET_STD\n",
    "# - Class Definition: FusionDINOv2, SingleSwinModel, SingleEVAModel\n",
    "# - Class Definition: CLAHETransform, TestDataset\n",
    "# - Fungsi Definition: convert_path_to_df\n",
    "# =================================================================================\n",
    "\n",
    "\n",
    "####################################################################################\n",
    "### BLOK C: FINAL ENSEMBLE INFERENCE & SUBMISSION (DENGAN TTA)\n",
    "####################################################################################\n",
    "\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"### MEMULAI BLOK C: ENSEMBLE INFERENCE & SUBMISSION (MULTI-MODEL TTA) ###\")\n",
    "print(\"#\"*80 + \"\\n\")\n",
    "\n",
    "# C.1. Definisikan TTA transformations\n",
    "# Kita akan menggunakan Horizontal Flip dan 4 Rotasi (0, 90, 180, 270)\n",
    "# Total augmentasi = 2 * 4 = 8 augmentasi per gambar\n",
    "print(\"Mendefinisikan TTA transforms (Flip + Rotasi)...\")\n",
    "tta_transforms = tta.Compose(\n",
    "    [\n",
    "        tta.HorizontalFlip(),\n",
    "        tta.Rotate90(angles=[0, 90, 180, 270]), \n",
    "        # tta.Scale(scales=[1, 1.1], use_fast_collate=True), # <-- Bisa ditambah jika perlu\n",
    "    ]\n",
    ")\n",
    "\n",
    "# C.2. Buat Wrapper Model untuk menangani resizing\n",
    "# Ini adalah cara yang \"benar\" agar TTA (misal: rotasi)\n",
    "# diterapkan pada gambar 518px SEBELUM di-resize ke 224px.\n",
    "class Model224Wrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        print(\" > Wrapper 224px dibuat\")\n",
    "\n",
    "    def forward(self, x_518):\n",
    "        # TTA Wrapper akan memberikan gambar 518px yang sudah di-augmentasi\n",
    "        # Kita resize ke 224px DI DALAM forward pass\n",
    "        x_224 = F.interpolate(x_518, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "        return self.model(x_224)\n",
    "\n",
    "# C.3. Inisialisasi model ASLI (tanpa TTA)\n",
    "print(\"\\nInisialisasi model dasar...\")\n",
    "model_a_base = FusionDINOv2(num_classes=len(label_mapping)).to(DEVICE)\n",
    "model_b_base = SingleSwinModel(num_classes=len(label_mapping)).to(DEVICE)\n",
    "model_c_base = SingleEVAModel(num_classes=len(label_mapping)).to(DEVICE)\n",
    "\n",
    "# C.4. Muat bobot ke model ASLI\n",
    "print(\"Memuat bobot...\")\n",
    "print(f\"Memuat Model A (FusionDINOv2) dari: best_gradual_unfreeze_model_one.pth\")\n",
    "model_a_base.load_state_dict(torch.load(\"/kaggle/input/swim/pytorch/default/4/best_gradual_unfreeze_model_one (1).pth\", map_location=DEVICE))\n",
    "print(f\"Memuat Model B (SingleSwinModel) dari: best_swin_standalone.pth\")\n",
    "model_b_base.load_state_dict(torch.load(\"/kaggle/input/palingbaru/pytorch/default/2/best_swin_standalone (3).pth\", map_location=DEVICE))\n",
    "print(f\"Memuat Model C (SingleEVAModel) dari: EVA.pth\")\n",
    "model_c_base.load_state_dict(torch.load(\"/kaggle/input/palingbaru/pytorch/default/2/best_eva_standalone (3).pth\", map_location=DEVICE))\n",
    "\n",
    "# C.5. BUNGKUS model dengan TTA\n",
    "print(\"\\nMembungkus model dengan TTA Wrapper...\")\n",
    "# Model A (Fusion/DINO) menggunakan 518px, bisa langsung dibungkus\n",
    "tta_model_a = tta.ClassificationTTAWrapper(model_a_base, tta_transforms).to(DEVICE)\n",
    "\n",
    "# Model B (Swin) dan C (EVA) dibungkus DENGAN wrapper 224px kita\n",
    "tta_model_b = tta.ClassificationTTAWrapper(\n",
    "    Model224Wrapper(model_b_base), tta_transforms\n",
    ").to(DEVICE)\n",
    "\n",
    "tta_model_c = tta.ClassificationTTAWrapper(\n",
    "    Model224Wrapper(model_c_base), tta_transforms\n",
    ").to(DEVICE)\n",
    "\n",
    "# Set semua model TTA ke mode eval\n",
    "tta_model_a.eval()\n",
    "tta_model_b.eval()\n",
    "tta_model_c.eval()\n",
    "print(\"Semua model siap untuk TTA.\")\n",
    "\n",
    "# C.6. Siapkan Test Loader (tetap 518, TTA wrapper menangani sisanya)\n",
    "final_test_transform = transforms.Compose([\n",
    "    CLAHETransform(),\n",
    "    transforms.Resize((518, 518)), # <-- Loader HANYA me-load gambar 518\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "test_df = convert_path_to_df(TEST_DIR, is_test=True)\n",
    "test_dataset = TestDataset(test_df, 'Filepath', transform=final_test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "reverse_label_mapping = {v: k for k, v in label_mapping.items()}\n",
    "\n",
    "# C.7. Fungsi bantu untuk ensemble (dengan OPSI WEIGHTS)\n",
    "def make_submission(ensemble_name, prob_list, paths, weights=None):\n",
    "    if weights is None:\n",
    "        # Simple Averaging\n",
    "        print(f\"Membuat submit (Average): {ensemble_name}...\")\n",
    "        avg_probs = torch.stack(prob_list).mean(dim=0)\n",
    "    else:\n",
    "        # Weighted Averaging\n",
    "        print(f\"Membuat submit (Weighted): {ensemble_name}...\")\n",
    "        # Pastikan bobot dalam bentuk tensor yang tepat untuk broadcasting\n",
    "        weights_tensor = torch.tensor(weights, dtype=torch.float32).view(-1, 1, 1)\n",
    "        # Normalisasi bobot agar jumlahnya 1\n",
    "        weights_tensor = weights_tensor / weights_tensor.sum() \n",
    "        \n",
    "        weighted_probs = torch.stack(prob_list) * weights_tensor\n",
    "        avg_probs = torch.sum(weighted_probs, dim=0)\n",
    "\n",
    "    preds = torch.argmax(avg_probs, dim=1)\n",
    "    submission_data = []\n",
    "    \n",
    "    # Ganti 'style' dengan 'label' jika diperlukan oleh format submisi\n",
    "    # Saya menggunakan 'style' sesuai kode asli Anda\n",
    "    for i, path in enumerate(paths):\n",
    "        img_id = os.path.splitext(os.path.basename(path))[0]\n",
    "        label_int = preds[i].item()\n",
    "        label_str = reverse_label_mapping[label_int]\n",
    "        submission_data.append({'id': img_id, 'style': label_str}) \n",
    "        \n",
    "    df = pd.DataFrame(submission_data)\n",
    "    df.sort_values(by='id', inplace=True)\n",
    "    filename = f\"submission_{ensemble_name}.csv\"\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"✅ File {filename} berhasil dibuat!\")\n",
    "\n",
    "# C.8. Proses inference TTA\n",
    "print(\"\\nMembuat prediksi TTA ensemble dari ketiga model...\")\n",
    "all_probs_a, all_probs_b, all_probs_c, all_paths = [], [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Gunakan batch size lebih kecil jika OOM (Out of Memory)\n",
    "    # TTA 8x augmentasi, jadi batch_size=8 akan memproses 64 gambar di VRAM\n",
    "    for images, paths in tqdm(test_loader, desc=\"Inferensi Multi-Model TTA\"):\n",
    "        images_518 = images.to(DEVICE)\n",
    "\n",
    "        # Semua model menerima input 518x518\n",
    "        # TTA Wrapper akan menangani augmentasi\n",
    "        # Wrapper kustom (Model224Wrapper) akan menangani resizing\n",
    "        \n",
    "        probs_a = tta_model_a(images_518)\n",
    "        probs_b = tta_model_b(images_518)\n",
    "        probs_c = tta_model_c(images_518)\n",
    "\n",
    "        # Output dari tta.ClassificationTTAWrapper SUDAH di-softmax dan di-average\n",
    "        \n",
    "        # Simpan hasil batch\n",
    "        all_probs_a.append(probs_a.cpu())\n",
    "        all_probs_b.append(probs_b.cpu())\n",
    "        all_probs_c.append(probs_c.cpu())\n",
    "        all_paths.extend(paths)\n",
    "\n",
    "# Gabungkan semua batch\n",
    "all_probs_a = torch.cat(all_probs_a, dim=0)\n",
    "all_probs_b = torch.cat(all_probs_b, dim=0)\n",
    "all_probs_c = torch.cat(all_probs_c, dim=0)\n",
    "\n",
    "# C.9. Buat berbagai kombinasi ensemble\n",
    "print(\"\\nMenyusun berbagai kombinasi ensemble...\")\n",
    "\n",
    "# --- Simple Averaging ---\n",
    "make_submission(\"tta_ensemble_eva_swin\", [all_probs_c, all_probs_b], all_paths)\n",
    "make_submission(\"tta_ensemble_eva_fusion\", [all_probs_c, all_probs_a], all_paths)\n",
    "make_submission(\"tta_ensemble_swin_fusion\", [all_probs_b, all_probs_a], all_paths)\n",
    "make_submission(\"tta_ensemble_eva_swin_fusion\", [all_probs_c, all_probs_b, all_probs_a], all_paths)\n",
    "\n",
    "\n",
    "# C.10. Bersihkan memori\n",
    "# del model_a_base, model_b_base, model_c_base\n",
    "# del tta_model_a, tta_model_b, tta_model_c\n",
    "# del all_probs_a, all_probs_b, all_probs_c, all_paths, test_loader, test_dataset\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-07T15:32:35.682312Z",
     "iopub.status.busy": "2025-11-07T15:32:35.681403Z",
     "iopub.status.idle": "2025-11-07T15:32:39.591238Z",
     "shell.execute_reply": "2025-11-07T15:32:39.590471Z",
     "shell.execute_reply.started": "2025-11-07T15:32:35.682276Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ttach\n",
      "  Downloading ttach-0.0.3-py3-none-any.whl.metadata (5.2 kB)\n",
      "Downloading ttach-0.0.3-py3-none-any.whl (9.8 kB)\n",
      "Installing collected packages: ttach\n",
      "Successfully installed ttach-0.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install ttach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-07T15:33:45.819964Z",
     "iopub.status.busy": "2025-11-07T15:33:45.819692Z",
     "iopub.status.idle": "2025-11-07T16:06:59.777489Z",
     "shell.execute_reply": "2025-11-07T16:06:59.776464Z",
     "shell.execute_reply.started": "2025-11-07T15:33:45.819945Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "### MEMULAI BLOK C: ENSEMBLE INFERENCE & SUBMISSION (4-MODEL TTA) ###\n",
      "################################################################################\n",
      "\n",
      "Mendefinisikan TTA transforms (Flip + Rotasi)...\n",
      "\n",
      "Inisialisasi model dasar...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de72e9f0b83746dd99088586e2563961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/343M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memuat bobot...\n",
      "Memuat Model A (FusionDINOv2) dari: best_gradual_unfreeze_model_one.pth\n",
      "Memuat Model B (SingleSwinModel) dari: best_swin_standalone.pth\n",
      "Memuat Model C (SingleEVAModel) dari: EVA.pth\n",
      "Memuat Model D (SingleConvNextModel) dari: best_convnext_standalone.pth\n",
      "\n",
      "Membungkus model dengan TTA Wrapper...\n",
      " > Wrapper 224px dibuat\n",
      " > Wrapper 224px dibuat\n",
      " > Wrapper 224px dibuat\n",
      "Semua 4 model siap untuk TTA.\n",
      "\n",
      "==================================================\n",
      "### MENJALANKAN VALIDASI ENSEMBLE PADA VAL SET ###\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5ffeae917c44fa7844e30a513623b7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validasi TTA Ensemble:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- HASIL VALIDASI TTA ENSEMBLE (4 MODEL) ---\n",
      "📈 F1-Score Macro: 0.967073\n",
      "🎯 Accuracy:       0.967296\n",
      "----------------------------------------------\n",
      "\n",
      "\n",
      "==================================================\n",
      "### MENJALANKAN INFERENSI PADA TEST SET ###\n",
      "==================================================\n",
      "\n",
      "Membuat prediksi TTA ensemble dari 4 model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16262c2e2f68467384a4459f0752af83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferensi Multi-Model TTA:   0%|          | 0/129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Menyusun berbagai kombinasi ensemble...\n",
      "Membuat submit (Average): tta_ensemble_4_models_avg...\n",
      "✅ File submission_tta_ensemble_4_models_avg.csv berhasil dibuat!\n",
      "Membuat submit (Weighted): tta_ensemble_4_models_weighted...\n",
      "✅ File submission_tta_ensemble_4_models_weighted.csv berhasil dibuat!\n",
      "\n",
      "==================================================\n",
      "SELURUH PROSES ENSEMBLE TTA SELESAI\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import ttach as tta  # <-- Impor TTA\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score, accuracy_score # <-- Perlu untuk validasi\n",
    "\n",
    "# =================================================================================\n",
    "# ASUMSI: Variabel dan Kelas ini sudah ada dari blok sebelumnya:\n",
    "#\n",
    "# - DEVICE = torch.device(...)\n",
    "# - label_mapping = {...}\n",
    "# - TEST_DIR = \"...\"\n",
    "# - BATCH_SIZE, NUM_WORKERS, IMAGENET_MEAN, IMAGENET_STD\n",
    "# - Class Definition: FusionDINOv2, SingleSwinModel, SingleEVAModel\n",
    "# - Class Definition: SingleConvNextModel <--- (DARI BLOK B SEBELUMNYA)\n",
    "# - Class Definition: CLAHETransform, TestDataset, CustomDataset <---\n",
    "# - Fungsi Definition: convert_path_to_df\n",
    "# - DataFrame: val_split <--- (PENTING: Pastikan ini tidak dihapus dari BLOK B)\n",
    "# =================================================================================\n",
    "\n",
    "\n",
    "####################################################################################\n",
    "### BLOK C: FINAL ENSEMBLE INFERENCE & SUBMISSION (4-MODEL TTA + VALIDASI)\n",
    "####################################################################################\n",
    "\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"### MEMULAI BLOK C: ENSEMBLE INFERENCE & SUBMISSION (4-MODEL TTA) ###\")\n",
    "print(\"#\"*80 + \"\\n\")\n",
    "\n",
    "# C.1. Definisikan TTA transformations\n",
    "print(\"Mendefinisikan TTA transforms (Flip + Rotasi)...\")\n",
    "tta_transforms = tta.Compose(\n",
    "    [\n",
    "        tta.HorizontalFlip(),\n",
    "        tta.Rotate90(angles=[0, 90, 180, 270]), \n",
    "    ]\n",
    ")\n",
    "\n",
    "# C.2. Buat Wrapper Model untuk resizing\n",
    "class Model224Wrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        print(\" > Wrapper 224px dibuat\")\n",
    "\n",
    "    def forward(self, x_518):\n",
    "        x_224 = F.interpolate(x_518, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "        return self.model(x_224)\n",
    "\n",
    "# C.3. Inisialisasi model ASLI (tanpa TTA)\n",
    "print(\"\\nInisialisasi model dasar...\")\n",
    "model_a_base = FusionDINOv2(num_classes=len(label_mapping)).to(DEVICE)\n",
    "model_b_base = SingleSwinModel(num_classes=len(label_mapping)).to(DEVICE)\n",
    "model_c_base = SingleEVAModel(num_classes=len(label_mapping)).to(DEVICE)\n",
    "model_d_base = SingleConvNextModel(num_classes=len(label_mapping)).to(DEVICE) # <-- TAMBAH\n",
    "\n",
    "# C.4. Muat bobot ke model ASLI\n",
    "print(\"Memuat bobot...\")\n",
    "print(f\"Memuat Model A (FusionDINOv2) dari: best_gradual_unfreeze_model_one.pth\")\n",
    "model_a_base.load_state_dict(torch.load(\"/kaggle/input/swim/pytorch/default/4/best_gradual_unfreeze_model_one (1).pth\", map_location=DEVICE))\n",
    "print(f\"Memuat Model B (SingleSwinModel) dari: best_swin_standalone.pth\")\n",
    "model_b_base.load_state_dict(torch.load(\"/kaggle/input/palingbaru/pytorch/default/2/best_swin_standalone (3).pth\", map_location=DEVICE))\n",
    "print(f\"Memuat Model C (SingleEVAModel) dari: EVA.pth\")\n",
    "model_c_base.load_state_dict(torch.load(\"/kaggle/input/palingbaru/pytorch/default/2/best_eva_standalone (3).pth\", map_location=DEVICE))\n",
    "print(f\"Memuat Model D (SingleConvNextModel) dari: best_convnext_standalone.pth\") # <-- TAMBAH\n",
    "model_d_base.load_state_dict(torch.load(\"best_convnext_standalone.pth\", map_location=DEVICE)) # <-- TAMBAH\n",
    "\n",
    "# C.5. BUNGKUS model dengan TTA\n",
    "print(\"\\nMembungkus model dengan TTA Wrapper...\")\n",
    "# Model A (Fusion/DINO) menggunakan 518px, bisa langsung dibungkus\n",
    "tta_model_a = tta.ClassificationTTAWrapper(model_a_base, tta_transforms).to(DEVICE)\n",
    "\n",
    "# Model B (Swin), C (EVA), dan D (ConvNext) pakai wrapper 224px\n",
    "tta_model_b = tta.ClassificationTTAWrapper(\n",
    "    Model224Wrapper(model_b_base), tta_transforms\n",
    ").to(DEVICE)\n",
    "\n",
    "tta_model_c = tta.ClassificationTTAWrapper(\n",
    "    Model224Wrapper(model_c_base), tta_transforms\n",
    ").to(DEVICE)\n",
    "\n",
    "tta_model_d = tta.ClassificationTTAWrapper( # <-- TAMBAH\n",
    "    Model224Wrapper(model_d_base), tta_transforms\n",
    ").to(DEVICE)\n",
    "\n",
    "# Set semua model TTA ke mode eval\n",
    "tta_model_a.eval()\n",
    "tta_model_b.eval()\n",
    "tta_model_c.eval()\n",
    "tta_model_d.eval() # <-- TAMBAH\n",
    "print(\"Semua 4 model siap untuk TTA.\")\n",
    "\n",
    "# C.6. Transformasi Test (akan digunakan untuk Val dan Test)\n",
    "final_test_transform = transforms.Compose([\n",
    "    CLAHETransform(),\n",
    "    transforms.Resize((518, 518)), # <-- Loader HANYA me-load gambar 518\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "reverse_label_mapping = {v: k for k, v in label_mapping.items()}\n",
    "\n",
    "# =============================================================================\n",
    "# === C.VAL: VALIDASI ENSEMBLE PADA VALIDATION SET (BLOK BARU)\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"### MENJALANKAN VALIDASI ENSEMBLE PADA VAL SET ###\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    # 1. Buat Val Dataset & Loader (menggunakan transform 518px)\n",
    "    # Pastikan 'val_split' (DataFrame) dan 'CustomDataset' (Class) ada\n",
    "    val_dataset_tta = CustomDataset(\n",
    "        val_split, \n",
    "        'Filepath', \n",
    "        'Label', \n",
    "        final_test_transform # <-- Pakai transform 518px\n",
    "    )\n",
    "    val_loader_tta = DataLoader(\n",
    "        val_dataset_tta, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False, \n",
    "        num_workers=NUM_WORKERS\n",
    "    )\n",
    "\n",
    "    all_val_probs_a, all_val_probs_b, all_val_probs_c, all_val_probs_d = [], [], [], []\n",
    "    all_val_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader_tta, desc=\"Validasi TTA Ensemble\"):\n",
    "            images_518 = images.to(DEVICE)\n",
    "            \n",
    "            # Dapatkan prediksi TTA\n",
    "            probs_a = tta_model_a(images_518)\n",
    "            probs_b = tta_model_b(images_518)\n",
    "            probs_c = tta_model_c(images_518)\n",
    "            probs_d = tta_model_d(images_518) # <-- TAMBAH\n",
    "\n",
    "            # Simpan hasil\n",
    "            all_val_probs_a.append(probs_a.cpu())\n",
    "            all_val_probs_b.append(probs_b.cpu())\n",
    "            all_val_probs_c.append(probs_c.cpu())\n",
    "            all_val_probs_d.append(probs_d.cpu()) # <-- TAMBAH\n",
    "            all_val_labels.append(labels.cpu())\n",
    "\n",
    "    # Gabungkan batch\n",
    "    all_val_probs_a = torch.cat(all_val_probs_a, dim=0)\n",
    "    all_val_probs_b = torch.cat(all_val_probs_b, dim=0)\n",
    "    all_val_probs_c = torch.cat(all_val_probs_c, dim=0)\n",
    "    all_val_probs_d = torch.cat(all_val_probs_d, dim=0) # <-- TAMBAH\n",
    "    all_val_labels = torch.cat(all_val_labels, dim=0)\n",
    "\n",
    "    # 2. Hitung Metrik Ensemble (Contoh: Weighted Averaging)\n",
    "    # !!! GANTI BOBOT INI dengan skor Akurasi/F1 validasi Anda !!!\n",
    "    W_A_FUSION = 0.93 # Ganti dengan skor validasi asli Anda\n",
    "    W_B_SWIN = 0.93   # Ganti dengan skor validasi asli Anda\n",
    "    W_C_EVA = 0.938   # Ganti dengan skor validasi asli Anda\n",
    "    W_D_CONVNEXT = 0.94 # Ganti dengan skor validasi asli Anda\n",
    "\n",
    "    weights = torch.tensor(\n",
    "        [W_A_FUSION, W_B_SWIN, W_C_EVA, W_D_CONVNEXT], \n",
    "        dtype=torch.float32\n",
    "    ).view(-1, 1, 1)\n",
    "    weights = weights / weights.sum() # Normalisasi\n",
    "\n",
    "    prob_list = [all_val_probs_a, all_val_probs_b, all_val_probs_c, all_val_probs_d]\n",
    "    weighted_probs = torch.stack(prob_list) * weights\n",
    "    avg_probs = torch.sum(weighted_probs, dim=0)\n",
    "\n",
    "    final_preds = torch.argmax(avg_probs, dim=1).numpy()\n",
    "    true_labels = all_val_labels.numpy()\n",
    "\n",
    "    # 3. Tampilkan Hasil\n",
    "    val_f1 = f1_score(true_labels, final_preds, average=\"macro\")\n",
    "    val_acc = accuracy_score(true_labels, final_preds)\n",
    "\n",
    "    print(\"\\n--- HASIL VALIDASI TTA ENSEMBLE (4 MODEL) ---\")\n",
    "    print(f\"📈 F1-Score Macro: {val_f1:.6f}\")\n",
    "    print(f\"🎯 Accuracy:       {val_acc:.6f}\")\n",
    "    print(\"----------------------------------------------\\n\")\n",
    "    \n",
    "except NameError as e:\n",
    "    print(f\"❌ GAGAL: Tidak bisa menjalankan validasi.\")\n",
    "    print(f\"Error: {e}. (Apakah 'val_split' atau 'CustomDataset' terdefinisi?)\")\n",
    "    print(\"Melanjutkan ke inferensi test set...\\n\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# === C.TEST: INFERENSI PADA TEST SET\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"### MENJALANKAN INFERENSI PADA TEST SET ###\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# C.7. Siapkan Test Loader (menggunakan final_test_transform 518px)\n",
    "test_df = convert_path_to_df(TEST_DIR, is_test=True)\n",
    "test_dataset = TestDataset(test_df, 'Filepath', transform=final_test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "# C.8. Fungsi bantu untuk ensemble (dengan OPSI WEIGHTS)\n",
    "def make_submission(ensemble_name, prob_list, paths, weights=None):\n",
    "    if weights is None:\n",
    "        # Simple Averaging\n",
    "        print(f\"Membuat submit (Average): {ensemble_name}...\")\n",
    "        avg_probs = torch.stack(prob_list).mean(dim=0)\n",
    "    else:\n",
    "        # Weighted Averaging\n",
    "        print(f\"Membuat submit (Weighted): {ensemble_name}...\")\n",
    "        weights_tensor = torch.tensor(weights, dtype=torch.float32).view(-1, 1, 1)\n",
    "        weights_tensor = weights_tensor / weights_tensor.sum() \n",
    "        \n",
    "        weighted_probs = torch.stack(prob_list) * weights_tensor\n",
    "        avg_probs = torch.sum(weighted_probs, dim=0)\n",
    "\n",
    "    preds = torch.argmax(avg_probs, dim=1)\n",
    "    submission_data = []\n",
    "    \n",
    "    for i, path in enumerate(paths):\n",
    "        img_id = os.path.splitext(os.path.basename(path))[0]\n",
    "        label_int = preds[i].item()\n",
    "        label_str = reverse_label_mapping[label_int]\n",
    "        # Pastikan nama kolom 'id' dan 'label' (atau 'style') sesuai format submisi\n",
    "        submission_data.append({'id': img_id, 'label': label_str}) \n",
    "        \n",
    "    df = pd.DataFrame(submission_data)\n",
    "    df.sort_values(by='id', inplace=True)\n",
    "    filename = f\"submission_{ensemble_name}.csv\"\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"✅ File {filename} berhasil dibuat!\")\n",
    "\n",
    "# C.9. Proses inference TTA\n",
    "print(\"\\nMembuat prediksi TTA ensemble dari 4 model...\")\n",
    "all_probs_a, all_probs_b, all_probs_c, all_probs_d, all_paths = [], [], [], [], [] # <-- TAMBAH\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, paths in tqdm(test_loader, desc=\"Inferensi Multi-Model TTA\"):\n",
    "        images_518 = images.to(DEVICE)\n",
    "        \n",
    "        probs_a = tta_model_a(images_518)\n",
    "        probs_b = tta_model_b(images_518)\n",
    "        probs_c = tta_model_c(images_518)\n",
    "        probs_d = tta_model_d(images_518) # <-- TAMBAH\n",
    "\n",
    "        # Simpan hasil batch\n",
    "        all_probs_a.append(probs_a.cpu())\n",
    "        all_probs_b.append(probs_b.cpu())\n",
    "        all_probs_c.append(probs_c.cpu())\n",
    "        all_probs_d.append(probs_d.cpu()) # <-- TAMBAH\n",
    "        all_paths.extend(paths)\n",
    "\n",
    "# Gabungkan semua batch\n",
    "all_probs_a = torch.cat(all_probs_a, dim=0)\n",
    "all_probs_b = torch.cat(all_probs_b, dim=0)\n",
    "all_probs_c = torch.cat(all_probs_c, dim=0)\n",
    "all_probs_d = torch.cat(all_probs_d, dim=0) # <-- TAMBAH\n",
    "\n",
    "# C.10. Buat berbagai kombinasi ensemble\n",
    "print(\"\\nMenyusun berbagai kombinasi ensemble...\")\n",
    "\n",
    "# --- Simple Averaging (4 Model) ---\n",
    "make_submission(\n",
    "    \"tta_ensemble_4_models_avg\", \n",
    "    [all_probs_a, all_probs_b, all_probs_c, all_probs_d], \n",
    "    all_paths\n",
    ")\n",
    "\n",
    "# --- Weighted Averaging (4 Model) ---\n",
    "# Gunakan bobot yang SAMA seperti di blok validasi\n",
    "make_submission(\n",
    "    \"tta_ensemble_4_models_weighted\", \n",
    "    [all_probs_a, all_probs_b, all_probs_c, all_probs_d], \n",
    "    all_paths,\n",
    "    weights=[W_A_FUSION, W_B_SWIN, W_C_EVA, W_D_CONVNEXT]\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\nSELURUH PROSES ENSEMBLE TTA SELESAI\\n\" + \"=\"*50)\n",
    "\n",
    "# # C.11. Bersihkan memori\n",
    "# del model_a_base, model_b_base, model_c_base, model_d_base\n",
    "# del tta_model_a, tta_model_b, tta_model_c, tta_model_d\n",
    "# del all_probs_a, all_probs_b, all_probs_c, all_probs_d, all_paths, test_loader, test_dataset\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-07T16:08:05.033403Z",
     "iopub.status.busy": "2025-11-07T16:08:05.033040Z",
     "iopub.status.idle": "2025-11-07T16:41:48.590987Z",
     "shell.execute_reply": "2025-11-07T16:41:48.590065Z",
     "shell.execute_reply.started": "2025-11-07T16:08:05.033374Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "### MEMULAI BLOK C: ENSEMBLE INFERENCE & SUBMISSION (4-MODEL TTA) ###\n",
      "################################################################################\n",
      "\n",
      "Mendefinisikan TTA transforms (Flip + Rotasi)...\n",
      "\n",
      "Inisialisasi model dasar...\n",
      "Memuat bobot...\n",
      "Memuat Model A (FusionDINOv2)\n",
      "Memuat Model B (SingleSwinModel)\n",
      "Memuat Model C (SingleEVAModel)\n",
      "Memuat Model D (SingleConvNextModel)\n",
      "\n",
      "Membungkus model dengan TTA Wrapper...\n",
      " > Wrapper 224px dibuat\n",
      " > Wrapper 224px dibuat\n",
      " > Wrapper 224px dibuat\n",
      "Semua 4 model siap untuk TTA.\n",
      "\n",
      "==================================================\n",
      "### MENJALANKAN VALIDASI ENSEMBLE PADA VAL SET ###\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "065f00dd3cb1489897ef12182ddd68c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validasi TTA Ensemble:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- HASIL VALIDASI TTA ENSEMBLE (SIMPLE AVERAGING) ---\n",
      "| Model Combination                | F1-Macro | Accuracy |\n",
      "|----------------------------------|----------|----------|\n",
      "| Model A: DINO                  | 0.968612 | 0.968553 |\n",
      "| Model B: Swin                  | 0.924038 | 0.924528 |\n",
      "| Model C: EVA                   | 0.938813 | 0.939623 |\n",
      "| Model D: ConvNext              | 0.905987 | 0.906918 |\n",
      "|----------------------------------|----------|----------|\n",
      "| A+B (DINO+Swin)                | 0.972218 | 0.972327 |\n",
      "| A+C (DINO+EVA)                 | 0.973575 | 0.973585 |\n",
      "| A+D (DINO+ConvNext)            | 0.973415 | 0.973585 |\n",
      "| B+C (Swin+EVA)                 | 0.941810 | 0.942138 |\n",
      "| B+D (Swin+ConvNext)            | 0.930334 | 0.930818 |\n",
      "| C+D (EVA+ConvNext)             | 0.939072 | 0.939623 |\n",
      "|----------------------------------|----------|----------|\n",
      "| A+B+C (DINO+Swin+EVA)          | 0.972223 | 0.972327 |\n",
      "| A+B+D (DINO+Swin+ConvNext)     | 0.970890 | 0.971069 |\n",
      "| A+C+D (DINO+EVA+ConvNext)      | 0.975999 | 0.976101 |\n",
      "| B+C+D (Swin+EVA+ConvNext)      | 0.941743 | 0.942138 |\n",
      "|----------------------------------|----------|----------|\n",
      "| A+B+C+D (ALL 4 AVG)            | 0.967073 | 0.967296 |\n",
      "-----------------------------------------------------\n",
      "\n",
      "🏆 Kombinasi Terbaik (Simple Avg): A+C+D (DINO+EVA+ConvNext) (Acc: 0.976101)\n",
      "\n",
      "==================================================\n",
      "### MENJALANKAN INFERENSI PADA TEST SET ###\n",
      "==================================================\n",
      "\n",
      "Membuat prediksi TTA ensemble dari 4 model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e029b2a0b774ceeae2f5d683fca1cdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferensi Multi-Model TTA:   0%|          | 0/129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Menyusun berbagai kombinasi ensemble untuk submisi...\n",
      "Membuat submit (Average): tta_A+B...\n",
      "✅ File submission_tta_A+B.csv berhasil dibuat!\n",
      "Membuat submit (Average): tta_A+C...\n",
      "✅ File submission_tta_A+C.csv berhasil dibuat!\n",
      "Membuat submit (Average): tta_A+D...\n",
      "✅ File submission_tta_A+D.csv berhasil dibuat!\n",
      "Membuat submit (Average): tta_B+C...\n",
      "✅ File submission_tta_B+C.csv berhasil dibuat!\n",
      "Membuat submit (Average): tta_B+D...\n",
      "✅ File submission_tta_B+D.csv berhasil dibuat!\n",
      "Membuat submit (Average): tta_C+D...\n",
      "✅ File submission_tta_C+D.csv berhasil dibuat!\n",
      "Membuat submit (Average): tta_A+B+C...\n",
      "✅ File submission_tta_A+B+C.csv berhasil dibuat!\n",
      "Membuat submit (Average): tta_A+B+D...\n",
      "✅ File submission_tta_A+B+D.csv berhasil dibuat!\n",
      "Membuat submit (Average): tta_A+C+D...\n",
      "✅ File submission_tta_A+C+D.csv berhasil dibuat!\n",
      "Membuat submit (Average): tta_B+C+D...\n",
      "✅ File submission_tta_B+C+D.csv berhasil dibuat!\n",
      "Membuat submit (Average): tta_A+B+C+D...\n",
      "✅ File submission_tta_A+B+C+D.csv berhasil dibuat!\n",
      "\n",
      "==================================================\n",
      "SELURUH PROSES ENSEMBLE TTA SELESAI\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import ttach as tta\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "# =================================================================================\n",
    "# ASUMSI: Semua variabel, kelas (termasuk SingleConvNextModel), \n",
    "# dan DataFrame (val_split) sudah ada dari blok sebelumnya.\n",
    "# =================================================================================\n",
    "\n",
    "\n",
    "####################################################################################\n",
    "### BLOK C: FINAL ENSEMBLE INFERENCE & SUBMISSION (PERBANDINGAN KOMBINASI LENGKAP)\n",
    "####################################################################################\n",
    "\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"### MEMULAI BLOK C: ENSEMBLE INFERENCE & SUBMISSION (4-MODEL TTA) ###\")\n",
    "print(\"#\"*80 + \"\\n\")\n",
    "\n",
    "# C.1. Definisikan TTA transformations\n",
    "print(\"Mendefinisikan TTA transforms (Flip + Rotasi)...\")\n",
    "tta_transforms = tta.Compose(\n",
    "    [\n",
    "        tta.HorizontalFlip(),\n",
    "        tta.Rotate90(angles=[0, 90, 180, 270]), \n",
    "    ]\n",
    ")\n",
    "\n",
    "# C.2. Buat Wrapper Model untuk resizing\n",
    "class Model224Wrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        print(\" > Wrapper 224px dibuat\")\n",
    "\n",
    "    def forward(self, x_518):\n",
    "        x_224 = F.interpolate(x_518, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "        return self.model(x_224)\n",
    "\n",
    "# C.3. Inisialisasi model ASLI\n",
    "print(\"\\nInisialisasi model dasar...\")\n",
    "model_a_base = FusionDINOv2(num_classes=len(label_mapping)).to(DEVICE)\n",
    "model_b_base = SingleSwinModel(num_classes=len(label_mapping)).to(DEVICE)\n",
    "model_c_base = SingleEVAModel(num_classes=len(label_mapping)).to(DEVICE)\n",
    "model_d_base = SingleConvNextModel(num_classes=len(label_mapping)).to(DEVICE)\n",
    "\n",
    "# C.4. Muat bobot ke model ASLI\n",
    "print(\"Memuat bobot...\")\n",
    "print(f\"Memuat Model A (FusionDINOv2)\")\n",
    "model_a_base.load_state_dict(torch.load(\"/kaggle/input/swim/pytorch/default/4/best_gradual_unfreeze_model_one (1).pth\", map_location=DEVICE))\n",
    "print(f\"Memuat Model B (SingleSwinModel)\")\n",
    "model_b_base.load_state_dict(torch.load(\"/kaggle/input/palingbaru/pytorch/default/2/best_swin_standalone (3).pth\", map_location=DEVICE))\n",
    "print(f\"Memuat Model C (SingleEVAModel)\")\n",
    "model_c_base.load_state_dict(torch.load(\"/kaggle/input/palingbaru/pytorch/default/2/best_eva_standalone (3).pth\", map_location=DEVICE))\n",
    "print(f\"Memuat Model D (SingleConvNextModel)\")\n",
    "model_d_base.load_state_dict(torch.load(\"best_convnext_standalone.pth\", map_location=DEVICE))\n",
    "\n",
    "# C.5. BUNGKUS model dengan TTA\n",
    "print(\"\\nMembungkus model dengan TTA Wrapper...\")\n",
    "tta_model_a = tta.ClassificationTTAWrapper(model_a_base, tta_transforms).to(DEVICE)\n",
    "tta_model_b = tta.ClassificationTTAWrapper(Model224Wrapper(model_b_base), tta_transforms).to(DEVICE)\n",
    "tta_model_c = tta.ClassificationTTAWrapper(Model224Wrapper(model_c_base), tta_transforms).to(DEVICE)\n",
    "tta_model_d = tta.ClassificationTTAWrapper(Model224Wrapper(model_d_base), tta_transforms).to(DEVICE)\n",
    "\n",
    "# Set semua model TTA ke mode eval\n",
    "tta_model_a.eval()\n",
    "tta_model_b.eval()\n",
    "tta_model_c.eval()\n",
    "tta_model_d.eval()\n",
    "print(\"Semua 4 model siap untuk TTA.\")\n",
    "\n",
    "# C.6. Transformasi Test\n",
    "final_test_transform = transforms.Compose([\n",
    "    CLAHETransform(),\n",
    "    transforms.Resize((518, 518)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "reverse_label_mapping = {v: k for k, v in label_mapping.items()}\n",
    "\n",
    "# =============================================================================\n",
    "# === C.VAL: VALIDASI ENSEMBLE PADA VALIDATION SET (BLOK BARU)\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"### MENJALANKAN VALIDASI ENSEMBLE PADA VAL SET ###\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# C.VAL.1. Fungsi Helper untuk Evaluasi\n",
    "def evaluate_ensemble(name, prob_list, true_labels_np):\n",
    "    \"\"\"Menghitung F1 & Akurasi untuk daftar probabilitas (simple averaging)\"\"\"\n",
    "    avg_probs = torch.stack(prob_list).mean(dim=0)\n",
    "    final_preds = torch.argmax(avg_probs, dim=1).numpy()\n",
    "    \n",
    "    f1 = f1_score(true_labels_np, final_preds, average=\"macro\")\n",
    "    acc = accuracy_score(true_labels_np, final_preds)\n",
    "    \n",
    "    print(f\"| {name.ljust(30)} | {f1:.6f} | {acc:.6f} |\")\n",
    "    return (name, f1, acc)\n",
    "\n",
    "try:\n",
    "    # C.VAL.2. Buat Val Loader\n",
    "    val_dataset_tta = CustomDataset(val_split, 'Filepath', 'Label', final_test_transform)\n",
    "    val_loader_tta = DataLoader(val_dataset_tta, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    all_val_probs_a, all_val_probs_b, all_val_probs_c, all_val_probs_d = [], [], [], []\n",
    "    all_val_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader_tta, desc=\"Validasi TTA Ensemble\"):\n",
    "            images_518 = images.to(DEVICE)\n",
    "            \n",
    "            probs_a = tta_model_a(images_518)\n",
    "            probs_b = tta_model_b(images_518)\n",
    "            probs_c = tta_model_c(images_518)\n",
    "            probs_d = tta_model_d(images_518)\n",
    "\n",
    "            all_val_probs_a.append(probs_a.cpu())\n",
    "            all_val_probs_b.append(probs_b.cpu())\n",
    "            all_val_probs_c.append(probs_c.cpu())\n",
    "            all_val_probs_d.append(probs_d.cpu())\n",
    "            all_val_labels.append(labels.cpu())\n",
    "\n",
    "    # C.VAL.3. Gabungkan Batch\n",
    "    all_val_probs_a = torch.cat(all_val_probs_a, dim=0)\n",
    "    all_val_probs_b = torch.cat(all_val_probs_b, dim=0)\n",
    "    all_val_probs_c = torch.cat(all_val_probs_c, dim=0)\n",
    "    all_val_probs_d = torch.cat(all_val_probs_d, dim=0)\n",
    "    all_val_labels_np = torch.cat(all_val_labels, dim=0).numpy()\n",
    "\n",
    "    # C.VAL.4. Tampilkan Tabel Perbandingan (Simple Averaging)\n",
    "    print(\"\\n--- HASIL VALIDASI TTA ENSEMBLE (SIMPLE AVERAGING) ---\")\n",
    "    print(\"| Model Combination                | F1-Macro | Accuracy |\")\n",
    "    print(\"|----------------------------------|----------|----------|\")\n",
    "    \n",
    "    results = [] # Simpan hasil untuk diurutkan\n",
    "    \n",
    "    # Model Tunggal (untuk baseline)\n",
    "    results.append(evaluate_ensemble(\"Model A: DINO\", [all_val_probs_a], all_val_labels_np))\n",
    "    results.append(evaluate_ensemble(\"Model B: Swin\", [all_val_probs_b], all_val_labels_np))\n",
    "    results.append(evaluate_ensemble(\"Model C: EVA\", [all_val_probs_c], all_val_labels_np))\n",
    "    results.append(evaluate_ensemble(\"Model D: ConvNext\", [all_val_probs_d], all_val_labels_np))\n",
    "    print(\"|----------------------------------|----------|----------|\")\n",
    "\n",
    "    # Kombinasi 2 Model\n",
    "    results.append(evaluate_ensemble(\"A+B (DINO+Swin)\", [all_val_probs_a, all_val_probs_b], all_val_labels_np))\n",
    "    results.append(evaluate_ensemble(\"A+C (DINO+EVA)\", [all_val_probs_a, all_val_probs_c], all_val_labels_np))\n",
    "    results.append(evaluate_ensemble(\"A+D (DINO+ConvNext)\", [all_val_probs_a, all_val_probs_d], all_val_labels_np))\n",
    "    results.append(evaluate_ensemble(\"B+C (Swin+EVA)\", [all_val_probs_b, all_val_probs_c], all_val_labels_np))\n",
    "    results.append(evaluate_ensemble(\"B+D (Swin+ConvNext)\", [all_val_probs_b, all_val_probs_d], all_val_labels_np))\n",
    "    results.append(evaluate_ensemble(\"C+D (EVA+ConvNext)\", [all_val_probs_c, all_val_probs_d], all_val_labels_np))\n",
    "    print(\"|----------------------------------|----------|----------|\")\n",
    "\n",
    "    # Kombinasi 3 Model\n",
    "    results.append(evaluate_ensemble(\"A+B+C (DINO+Swin+EVA)\", [all_val_probs_a, all_val_probs_b, all_val_probs_c], all_val_labels_np))\n",
    "    results.append(evaluate_ensemble(\"A+B+D (DINO+Swin+ConvNext)\", [all_val_probs_a, all_val_probs_b, all_val_probs_d], all_val_labels_np))\n",
    "    results.append(evaluate_ensemble(\"A+C+D (DINO+EVA+ConvNext)\", [all_val_probs_a, all_val_probs_c, all_val_probs_d], all_val_labels_np))\n",
    "    results.append(evaluate_ensemble(\"B+C+D (Swin+EVA+ConvNext)\", [all_val_probs_b, all_val_probs_c, all_val_probs_d], all_val_labels_np))\n",
    "    print(\"|----------------------------------|----------|----------|\")\n",
    "\n",
    "    # Kombinasi 4 Model\n",
    "    results.append(evaluate_ensemble(\"A+B+C+D (ALL 4 AVG)\", [all_val_probs_a, all_val_probs_b, all_val_probs_c, all_val_probs_d], all_val_labels_np))\n",
    "    print(\"-----------------------------------------------------\")\n",
    "\n",
    "    # Temukan yang terbaik\n",
    "    best_result = sorted(results, key=lambda x: x[2], reverse=True)[0]\n",
    "    print(f\"\\n🏆 Kombinasi Terbaik (Simple Avg): {best_result[0]} (Acc: {best_result[2]:.6f})\")\n",
    "    \n",
    "except NameError as e:\n",
    "    print(f\"❌ GAGAL: Tidak bisa menjalankan validasi. Error: {e}\")\n",
    "    print(\"Melanjutkan ke inferensi test set...\\n\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# === C.TEST: INFERENSI PADA TEST SET\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"### MENJALANKAN INFERENSI PADA TEST SET ###\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# C.7. Siapkan Test Loader\n",
    "test_df = convert_path_to_df(TEST_DIR, is_test=True)\n",
    "test_dataset = TestDataset(test_df, 'Filepath', transform=final_test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "# C.8. Fungsi bantu untuk ensemble (dengan OPSI WEIGHTS)\n",
    "def make_submission(ensemble_name, prob_list, paths, weights=None):\n",
    "    if weights is None:\n",
    "        print(f\"Membuat submit (Average): {ensemble_name}...\")\n",
    "        avg_probs = torch.stack(prob_list).mean(dim=0)\n",
    "    else:\n",
    "        print(f\"Membuat submit (Weighted): {ensemble_name}...\")\n",
    "        weights_tensor = torch.tensor(weights, dtype=torch.float32).view(-1, 1, 1)\n",
    "        weights_tensor = weights_tensor / weights_tensor.sum() \n",
    "        weighted_probs = torch.stack(prob_list) * weights_tensor\n",
    "        avg_probs = torch.sum(weighted_probs, dim=0)\n",
    "\n",
    "    preds = torch.argmax(avg_probs, dim=1)\n",
    "    submission_data = []\n",
    "    \n",
    "    for i, path in enumerate(paths):\n",
    "        img_id = os.path.splitext(os.path.basename(path))[0]\n",
    "        label_int = preds[i].item()\n",
    "        label_str = reverse_label_mapping[label_int]\n",
    "        submission_data.append({'id': img_id, 'label': label_str}) # Pastikan nama kolom 'label'\n",
    "        \n",
    "    df = pd.DataFrame(submission_data)\n",
    "    df.sort_values(by='id', inplace=True)\n",
    "    filename = f\"submission_{ensemble_name}.csv\"\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"✅ File {filename} berhasil dibuat!\")\n",
    "\n",
    "# C.9. Proses inference TTA\n",
    "print(\"\\nMembuat prediksi TTA ensemble dari 4 model...\")\n",
    "all_probs_a, all_probs_b, all_probs_c, all_probs_d, all_paths = [], [], [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, paths in tqdm(test_loader, desc=\"Inferensi Multi-Model TTA\"):\n",
    "        images_518 = images.to(DEVICE)\n",
    "        \n",
    "        probs_a = tta_model_a(images_518)\n",
    "        probs_b = tta_model_b(images_518)\n",
    "        probs_c = tta_model_c(images_518)\n",
    "        probs_d = tta_model_d(images_518) \n",
    "\n",
    "        all_probs_a.append(probs_a.cpu())\n",
    "        all_probs_b.append(probs_b.cpu())\n",
    "        all_probs_c.append(probs_c.cpu())\n",
    "        all_probs_d.append(probs_d.cpu())\n",
    "        all_paths.extend(paths)\n",
    "\n",
    "# Gabungkan semua batch\n",
    "all_probs_a = torch.cat(all_probs_a, dim=0)\n",
    "all_probs_b = torch.cat(all_probs_b, dim=0)\n",
    "all_probs_c = torch.cat(all_probs_c, dim=0)\n",
    "all_probs_d = torch.cat(all_probs_d, dim=0)\n",
    "\n",
    "# C.10. Buat SEMUA kombinasi submisi (Simple Averaging)\n",
    "print(\"\\nMenyusun berbagai kombinasi ensemble untuk submisi...\")\n",
    "\n",
    "# --- Kombinasi 2 Model ---\n",
    "make_submission(\"tta_A+B\", [all_probs_a, all_probs_b], all_paths)\n",
    "make_submission(\"tta_A+C\", [all_probs_a, all_probs_c], all_paths)\n",
    "make_submission(\"tta_A+D\", [all_probs_a, all_probs_d], all_paths)\n",
    "make_submission(\"tta_B+C\", [all_probs_b, all_probs_c], all_paths)\n",
    "make_submission(\"tta_B+D\", [all_probs_b, all_probs_d], all_paths)\n",
    "make_submission(\"tta_C+D\", [all_probs_c, all_probs_d], all_paths)\n",
    "\n",
    "# --- Kombinasi 3 Model ---\n",
    "make_submission(\"tta_A+B+C\", [all_probs_a, all_probs_b, all_probs_c], all_paths)\n",
    "make_submission(\"tta_A+B+D\", [all_probs_a, all_probs_b, all_probs_d], all_paths)\n",
    "make_submission(\"tta_A+C+D\", [all_probs_a, all_probs_c, all_probs_d], all_paths)\n",
    "make_submission(\"tta_B+C+D\", [all_probs_b, all_probs_c, all_probs_d], all_paths)\n",
    "\n",
    "# --- Kombinasi 4 Model ---\n",
    "make_submission(\"tta_A+B+C+D\", [all_probs_a, all_probs_b, all_probs_c, all_probs_d], all_paths)\n",
    "\n",
    "# --- (Opsional) Weighted Average Berdasarkan Skor Val Anda ---\n",
    "# Ganti dengan skor akurasi validasi individu\n",
    "# W_A = 0.93; W_B = 0.93; W_C = 0.938; W_D = 0.94 \n",
    "# make_submission(\n",
    "#     \"tta_4_models_WEIGHTED\", \n",
    "#     [all_probs_a, all_probs_b, all_probs_c, all_probs_d], \n",
    "#     all_paths,\n",
    "#     weights=[W_A, W_B, W_C, W_D]\n",
    "# )\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\nSELURUH PROSES ENSEMBLE TTA SELESAI\\n\" + \"=\"*50)\n",
    "\n",
    "# # C.11. Bersihkan memori\n",
    "# del model_a_base, model_b_base, model_c_base, model_d_base\n",
    "# del tta_model_a, tta_model_b, tta_model_c, tta_model_d\n",
    "# del all_probs_a, all_probs_b, all_probs_c, all_probs_d, all_paths, test_loader, test_dataset\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-07T16:48:57.537416Z",
     "iopub.status.busy": "2025-11-07T16:48:57.537075Z",
     "iopub.status.idle": "2025-11-07T16:56:48.308931Z",
     "shell.execute_reply": "2025-11-07T16:56:48.307840Z",
     "shell.execute_reply.started": "2025-11-07T16:48:57.537391Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "### BLOK VALIDASI KHUSUS: Model Fusion (DINOv2) ###\n",
      "################################################################################\n",
      "\n",
      "Memuat Model A (FusionDINOv2)...\n",
      "Membungkus Model A dengan TTA...\n",
      "Mempersiapkan Validation Loader (data: 795)...\n",
      "Memulai evaluasi TTA vs Non-TTA...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fff0a1825944082a12f283ddc8e84a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validasi Model A:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- HASIL VALIDASI UNTUK Model A (FusionDINOv2) ---\n",
      "\n",
      "Metrik TANPA TTA:\n",
      "  > F1-Score Macro: 0.963352\n",
      "  > Accuracy:       0.963522\n",
      "\n",
      "Metrik DENGAN TTA (Flip + Rotasi):\n",
      "  > F1-Score Macro: 0.968612\n",
      "  > Accuracy:       0.968553\n",
      "\n",
      "==================================================\n",
      "📈 PENINGKATAN (TTA - NonTTA):\n",
      "   F1-Score: +0.005260\n",
      "   Accuracy: +0.005031\n",
      "==================================================\n",
      "Membersihkan memori...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import ttach as tta\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "####################################################################################\n",
    "### BLOK VALIDASI KHUSUS: Model Fusion (DINOv2) TTA vs Non-TTA\n",
    "####################################################################################\n",
    "\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"### BLOK VALIDASI KHUSUS: Model Fusion (DINOv2) ###\")\n",
    "print(\"#\"*80 + \"\\n\")\n",
    "\n",
    "# 1. Inisialisasi Model (Asumsi Kelas sudah terdefinisi)\n",
    "print(\"Memuat Model A (FusionDINOv2)...\")\n",
    "try:\n",
    "    model_a_base = FusionDINOv2(num_classes=len(label_mapping)).to(DEVICE)\n",
    "    model_a_base.load_state_dict(torch.load(\"/kaggle/input/swim/pytorch/default/4/best_gradual_unfreeze_model_one (1).pth\", map_location=DEVICE))\n",
    "    model_a_base.eval() # Set ke mode eval\n",
    "\n",
    "    # 2. Inisialisasi Model TTA\n",
    "    # (Asumsi tta_transforms sudah terdefinisi dari BLOK C)\n",
    "    print(\"Membungkus Model A dengan TTA...\")\n",
    "    tta_model_a = tta.ClassificationTTAWrapper(model_a_base, tta_transforms).to(DEVICE)\n",
    "    tta_model_a.eval() # Set ke mode eval\n",
    "\n",
    "    # 3. Buat Val Loader\n",
    "    # (Asumsi val_split (DataFrame) dan CustomDataset (Class) sudah ada)\n",
    "    # (Asumsi final_test_transform (518px) sudah ada)\n",
    "    print(f\"Mempersiapkan Validation Loader (data: {len(val_split)})...\")\n",
    "    val_dataset_check = CustomDataset(\n",
    "        val_split, \n",
    "        'Filepath', \n",
    "        'Label', \n",
    "        final_test_transform # <-- Pakai transform 518px\n",
    "    )\n",
    "    val_loader_check = DataLoader(\n",
    "        val_dataset_check, \n",
    "        batch_size=BATCH_SIZE, # Atur BATCH_SIZE lebih kecil jika OOM\n",
    "        shuffle=False, \n",
    "        num_workers=NUM_WORKERS\n",
    "    )\n",
    "\n",
    "    # 4. Jalankan Evaluasi\n",
    "    all_labels_list = []\n",
    "    all_preds_no_tta_list = []\n",
    "    all_preds_tta_list = []\n",
    "\n",
    "    print(\"Memulai evaluasi TTA vs Non-TTA...\")\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader_check, desc=\"Validasi Model A\"):\n",
    "            images_518 = images.to(DEVICE)\n",
    "            labels_cpu = labels.cpu()\n",
    "            all_labels_list.append(labels_cpu)\n",
    "\n",
    "            # --- 1. Prediksi NON-TTA (Manual) ---\n",
    "            with torch.amp.autocast(device_type = 'cuda'):\n",
    "                outputs_no_tta = model_a_base(images_518)\n",
    "            preds_no_tta = torch.argmax(outputs_no_tta, dim=1).cpu()\n",
    "            all_preds_no_tta_list.append(preds_no_tta)\n",
    "\n",
    "            # --- 2. Prediksi DENGAN TTA ---\n",
    "            # tta_model_a sudah menangani autocast, softmax, dan averaging\n",
    "            probs_tta = tta_model_a(images_518) \n",
    "            preds_tta = torch.argmax(probs_tta, dim=1).cpu()\n",
    "            all_preds_tta_list.append(preds_tta)\n",
    "\n",
    "    # 5. Gabungkan hasil\n",
    "    all_labels = torch.cat(all_labels_list).numpy()\n",
    "    all_preds_no_tta = torch.cat(all_preds_no_tta_list).numpy()\n",
    "    all_preds_tta = torch.cat(all_preds_tta_list).numpy()\n",
    "\n",
    "    # 6. Hitung dan Tampilkan Metrik\n",
    "    print(\"\\n--- HASIL VALIDASI UNTUK Model A (FusionDINOv2) ---\")\n",
    "    \n",
    "    # Non-TTA\n",
    "    f1_no_tta = f1_score(all_labels, all_preds_no_tta, average=\"macro\")\n",
    "    acc_no_tta = accuracy_score(all_labels, all_preds_no_tta)\n",
    "    print(\"\\nMetrik TANPA TTA:\")\n",
    "    print(f\"  > F1-Score Macro: {f1_no_tta:.6f}\")\n",
    "    print(f\"  > Accuracy:       {acc_no_tta:.6f}\")\n",
    "\n",
    "    # TTA\n",
    "    f1_tta = f1_score(all_labels, all_preds_tta, average=\"macro\")\n",
    "    acc_tta = accuracy_score(all_labels, all_preds_tta)\n",
    "    print(\"\\nMetrik DENGAN TTA (Flip + Rotasi):\")\n",
    "    print(f\"  > F1-Score Macro: {f1_tta:.6f}\")\n",
    "    print(f\"  > Accuracy:       {acc_tta:.6f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"📈 PENINGKATAN (TTA - NonTTA):\")\n",
    "    print(f\"   F1-Score: {(f1_tta - f1_no_tta):+.6f}\")\n",
    "    print(f\"   Accuracy: {(acc_tta - acc_no_tta):+.6f}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # 7. Bersihkan Memori\n",
    "    print(\"Membersihkan memori...\")\n",
    "    del model_a_base, tta_model_a, val_loader_check, val_dataset_check\n",
    "    del all_labels, all_preds_no_tta, all_preds_tta\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"\\n❌ GAGAL: Tidak bisa menjalankan validasi.\")\n",
    "    print(f\"Error: {e}.\")\n",
    "    print(\"Pastikan 'val_split', 'CustomDataset', 'FusionDINOv2', 'tta_transforms',\")\n",
    "    print(\"dan 'final_test_transform' sudah terdefinisi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-08T17:11:35.436504Z",
     "iopub.status.busy": "2025-11-08T17:11:35.436216Z",
     "iopub.status.idle": "2025-11-08T17:12:15.246787Z",
     "shell.execute_reply": "2025-11-08T17:12:15.246199Z",
     "shell.execute_reply.started": "2025-11-08T17:11:35.436482Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "### MEMULAI BLOK C: INFERENSI DAN SUBMISI EVA TRANSFORMER ###\n",
      "################################################################################\n",
      "\n",
      "Memuat bobot dari: best_eva_standalone.pth\n",
      "Bobot EVA Transformer berhasil dimuat.\n",
      "Memulai inferensi pada data tes...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f579fa585b4e5ab742de08dbbd431b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferensi:   0%|          | 0/129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferensi selesai. Total prediksi: 2057\n",
      "\n",
      "==================================================\n",
      "🎉 Submisi berhasil dibuat!\n",
      "File submisi disimpan di: submission_eva_transformer.csv\n",
      "Contoh 5 baris pertama:\n",
      "          id        label\n",
      "0  0001.jpg  Sate Padang\n",
      "1  0002.jpg      Rendang\n",
      "2  0003.jpg  Ayam Betutu\n",
      "3  0004.jpg      Rendang\n",
      "4  0005.jpg     Ayam Pop\n",
      "==================================================\n",
      "Membersihkan memori GPU setelah inferensi...\n"
     ]
    }
   ],
   "source": [
    "# GANTI NAMA FILE BOBOT\n",
    "BEST_MODEL_NAME_EVA = \"best_eva_standalone.pth\"\n",
    "# (Asumsi idx_to_class, TEST_DIR, test_transform, dll sudah terdefinisi)\n",
    "# ...\n",
    "\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"### MEMULAI BLOK C: INFERENSI DAN SUBMISI EVA TRANSFORMER ###\") # Ganti teks\n",
    "print(\"#\"*80 + \"\\n\")\n",
    "\n",
    "# C.1. Definisikan Custom Dataset untuk Data Tes\n",
    "# (Tidak ada perubahan di sini, TestImageDataset sudah benar)\n",
    "class TestImageDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.image_files = sorted([f for f in os.listdir(img_dir) if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, img_name\n",
    "\n",
    "# C.2. Tentukan Transformasi untuk Data Tes\n",
    "# (Tidak ada perubahan di sini, test_transform sudah benar)\n",
    "test_transform = transforms.Compose([\n",
    "    CLAHETransform(), \n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "# C.3. Inisialisasi Dataset dan DataLoader\n",
    "# (Tidak ada perubahan di sini)\n",
    "test_dataset = TestImageDataset(TEST_DIR, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0) \n",
    "\n",
    "# C.4. Inisialisasi Model dan Pemuatan Bobot (DIGANTI KE EVA)\n",
    "model_inferensi = SingleEvaModel(num_classes=len(label_mapping)).to(DEVICE) # Panggil kelas EVA\n",
    "print(f\"Memuat bobot dari: {BEST_MODEL_NAME_EVA}\") # Ganti variabel\n",
    "\n",
    "if os.path.exists(BEST_MODEL_NAME_EVA): # Ganti variabel\n",
    "    model_inferensi.load_state_dict(torch.load(BEST_MODEL_NAME_EVA, map_location=DEVICE)) # Ganti variabel\n",
    "    print(\"Bobot EVA Transformer berhasil dimuat.\") # Ganti teks\n",
    "else:\n",
    "    print(f\"❌ Error: File bobot {BEST_MODEL_NAME_EVA} tidak ditemukan!\") # Ganti variabel\n",
    "    # raise FileNotFoundError(f\"File bobot {BEST_MODEL_NAME_EVA} tidak ditemukan.\")\n",
    "\n",
    "\n",
    "# C.5. Melakukan Inferensi\n",
    "# (Tidak ada perubahan di sini)\n",
    "model_inferensi.eval() \n",
    "image_ids = []\n",
    "predictions = []\n",
    "\n",
    "print(\"Memulai inferensi pada data tes...\")\n",
    "with torch.no_grad():\n",
    "    for images, file_names in tqdm(test_loader, desc=\"Inferensi\"):\n",
    "        images = images.to(DEVICE)\n",
    "        with torch.amp.autocast(device_type = 'cuda' if 'cuda' in DEVICE.type else 'cpu'):\n",
    "            outputs = model_inferensi(images)\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        image_ids.extend(file_names)\n",
    "        predictions.extend(preds)\n",
    "\n",
    "print(\"Inferensi selesai. Total prediksi:\", len(predictions))\n",
    "\n",
    "# C.6. Pemetaan Ulang Label ke Nama Kelas\n",
    "# (Tidak ada perubahan di sini)\n",
    "# Buat reverse mapping: indeks ke nama kelas\n",
    "idx_to_class = {v: k for k, v in label_mapping.items()}\n",
    "predicted_labels = [idx_to_class[p] for p in predictions]\n",
    "\n",
    "# C.7. Membuat DataFrame Submisi\n",
    "# (Tidak ada perubahan di sini)\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': image_ids,\n",
    "    'label': predicted_labels\n",
    "})\n",
    "\n",
    "# C.8. Menyimpan ke File CSV (GANTI NAMA FILE)\n",
    "submission_filename = \"submission_eva_transformer.csv\" \n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"🎉 Submisi berhasil dibuat!\")\n",
    "print(f\"File submisi disimpan di: {submission_filename}\")\n",
    "print(\"Contoh 5 baris pertama:\\n\", submission_df.head())\n",
    "print(\"=\"*50)\n",
    "\n",
    "# C.9. Bersihkan Memori GPU\n",
    "print(\"Membersihkan memori GPU setelah inferensi...\")\n",
    "# del model_inferensi\n",
    "# torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8601058,
     "sourceId": 13543532,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8660360,
     "sourceId": 13626164,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 485889,
     "modelInstanceId": 470010,
     "sourceId": 624571,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 485889,
     "modelInstanceId": 470010,
     "sourceId": 629269,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 492265,
     "modelInstanceId": 476344,
     "sourceId": 632184,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
